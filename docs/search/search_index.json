{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Book of the Runtime","text":"<p>Welcome to the Book of the Runtime (BOTR) for the .NET Runtime.   This contains a collection of articles about the non-trivial internals of the .NET Runtime.   Its intended audience are people actually modifying the code or simply wishing to have a deep understanding of the runtime.</p> <p>Below is a table of contents.</p> <ul> <li>Book of the Runtime FAQ</li> <li>Introduction to the Common Language Runtime</li> <li>Garbage Collection Design</li> <li>Threading</li> <li>RyuJIT Overview</li> <li>Porting RyuJIT to other platforms</li> <li>Type System</li> <li>Type Loader</li> <li>Method Descriptor</li> <li>Virtual Stub Dispatch</li> <li>Stack Walking</li> <li><code>System.Private.CoreLib</code> and calling into the runtime</li> <li>Data Access Component (DAC) Notes</li> <li>Profiling</li> <li>Implementing Profilability</li> <li>What Every Dev needs to Know About Exceptions in the Runtime</li> <li>ReadyToRun Overview</li> <li>CLR ABI</li> <li>Cross-platform Minidumps</li> <li>Mixed Mode Assemblies</li> <li>Guide For Porting</li> <li>Vectors and Intrinsics</li> </ul> <p>It may be possible that this table is not complete.  You can get a complete list by looking at the directory where all the chapters are stored:</p> <ul> <li>All Book of the Runtime (BOTR) chapters on GitHub</li> </ul>"},{"location":"botr-faq/","title":"Book of the Runtime (BotR) FAQ","text":""},{"location":"botr-faq/#what-is-the-botr","title":"What is the BotR?","text":"<p>The Book of the Runtime is a set of documents that describe components in the CLR and BCL. They are intended to focus more on architecture and invariants and not an annotated description of the codebase.</p> <p>It was originally created within Microsoft in ~ 2007, including this document. Developers were responsible to document their feature areas. This helped new devs joining the team and also helped share the product architecture across the team.</p> <p>We realized that the BotR is even more valuable now, with CoreCLR being open source on GitHub. We are publishing BotR chapters to help a new set of CLR developers.</p> <p>Each of the BoTR documents were written with a certain perspective, both in terms of the timeframe and the author. We did not think it was right to mutate the documents to make them more \"2015\". They remain the docs that they were, modulo a few spelling corrections and a conversion to markdown. That said, we'll accept PRs to the docs to improve them.</p>"},{"location":"botr-faq/#who-is-the-main-audience-of-botr","title":"Who is the main audience of BotR?","text":"<ul> <li>Developers who are working on bugs that impinge on an area and need a high level overview of the component.</li> <li>Developers working on new features with dependencies on a component need to know enough about it to ensure the new feature will interact correctly with existing components.</li> <li>New developers need this chapter to maintain a given component.</li> </ul>"},{"location":"botr-faq/#what-should-be-in-a-botr-chapter","title":"What should be in a BotR chapter?","text":"<p>The purpose of Book of the Runtime chapters is to capture information that we cannot easily reconstruct from the functional specification and source code alone, and to enable communication at a high level between team members. It explains concepts and presents a top-down description, and most importantly, explains why we made the design decisions we made.</p>"},{"location":"botr-faq/#how-is-this-different-from-a-design-doc","title":"How is this different from a design doc?","text":"<p>A design doc is what you write before you start implementation. A BotR chapter is usually written after a feature is implemented, at which point you have already decided the pros and cons of various design options and settled on one (and perhaps have plans to use an improved design in the future), and have a much better idea about all the details (some of which could be very hard to think of without actually going through the implementation/testing). So you can talk about rationales behind design decisions a lot better.</p>"},{"location":"botr-faq/#i-am-a-new-dev-and-not-familiar-with-any-features-yet-how-can-i-contribute","title":"I am a new dev and not familiar with any features yet, how can I contribute?","text":"<p>A new dev can be a great contributor to BotR as one of the most important purposes of BotR is to help new devs with getting up to speed. Here are some ways you can contribute:</p> <ul> <li>Be a reviewer! If you think some things are not clear or could be explained better, do not hesitate to contact the author of the chapter and chat with them to see how you can make it more understandable.</li> <li>As you are getting up to speed in your area, look over the BotR chapters for your area and see if there are any errors or anything that requires an update and make the modifications yourself.</li> <li>Volunteer to write a chapter or part of a chapter. This might seem like a daunting task but you can start by just accumulating knowledge - take notes as you learn stuff about your area and gradually mold it into a BotR chapter.</li> </ul>"},{"location":"botr-faq/#what-are-the-responsibilities-of-a-botr-reviewer","title":"What are the responsibilities of a BotR reviewer?","text":"<p>As a reviewer you will be expected to give constructive comments on the chapter you are reviewing. You can comment on any aspect, eg. the technical depth, writing style, content coverage. Keep in mind that BotR is mostly about design and architectural issues that may not be obvious. It is not meant to walk you through implementation details. Please focus on that.</p>"},{"location":"botr-faq/#i-really-dont-have-time-to-work-on-a-botr-chapter-it-seems-like-i-always-have-other-things-to-do-what-do-i-do","title":"I really don't have time to work on a BotR chapter \u2013 it seems like I always have other things to do. What do I do?","text":"<p>Here are some ways I think would be useful when working on BotR.</p> <ul> <li>Spread the work out; don't make it a workitem as in \"I will need to spend the next Monday through Thursday to work on my chapter\"; think of it more like something you do when you want to take a break from coding or bug fixing, or just a change of scenery. I find it much easier to spend a little time here and there working on a chapter than having to specifically allocate a contiguous number of days which always seem hard to come by.</li> <li>Have someone else write the chapter or most of the chapter for you. I am not joking. This is actually a very good way to help new devs ramp up. If you will be mentoring a new dev in your area, spend time with them to explain the feature area and encourage them to write a BotR chapter if one doesn't already exist. Of course be a reviewer of it.</li> <li>Use other documentation that is already there. There are MSDN docs and blog posts on .NET features. This can certainly be a base for your BotR chapter as well.</li> </ul>"},{"location":"clr-abi/","title":"CLR ABI","text":"<p>This document describes the .NET Common Language Runtime (CLR) software conventions (or ABI, \"Application Binary Interface\"). It focuses on the ABI for the x64 (aka, AMD64), ARM (aka, ARM32 or Thumb-2), and ARM64 processor architectures. Documentation for the x86 ABI is somewhat scant, but information on the basics of the calling convention is included at the bottom of this document.</p> <p>It describes requirements that the Just-In-Time (JIT) compiler imposes on the VM and vice-versa.</p> <p>A note on the JIT codebases: JIT32 refers to the original JIT codebase that originally generated x86 code and was later ported to generate ARM code. JIT64 refers to the legacy .NET Framework codebase that supports AMD64. The RyuJIT compiler evolved from JIT32, and now supports all platforms and architectures. See this post for more RyuJIT history.</p> <p>NativeAOT refers to a runtime that is optimized for ahead-of-time compilation (AOT). The NativeAOT ABI differs in a few details for simplicity and consistency across platforms.</p>"},{"location":"clr-abi/#getting-started","title":"Getting started","text":"<p>Read everything in the documented Windows and non-Windows ABI documentation. The CLR follows those basic conventions. This document only describes things that are CLR-specific, or exceptions from those documents.</p>"},{"location":"clr-abi/#windows-abi-documentation","title":"Windows ABI documentation","text":"<p>AMD64: See x64 Software Conventions.</p> <p>ARM: See Overview of ARM32 ABI Conventions.</p> <p>ARM64: See Overview of ARM64 ABI conventions.</p>"},{"location":"clr-abi/#non-windows-abi-documentation","title":"Non-Windows ABI documentation","text":"<p>Arm corporation ABI documentation (for ARM32 and ARM64) is here and here. Apple's ARM64 calling convention differences can be found here.</p> <p>The Linux System V x86_64 ABI is documented in System V Application Binary Interface / AMD64 Architecture Processor Supplement, with document source material here.</p> <p>The LoongArch64 ABI documentation is here</p> <p>The RISC-V ABIs Specification: latest release, latest draft, document source repo.</p>"},{"location":"clr-abi/#general-unwindframe-layout","title":"General Unwind/Frame Layout","text":"<p>For all non-x86 platforms, all methods must have unwind information so the garbage collector (GC) can unwind them (unlike native code in which a leaf method may be omitted).</p> <p>ARM and ARM64: Managed methods must always push LR on the stack, and create a minimal frame, so that the method can be properly hijacked using return address hijacking.</p>"},{"location":"clr-abi/#frame-pointer-chains","title":"Frame pointer chains","text":"<p>A frame pointer chain exists when the frame pointer register points to a location on the stack containing the address of the saved previous frame pointer value (from a caller of the current function). This chaining is required is certain scenarios, such as: 1. gdb debugger stack walking on Linux. 2. ETW event trace stack walking.</p> <p>There are two considerations: 1. Reserving the frame pointer register for stack walking, and not using it for other purposes, such as general-purpose code generation, and 2. Creating a frame chain.</p> <p>Note that even if a function is not added to the frame chain, as long as the function does not modify the frame pointer, the existing frame chain is still viable, although that function will not appear when walking the chain. The JIT may have reasons to create and use a frame pointer register even if a frame chain is not created, such as to access main function local variables within an exception handling funclet.</p> <p>The frame pointer register is, for each architecture: ARM: r11, ARM64: x29, x86: EBP, x64: RBP.</p> <p>The JIT creates frame chains most of the time for all platforms except Windows x64. Very simple functions may not get added to the frame chain, with the intent to improve performance by reducing frame setup cost (the heuristics for this choice are in <code>Compiler::rpMustCreateEBPFrame()</code>). For Windows x64, unwinding will always be done using the generated unwind codes, and not simple frame chain traversal.</p> <p>Some additional links: - See ARM64 JIT frame layout for documentation on that architecture's frame design. - The CoreCLR change to always create RBP chains on Unix x64 is here (issue with discussion here).</p>"},{"location":"clr-abi/#specialextra-parameters","title":"Special/extra parameters","text":""},{"location":"clr-abi/#the-this-pointer","title":"The <code>this</code> pointer","text":"<p>The managed <code>this</code> pointer is treated like a new kind of argument not covered by the native ABI, so we chose to always pass it as the first argument in (AMD64) <code>RCX</code> or (ARM, ARM64) <code>R0</code>.</p> <p>AMD64-only: Up to .NET Framework 4.5, the managed <code>this</code> pointer was treated just like the native <code>this</code> pointer (meaning it was the second argument when the call used a return buffer and was passed in RDX instead of RCX). Starting with .NET Framework 4.5, it is always the first argument.</p>"},{"location":"clr-abi/#varargs","title":"Varargs","text":"<p>Varargs refers to passing or receiving a variable number of arguments for a call.</p> <p>C# varargs, using the <code>params</code> keyword, are at the IL level just normal calls with a fixed number of parameters.</p> <p>Managed varargs (using C#'s pseudo-documented \"...\", <code>__arglist</code>, etc.) are implemented almost exactly like C++ varargs. The biggest difference is that the JIT adds a \"vararg cookie\" after the optional return buffer and the optional <code>this</code> pointer, but before any other user arguments. The callee must spill this cookie and all subsequent arguments into their home location, as they may be addressed via pointer arithmetic starting with the cookie as a base. The cookie happens to be to a pointer to a signature that the runtime can parse to (1) report any GC pointers within the variable portion of the arguments or (2) type-check (and properly walk over) any arguments extracted via ArgIterator. This is marked by <code>IMAGE_CEE_CS_CALLCONV_VARARG</code>, which should not be confused with <code>IMAGE_CEE_CS_CALLCONV_NATIVEVARARG</code>, which really is exactly native varargs (no cookie) and should only appear in PInvoke IL stubs, which properly handle pinning and other GC magic.</p> <p>On AMD64, just like native, any floating point arguments passed in floating point registers (including the fixed arguments) will be shadowed (i.e. duplicated) in the integer registers.</p> <p>On ARM and ARM64, just like native, nothing is put in the floating point registers.</p> <p>However, unlike native varargs, all floating point arguments are not promoted to double (<code>R8</code>), and instead retain their original type (<code>R4</code> or <code>R8</code>) (although this does not preclude an IL generator like managed C++ from explicitly injecting an upcast at the call-site and adjusting the call-site-sig appropriately). This leads to unexpected behavior when native C++ is ported to C# or even just managed via the different flavors of managed C++.</p> <p>Managed varargs are supported on Windows only.</p> <p>Managed/native varargs are supported on Windows only. Support for managed/native varargs on non-Windows platforms is tracked by this issue.</p>"},{"location":"clr-abi/#generics","title":"Generics","text":"<p>Shared generics. In cases where the code address does not uniquely identify a generic instantiation of a method, then a 'generic instantiation parameter' is required. Often the <code>this</code> pointer can serve dual-purpose as the instantiation parameter. When the <code>this</code> pointer is not the generic parameter, the generic parameter is passed as an additional argument. On ARM and AMD64, it is passed after the optional return buffer and the optional <code>this</code> pointer, but before any user arguments. On ARM64 and RISC-V, the generic parameter is passed after the optional <code>this</code> pointer, but before any user arguments. On x86, if all arguments of the function including <code>this</code> pointer fit into argument registers (ECX and EDX) and we still have argument registers available, we store the hidden argument in the next available argument register. Otherwise it is passed as the last stack argument. For generic methods (where there is a type parameter directly on the method, as compared to the type), the generic parameter currently is a MethodDesc pointer (I believe an InstantiatedMethodDesc). For static methods (where there is no <code>this</code> pointer) the generic parameter is a MethodTable pointer/TypeHandle.</p> <p>Sometimes the VM asks the JIT to report and keep alive the generics parameter. In this case, it must be saved on the stack someplace and kept alive via normal GC reporting (if it was the <code>this</code> pointer, as compared to a MethodDesc or MethodTable) for the entire method except the prolog and epilog. Also note that the code to home it, must be in the range of code reported as the prolog in the GC info (which probably isn't the same as the range of code reported as the prolog in the unwind info).</p> <p>There is no defined/enforced/declared ordering between the generic parameter and the varargs cookie because the runtime does not support that combination. There are chunks of code in the VM and JITs that would appear to support that, but other places assert and disallow it, so nothing is tested, and I would assume there are bugs and differences (i.e. one JIT using a different ordering than the other JIT or the VM).</p>"},{"location":"clr-abi/#example","title":"Example","text":"<pre><code>call([\"this\" pointer] [return buffer pointer] [generics context|varargs cookie] [userargs]*)\n</code></pre>"},{"location":"clr-abi/#async","title":"Async","text":"<p>Async calling convention is additive to other calling conventions when supported. The set of scenarios is constrained to regular static/virtual calls and does not, for example, support PInvokes or varargs. At the minimum ordinary static calls, calls with <code>this</code> parameter or generic hidden parameters are supported.</p> <p>Async calling convention adds an extra <code>Continuation</code> parameter and an extra return, which sematically takes precedence when not <code>null</code>. A non-null <code>Continuation</code> upon return signals that the computation is not complete and the formal result is not ready. A non-null argument means that the function is resuming and should extract the state from the <code>Continuation</code> and continue execution (while ignoring all other arguments).</p> <p>The <code>Continuation</code> is a managed object and needs to be tracked accordingly. The GC info includes the continuation result as live at Async call sites.</p>"},{"location":"clr-abi/#returning-continuation","title":"Returning <code>Continuation</code>","text":"<p>To return <code>Continuation</code> we use a volatile/calee-trash register that cannot be used to return the actual result.</p> arch <code>REG_ASYNC_CONTINUATION_RET</code> x86 ecx x64 rcx arm r2 arm64 x2 risc-v a2"},{"location":"clr-abi/#passing-continuation-argument","title":"Passing <code>Continuation</code> argument","text":"<p>The <code>Continuation</code> parameter is passed at the same position as generic instantiation parameter or immediately after, if both present. For x86 the argument order is reversed.</p> <pre><code>call([\"this\" pointer] [return buffer pointer] [generics context] [continuation] [userargs])   // not x86\n\ncall([\"this\" pointer] [return buffer pointer] [userargs] [continuation] [generics context])   // x86\n</code></pre>"},{"location":"clr-abi/#amd64-only-by-value-value-types","title":"AMD64-only: by-value value types","text":"<p>Just like native, AMD64 has implicit-byrefs. Any structure (value type in IL parlance) that is not 1, 2, 4, or 8 bytes in size (i.e., 3, 5, 6, 7, or &gt;= 9 bytes in size) that is declared to be passed by value, is instead passed by reference. For JIT generated code, it follows the native ABI where the passed-in reference is a pointer to a compiler generated temp local on the stack. However, there are some cases within remoting or reflection where apparently stackalloc is too hard, and so they pass in pointers within the GC heap, thus the JITed code must report these implicit byref parameters as interior pointers (BYREFs in JIT parlance), in case the callee is one of these reflection paths. Similarly, all writes must use checked write barriers.</p> <p>The AMD64 native calling conventions (Windows 64 and System V) require return buffer address to be returned by callee in RAX. JIT also follows this rule.</p>"},{"location":"clr-abi/#risc-v-only-structs-passedreturned-according-to-hardware-floating-point-calling-convention","title":"RISC-V only: structs passed/returned according to hardware floating-point calling convention","text":"<p>Passing/returning structs according to hardware floating-point calling convention like native is currently supported only up to 16 bytes, ones larger than that differ from the standard ABI and are passed/returned according to integer calling convention (by implicit reference).</p>"},{"location":"clr-abi/#return-buffers","title":"Return buffers","text":"<p>Since .NET 10, return buffers must always be allocated on the stack by the caller. After the call, the caller is responsible for copying the return buffer to the final destination using write barriers if necessary. The JIT can assume that the return buffer is always on the stack and may optimize accordingly, such as by omitting write barriers when writing GC pointers to the return buffer. In addition, the buffer is allowed to be used for temporary storage within the method since its content must not be aliased or cross-thread visible.</p> <p>ARM64-only: When a method returns a structure that is larger than 16 bytes the caller reserves a return buffer of sufficient size and alignment to hold the result. The address of the buffer is passed as an argument to the method in <code>R8</code> (defined in the JIT as <code>REG_ARG_RET_BUFF</code>). The callee isn't required to preserve the value stored in <code>R8</code>.</p>"},{"location":"clr-abi/#hidden-parameters","title":"Hidden parameters","text":"<p>Stub dispatch - when a virtual call uses a VSD stub, rather than back-patching the calling code (or disassembling it), the JIT must place the address of the stub used to load the call target, the \"stub indirection cell\", in (x86) <code>EAX</code> / (AMD64) <code>R11</code> / (ARM) <code>R4</code> / (ARM NativeAOT ABI) <code>R12</code> / (ARM64) <code>R11</code>. In the JIT, this is encapsulated in the <code>VirtualStubParamInfo</code> class.</p> <p>Calli Pinvoke - The VM wants the address of the PInvoke in (AMD64) <code>R10</code> / (ARM) <code>R12</code> / (ARM64) <code>R14</code> (In the JIT: <code>REG_PINVOKE_TARGET_PARAM</code>), and the signature (the pinvoke cookie) in (AMD64) <code>R11</code> / (ARM) <code>R4</code> / (ARM64) <code>R15</code> (in the JIT: <code>REG_PINVOKE_COOKIE_PARAM</code>).</p> <p>Normal PInvoke - The VM shares IL stubs based on signatures, but wants the right method to show up in call stack and exceptions, so the MethodDesc for the exact PInvoke is passed in the (x86) <code>EAX</code> / (AMD64) <code>R10</code> / (ARM, ARM64) <code>R12</code> (in the JIT: <code>REG_SECRET_STUB_PARAM</code>). Then in the IL stub, when the JIT gets <code>CORJIT_FLG_PUBLISH_SECRET_PARAM</code>, it must move the register into a compiler temp. The value is returned for the intrinsic <code>NI_System_StubHelpers_GetStubContext</code>.</p>"},{"location":"clr-abi/#small-primitive-returns","title":"Small primitive returns","text":"<p>Primitive value types smaller than 32-bits are widened to 32-bits: signed small types are sign extended and unsigned small types are zero extended. This can be different from the standard calling conventions that may leave the state of unused bits in the return register undefined.</p>"},{"location":"clr-abi/#small-primitive-arguments","title":"Small primitive arguments","text":"<p>Small primitive arguments have undefined upper bits. This can be different from the standard calling conventions that may require normalization (e.g. on ARM32 and Apple ARM64).</p> <p>On RISC-V small primitive arguments are extended according to standard calling conventions.</p>"},{"location":"clr-abi/#pinvokes","title":"PInvokes","text":"<p>The convention is that any method with an InlinedCallFrame (either an IL stub or a normal method with an inlined PInvoke) saves/restores all non-volatile integer registers in its prolog/epilog respectively. This is done so that the InlinedCallFrame can just contain a return address, a stack pointer and a frame pointer. Then using just those three it can start a full stack walk using the normal RtlVirtualUnwind.</p> <p>When encountering a PInvoke, the JIT will query the VM if the GC transition should be suppressed. Suppression of the GC transition is indicated by the addition of an attribute on the PInvoke definition. If the VM indicates the GC transition is to be suppressed, the PInvoke frame will be omitted in either the IL stub or inlined scenario and a GC Poll will be inserted near the unmanaged call site. If an enclosing function contains more than one inlined PInvoke but not all have requested a suppression of the GC transition a PInvoke frame will still be constructed for the other inlined PInvokes.</p> <p>For AMD64, a method with an InlinedCallFrame must use RBP as the frame register.</p> <p>For ARM and ARM64, we will also always use a frame pointer (R11). That is partially due to the frame chaining requirement. However, the VM also requires it for PInvokes with InlinedCallFrames.</p> <p>For ARM, the VM also has a dependency on <code>REG_SAVED_LOCALLOC_SP</code>.</p> <p>All these dependencies show up in the implementation of <code>InlinedCallFrame::UpdateRegDisplay</code>.</p> <p>JIT32 only generates one epilog (and causes all returns to branch to it) when there are PInvokes/InlinedCallFrame in the current method.</p>"},{"location":"clr-abi/#per-frame-pinvoke-initialization","title":"Per-frame PInvoke initialization","text":"<p>The InlinedCallFrame is initialized once at the head of IL stubs and once in each path that does an inlined PInvoke.</p> <p>In JIT64 this happens in blocks that actually contain calls, but pushing it out of loops that have landing pads, and then looking for dominator blocks. For IL stubs and methods with EH, we give up and place the initialization in the first block.</p> <p>In RyuJIT/JIT32 (ARM), all methods are treated like JIT64's IL stubs (meaning the per-frame initialization happens once just after the prolog).</p> <p>The JIT generates a call to <code>CORINFO_HELP_INIT_PINVOKE_FRAME</code> passing the address of the InlinedCallFrame and either NULL or the secret parameter for IL stubs. <code>JIT_InitPInvokeFrame</code> initializes the InlinedCallFrame and sets it to point to the current Frame chain top. Then it returns the current thread's native Thread object.</p> <p>On AMD64, the JIT generates code to save RSP and RBP into the InlinedCallFrame.</p> <p>For IL stubs only, the per-frame initialization includes setting <code>Thread-&gt;m_pFrame</code> to the InlinedCallFrame (effectively 'pushing' the Frame).</p>"},{"location":"clr-abi/#per-call-site-pinvoke-work","title":"Per-call-site PInvoke work","text":"<p>The below is performed when the GC transition is not suppressed.</p> <ol> <li>For direct calls, the JITed code sets <code>InlinedCallFrame-&gt;m_pDatum</code> to the MethodDesc of the call target.<ul> <li>For JIT64, indirect calls within IL stubs sets it to the secret parameter (this seems redundant, but it might have changed since the per-frame initialization?).</li> <li>For JIT32 (ARM) indirect calls, it sets this member to the size of the pushed arguments, according to the comments. The implementation however always passed 0.</li> </ul> </li> <li>For JIT64/AMD64 only: Next for non-IL stubs, the InlinedCallFrame is 'pushed' by setting <code>Thread-&gt;m_pFrame</code> to point to the InlinedCallFrame (recall that the per-frame initialization already set <code>InlinedCallFrame-&gt;m_pNext</code> to point to the previous top). For IL stubs this step is accomplished in the per-frame initialization.</li> <li>The Frame is made active by setting <code>InlinedCallFrame-&gt;m_pCallerReturnAddress</code>.</li> <li>The code then toggles the GC mode by setting <code>Thread-&gt;m_fPreemptiveGCDisabled = 0</code>.</li> <li>Starting now, no GC pointers may be live in registers. RyuJit LSRA meets this requirement by adding special refPosition <code>RefTypeKillGCRefs</code> before unmanaged calls and special helpers.</li> <li>Then comes the actual call/PInvoke.</li> <li>The GC mode is set back by setting <code>Thread-&gt;m_fPreemptiveGCDisabled = 1</code>.</li> <li>Then we check to see if <code>g_TrapReturningThreads</code> is set (non-zero). If it is, we call <code>CORINFO_HELP_STOP_FOR_GC</code>.<ul> <li>For ARM, this helper call preserves the return register(s): <code>R0</code>, <code>R1</code>, <code>S0</code>, and <code>D0</code>.</li> <li>For AMD64, the generated code must manually preserve the return value of the PInvoke by moving it to a non-volatile register or a stack location.</li> </ul> </li> <li>Starting now, GC pointers may once again be live in registers.</li> <li>Clear the <code>InlinedCallFrame-&gt;m_pCallerReturnAddress</code> back to 0.</li> <li>For JIT64/AMD64 only: For non-IL stubs 'pop' the Frame chain by resetting <code>Thread-&gt;m_pFrame</code> back to <code>InlinedCallFrame.m_pNext</code>.</li> </ol> <p>Saving/restoring all the non-volatile registers helps by preventing any registers that are unused in the current frame from accidentally having a live GC pointer value from a parent frame. The argument and return registers are 'safe' because they cannot be GC refs. Any refs should have been pinned elsewhere and instead passed as native pointers.</p> <p>For IL stubs, the Frame chain isn't popped at the call site, so instead it must be popped right before the epilog and right before any jmp calls. It looks like we do not support tail calls from PInvoke IL stubs?</p>"},{"location":"clr-abi/#exception-handling","title":"Exception handling","text":"<p>This section describes the conventions the JIT needs to follow when generating code to implement managed exception handling (EH). The JIT and VM must agree on these conventions for a correct implementation.</p>"},{"location":"clr-abi/#funclets","title":"Funclets","text":"<p>For all platforms, managed EH handlers (finally, fault, filter, filter-handler, and catch) are extracted into their own 'funclets'. To the OS they are treated just like first class functions (separate PDATA and XDATA (<code>RUNTIME_FUNCTION</code> entry), etc.). The CLR currently treats them just like part of the parent function in many ways. The main function and all funclets must be allocated in a single code allocation (see hot cold splitting). They 'share' GC info. Only the main function prolog can be hot patched.</p> <p>The only way to enter a handler funclet is via a call. In the case of an exception, the call is from the VM's EH subsystem as part of exception dispatch/unwind. In the non-exceptional case, this is called local unwind or a non-local exit. In C# this is accomplished by simply falling-through/out of a try body or an explicit goto. In IL this is always accomplished via a LEAVE opcode, within a try body, targeting an IL offset outside the try body. In such cases the call is from the JITed code of the parent function.</p>"},{"location":"clr-abi/#cloned-finallys","title":"Cloned finallys","text":"<p>RyuJIT attempts to speed the normal control flow by 'inlining' a called finally along the 'normal' control flow (i.e., leaving a try body in a non-exceptional manner via C# fall-through). This optimization is supported on all architectures.</p>"},{"location":"clr-abi/#invoking-finallysnon-local-exits","title":"Invoking Finallys/Non-local exits","text":"<p>In order to have proper forward progress and <code>Thread.Abort</code> semantics, there are restrictions on where a call-to-finally can be, and what the call site must look like. The return address can NOT be in the corresponding try body (otherwise the VM would think the finally protects itself). The return address MUST be within any outer protected region (so exceptions from the finally body are properly handled).</p> <p>RyuJIT creates something similar to a jump island: a block of code outside the try body that calls the finally and then branches to the final target of the leave/non-local-exit. This jump island is then marked in the EH tables as if it were a cloned finally. The cloned finally clause prevents a Thread.Abort from firing before entering the handler. By having the return address outside of the try body we satisfy the other constraint.</p>"},{"location":"clr-abi/#threadabortexception-considerations","title":"ThreadAbortException considerations","text":"<p>There are three kinds of thread abort: (1) rude thread abort, that cannot be stopped, and doesn't run (all?) handlers, (2) calls to the <code>Thread.Abort()</code> api, and (3) asynchronous thread abort, injected from another thread.</p> <p>Note that ThreadAbortException is fully available in the desktop framework, and is heavily used in ASP.NET, for example. However, it is not supported in .NET Core, CoreCLR, or the Windows 8 \"modern app profile\". Nonetheless, the JIT generates ThreadAbort-compatible code on all platforms.</p> <p>For non-rude thread abort, the VM walks the stack, running any catch handler that catches ThreadAbortException (or a parent, like System.Exception, or System.Object), and running finallys. There is one very particular characteristic of ThreadAbortException: if a catch handler has caught ThreadAbortException, and the handler returns from handling the exception without calling Thread.ResetAbort(), then the VM automatically re-raises ThreadAbortException. To do so, it uses the resume address that the catch handler returned as the effective address where the re-raise is considered to have been raised. This is the address of the label that is specified by a LEAVE opcode within the catch handler. There are cases where the JIT must insert synthetic \"step blocks\" such that this label is within an appropriate enclosing \"try\" region, to ensure that the re-raise can be caught by an enclosing catch handler.</p> <p>For example:</p> <pre><code>try { // try 1\n    try { // try 2\n        System.Threading.Thread.CurrentThread.Abort();\n    } catch (System.Threading.ThreadAbortException) { // catch 2\n        ...\n        LEAVE L;\n    }\n} catch (System.Exception) { // catch 1\n     ...\n}\nL:\n</code></pre> <p>In this case, if the address returned in catch 2 corresponding to label L is outside try 1, then the ThreadAbortException re-raised by the VM will not be caught by catch 1, as is expected. The JIT needs to insert a block such that this is the effective code generation:</p> <pre><code>try { // try 1\n    try { // try 2\n        System.Threading.Thread.CurrentThread.Abort();\n    } catch (System.Threading.ThreadAbortException) { // catch 2\n        ...\n        LEAVE L';\n    }\n    L': LEAVE L;\n} catch (System.Exception) { // catch 1\n     ...\n}\nL:\n</code></pre> <p>Similarly, the automatic re-raise address for a ThreadAbortException can't be within a finally handler, or the VM will abort the re-raise and swallow the exception. This can happen due to call-to-finally thunks marked as \"cloned finally\", as described above. For example (this is pseudo-assembly-code, not C#):</p> <pre><code>try { // try 1\n    try { // try 2\n        System.Threading.Thread.CurrentThread.Abort();\n    } catch (System.Threading.ThreadAbortException) { // catch 2\n        ...\n        LEAVE L;\n    }\n} finally { // finally 1\n     ...\n}\nL:\n</code></pre> <p>This would generate something like:</p> <pre><code>    // beginning of 'try 1'\n    // beginning of 'try 2'\n    System.Threading.Thread.CurrentThread.Abort();\n    // end of 'try 2'\n    // beginning of call-to-finally 'cloned finally' region\nL1: call finally1\n    nop\n    // end of call-to-finally 'cloned finally' region\n    // end of 'try 1'\n    // function epilog\n    ret\n\nCatch2:\n    // do something\n    lea rax, &amp;L1; // load up resume address\n    ret\n\nFinally1:\n    // do something\n    ret\n</code></pre> <p>Note that the JIT must already insert a \"step\" block so the finally will be called. However, this isn't sufficient to support ThreadAbortException processing, because \"L1\" is marked as \"cloned finally\". In this case, the JIT must insert another step block that is within \"try 1\" but outside the cloned finally block, that will allow for correct re-raise semantics. For example:</p> <pre><code>    // beginning of 'try 1'\n    // beginning of 'try 2'\n    System.Threading.Thread.CurrentThread.Abort();\n    // end of 'try 2'\nL1':    nop\n    // beginning of call-to-finally 'cloned finally' region\nL1: call finally1\n    nop\n    // end of call-to-finally 'cloned finally' region\n    // end of 'try 1'\n    // function epilog\n    ret\n\nCatch2:\n    // do something\n    lea rax, &amp;L1'; // load up resume address\n    ret\n\nFinally1:\n    // do something\n    ret\n</code></pre> <p>Note that JIT64 does not implement this properly. The C# compiler used to always insert all necessary \"step\" blocks. The Roslyn C# compiler at one point did not, but then was changed to once again insert them.</p>"},{"location":"clr-abi/#funclet-parameters","title":"Funclet parameters","text":"<p>Catch, Filter, and Filter-handlers get an Exception object (GC ref) as an argument (<code>REG_EXCEPTION_OBJECT</code>). On AMD64 it is passed in RCX (Windows ABI) or RSI (Unix ABI). On ARM and ARM64 this is the first argument and passed in R0.</p>"},{"location":"clr-abi/#funclet-return-values","title":"Funclet Return Values","text":"<p>The filter funclet returns a simple boolean value in the normal return register (x86: <code>EAX</code>, AMD64: <code>RAX</code>, ARM/ARM64: <code>R0</code>). Non-zero indicates to the VM/EH subsystem that the corresponding filter-handler will handle the exception (i.e. begin the second pass). Zero indicates to the VM/EH subsystem that the exception is not handled, and it should continue looking for another filter or catch.</p> <p>The catch and filter-handler funclets return a code address in the normal return register that indicates where the VM should resume execution after unwinding the stack and cleaning up from the exception. This address should be somewhere in the parent funclet (or main function if the catch or filter-handler is not nested within any other funclet). Because an IL 'leave' opcode can exit out of arbitrary nesting of funclets and try bodies, the JIT is often required to inject step blocks. These are intermediate branch target(s) that then branch to the next outermost target until the real target can be directly reached via the native ABI constraints. These step blocks can also invoke finallys (see Invoking Finallys/Non-local exits).</p> <p>Finally and fault funclets do not have a return value.</p>"},{"location":"clr-abi/#register-values-and-exception-handling","title":"Register values and exception handling","text":"<p>Exception handling imposes certain restrictions on the usage of registers in functions with exception handling.</p> <p>CoreCLR and \"desktop\" CLR behave the same way. Windows and non-Windows implementations of the CLR both follow these rules.</p> <p>Some definitions:</p> <p>Non-volatile (aka callee-saved or preserved) registers are those defined by the ABI that a function call preserves. Non-volatile registers include the frame pointer and the stack pointer, among others.</p> <p>Volatile (aka caller-saved or trashed) registers are those defined by the ABI that a function call does not preserve, and thus might have a different value when the function returns.</p>"},{"location":"clr-abi/#registers-on-entry-to-a-funclet","title":"Registers on entry to a funclet","text":"<p>When an exception occurs, the VM is invoked to do some processing. If the exception is within a \"try\" region, it eventually calls a corresponding handler (which also includes calling filters). The exception location within a function might be where a \"throw\" instruction executes, the point of a processor exception like null pointer dereference or divide by zero, or the point of a call where the callee threw an exception but did not catch it.</p> <p>The VM sets the frame register to be the same as the parent function. This allows the funclets to access local variables using frame-relative addresses.</p> <p>For filter funclets, all other register values that existed at the exception point in the corresponding \"try\" region are trashed on entry to the funclet. That is, the only registers that have known values are those of the funclet parameters and the frame register.</p> <p>For other funclets, all non-volatile registers are restored to their values at the exception point. The JIT codegen does not take advantage of it currently.</p>"},{"location":"clr-abi/#registers-on-return-from-a-funclet","title":"Registers on return from a funclet","text":"<p>When a funclet finishes execution, and the VM returns execution to the function (or an enclosing funclet, if there is EH clause nesting), the non-volatile registers are restored to the values they held at the exception point. Note that the volatile registers have been trashed.</p> <p>Any register value changes made in the funclet are lost. If a funclet wants to make a variable change known to the main function (or the funclet that contains the \"try\" region), that variable change needs to be made to the shared main function stack frame. This not a fundamental limitation. If necessary, the runtime can be updated to preserve non-volatile register changes made in funclets.</p> <p>Funclets are not required to preserve non-volatile registers.</p>"},{"location":"clr-abi/#eh-info-gc-info-and-hot-cold-splitting","title":"EH Info, GC Info, and Hot &amp; Cold Splitting","text":"<p>All GC info offsets and EH info offsets treat the function and funclets as if it was one big method body. Thus all offsets are relative to the start of the main method. Funclets are assumed to always be at the end of (after) all of the main function code. Thus if the main function has any cold code, all funclets must be cold. Or conversely, if there is any hot funclet code, all of the main method must be hot.</p>"},{"location":"clr-abi/#eh-clause-ordering","title":"EH clause ordering","text":"<p>EH clauses must be sorted inner-to-outer, first-to-last based on IL offset of the try start/try end pair. The only exceptions are cloned finallys, which always appear at the end.</p>"},{"location":"clr-abi/#how-eh-affects-gc-inforeporting","title":"How EH affects GC info/reporting","text":"<p>Because a main function body will always be on the stack when one of its funclets is on the stack, the GC info must be careful not to double-report. JIT64 accomplished this by having all named locals appear in the parent method frame, anything shared between the function and funclets was homed to the stack, and only the parent function reported stack locals (funclets might report local registers). JIT32 and RyuJIT (for AMD64, ARM, and ARM64) take the opposite direction. The leaf-most funclet is responsible for reporting everything that might be live out of a funclet (in the case of a filter, this might resume back in the original method body). This is accomplished with the GC header flag WantsReportOnlyLeaf (JIT32 and RyuJIT set it, JIT64 doesn't) and the VM tracking if it has already seen a funclet for a given frame. Once JIT64 is fully retired, we should be able to remove this flag from GC info.</p> <p>There is one \"corner case\" in the VM implementation of WantsReportOnlyLeaf model that has implications for the code the JIT is allowed to generate. Consider this function with nested exception handling:</p> <pre><code>public void runtest() {\n    try {\n        try {\n            throw new UserException3(ThreadId); // 1\n        }\n        catch (UserException3 e){\n            Console.WriteLine(\"Exception3 was caught\");\n            throw new UserException4(ThreadId);\n        }\n    }\n    catch (UserException4 e) { // 2\n        Console.WriteLine(\"Exception4 was caught\");\n    }\n}\n</code></pre> <p>When the inner \"throw new UserException4\" is executed, the exception handling first pass finds that the outer catch handler will handle the exception. The exception handling second pass unwinds stack frames back to the \"runtest\" frame, and then executes the catch handler. There is a period of time during which the original catch handler (\"catch (UserException3 e)\") is no longer on the stack, but before the new catch handler is executed. During this time, a GC might occur. In this case, the VM needs to make sure to report GC roots properly for the \"runtest\" function. The inner catch has been unwound, so we can't report that. We don't want to report at \"// 1\", which is still on the stack, because that effectively is \"going backwards\" in execution, and doesn't properly represent what object references are live. We need to report live object references at the next location where execution will occur. This is the \"// 2\" location. However, we can't report the first location of the catch funclet, as that will be non-interruptible. The VM instead looks forward for the first interruptible point in that handler, and reports live references that the JIT reports for that location. This will be the first location after the handler prolog. There are several implications of this implementation for the JIT. It requires that:</p> <ol> <li>Methods which have EH clauses are fully interruptible.</li> <li>All catch funclets have an interruptible point immediately after the prolog.</li> <li>The first interruptible point in the catch funclet reports the following live objects on the stack<ul> <li>Only objects that are shared with parent method i.e. no additional stack object which is live only in catch funclet and not live in parent method.</li> <li>All shared objects which are referenced in catch funclet and any subsequent control flow are reported live.</li> </ul> </li> </ol>"},{"location":"clr-abi/#filter-gc-semantics","title":"Filter GC semantics","text":"<p>Filters are invoked in the 1st pass of EH processing and as such execution might resume back at the faulting address, or in the filter-handler, or someplace else. Because the VM must allow GC's to occur during and after a filter invocation, but before the EH subsystem knows where it will resume, we need to keep everything alive at both the faulting address and within the filter. This is accomplished by 3 means: (1) the VM's stackwalker and GCInfoDecoder report as live both the filter frame and its corresponding parent frame, (2) the JIT encodes all stack slots that are live within the filter as being pinned, and (3) the JIT reports as live (and possible zero-initializes) anything live-out of the filter. Because of (1) it is likely that a stack variable that is live within the filter and the try body will be double reported. During the mark phase of the GC double reporting is not a problem. The problem only arises if the object is relocated: if the same location is reported twice, the GC will try to relocate the address stored at that location twice. Thus we prevent the object from being relocated by pinning it, which leads us to why we must do (2). (3) is done so that after the filter returns, we can still safely incur a GC before executing the filter-handler or any outer handler within the same frame. For the same reason, control must exit a filter region via its final block (in other words, a filter region must terminate with the instruction that leaves the filter region, and the program may not exit the filter region via other paths).</p>"},{"location":"clr-abi/#clauses-covering-the-same-try-region","title":"Clauses covering the same try region","text":"<p>Several consecutive clauses may cover the same <code>try</code> block. A clause covering the same region as the previous one is marked by the <code>COR_ILEXCEPTION_CLAUSE_SAMETRY</code> flag. When exception ex1 is thrown while running handler for another exception ex2 and the exception ex2 escapes the ex1's handler frame, this enables the runtime to skip clauses that cover the same <code>try</code> block as the clause that handled the ex1. This flag is used by the NativeAOT and also a new exception handling mechanism in CoreCLR. The NativeAOT doesn't store that flag in the encoded clause data, but rather injects a dummy clause between the clauses with same <code>try</code> block. CoreCLR keeps that flag as part of the runtime representation of the clause data. The current CoreCLR exception handling doesn't use it, but a new exception handling mechanism that's being developed is taking advantage of it.</p>"},{"location":"clr-abi/#gc-interruptibility-and-eh","title":"GC Interruptibility and EH","text":"<p>The VM assumes that anytime a thread is stopped, it must be at a GC safe point, or the current frame is non-resumable (i.e. a throw that will never be caught in the same frame). Thus effectively all methods with EH must be fully interruptible (or at a minimum all try bodies). Currently the GC info appears to support mixing of partially interruptible and fully-interruptible regions within the same method, but no JIT uses this, so use at your own risk.</p> <p>The debugger always wants to stop at GC safe points, and thus debuggable code should be fully interruptible to maximize the places where the debugger can safely stop. If the JIT creates non-interruptible regions within fully interruptible code, the code should ensure that each sequence point begins on an interruptible instruction.</p> <p>AMD64/JIT64 only: The JIT will add an interruptible NOP if needed.</p>"},{"location":"clr-abi/#security-object","title":"Security Object","text":"<p>The security object is a GC pointer and must be reported as such, and kept alive the duration of the method.</p>"},{"location":"clr-abi/#gs-cookie","title":"GS Cookie","text":"<p>The GS Cookie is not a GC object, but still needs to be reported. It can only have one lifetime due to how it is encoded/reported in the GC info. Since the GS Cookie ceases being valid once we pop the stack, the epilog cannot be part of the live range. Since we only get one live range that means there cannot be any code (except funclets) after the epilog in methods with a GS cookie.</p>"},{"location":"clr-abi/#nops-and-other-padding","title":"NOPs and other Padding","text":""},{"location":"clr-abi/#amd64-padding-info","title":"AMD64 padding info","text":"<p>The unwind callbacks don't know if the current frame is a leaf or a return address. Consequently, the JIT must ensure that the return address of a call is in the same region as the call. Specifically, the JIT must add a NOP (or some other instruction) after any call that otherwise would directly precede the start of a try body, the end of a try body, or the end of a method.</p> <p>The OS has an optimization in the unwinder such that if an unwind results in a PC being within (or at the start of) an epilog, it assumes that frame is unimportant and unwinds again. Since the CLR considers every frame important, it does not want this double-unwind behavior and requires the JIT to place a NOP (or other instruction) between any call and any epilog.</p>"},{"location":"clr-abi/#arm-and-arm64-padding-info","title":"ARM and ARM64 padding info","text":"<p>The OS unwinder uses the <code>RUNTIME_FUNCTION</code> extents to determine which function or funclet to unwind out of. The net result is that a call (bl opcode) to <code>IL_Throw</code> cannot be the last thing. So similar to AMD64 the JIT must inject an opcode (a breakpoint in this case) when the <code>bl IL_Throw</code> would otherwise be the last opcode of a function or funclet, the last opcode before the end of the hot section, or (this might be an x86-ism leaking into ARM) the last before a \"special throw block\".</p>"},{"location":"clr-abi/#profiler-hooks","title":"Profiler Hooks","text":"<p>If the JIT gets passed <code>CORJIT_FLG_PROF_ENTERLEAVE</code>, then the JIT might need to insert native entry/exit/tail call probes. To determine for sure, the JIT must call GetProfilingHandle. This API returns as out parameters, the true dynamic boolean indicating if the JIT should actually insert the probes and a parameter to pass to the callbacks (typed as void*), with an optional indirection (used for NGEN). This parameter is always the first argument to all of the call-outs (thus placed in the usual first argument register <code>RCX</code> (AMD64) or <code>R0</code> (ARM, ARM64)).</p> <p>Outside of the prolog (in a GC interruptible location), the JIT injects a call to <code>CORINFO_HELP_PROF_FCN_ENTER</code>. For AMD64,  on Windows all argument registers will be homed into their caller-allocated stack locations (similar to varargs), on Unix all argument registers will be stored in the inner structure. For ARM and ARM64, all arguments are prespilled (again similar to varargs).</p> <p>After computing the return value and storing it in the correct register, but before any epilog code (including before a possible GS cookie check), the JIT injects a call to <code>CORINFO_HELP_PROF_FCN_LEAVE</code>. For AMD64 this call must preserve the return register: <code>RAX</code> or <code>XMM0</code> on Windows and <code>RAX</code> and <code>RDX</code> or <code>XMM0</code> and <code>XMM1</code> on Unix. For ARM, the return value will be moved from <code>R0</code> to <code>R2</code> (if it was in <code>R0</code>), <code>R1</code>, <code>R2</code>, and <code>S0/D0</code> must be preserved by the callee (longs will be <code>R2</code>, <code>R1</code> - note the unusual ordering of the registers, floats in <code>S0</code>, doubles in <code>D0</code>, smaller integrals in <code>R2</code>).</p> <p>TODO: describe ARM64 profile leave conventions.</p> <p>Before the argument setup (but after any argument side-effects) for any tail calls or jump calls, the JIT injects a call to <code>CORINFO_HELP_PROF_FCN_TAILCALL</code>. Note that it is NOT called for self-recursive tail calls turned into loops.</p> <p>For ARM tail calls, the JIT actually loads the outgoing arguments first, and then just before the profiler call-out, spills the argument in <code>R0</code> to another non-volatile register, makes the call (passing the callback parameter in <code>R0</code>), and then restores <code>R0</code>.</p> <p>For AMD64, all probes receive a second parameter (passed in <code>RDX</code> according to the default argument rules) which is the address of the start of the arguments' home location (equivalent to the value of the caller's stack pointer).</p> <p>TODO: describe ARM64 tail call convention.</p> <p>On Linux/x86 the profiling hooks are declared with the <code>__cdecl</code> attribute.  In cdecl (which stands for C declaration), subroutine arguments are passed on the stack. Integer values and memory addresses are returned in the EAX register, floating point values in the ST0 x87 register. Registers EAX, ECX, and EDX are caller-saved, and the rest are callee-saved. The x87 floating point registers ST0 to ST7 must be empty (popped or freed) when calling a new function, and ST1 to ST7 must be empty on exiting a function. ST0 must also be empty when not used for returning a value. Returned values of managed-code are formed before the leave/tailcall profiling hooks, so they should be saved in these hooks and restored on returning from them. The instruction <code>ret</code> for assembler implementations of profiling hooks should be without a parameter.</p> <p>JIT32 only generates one epilog (and causes all returns to branch to it) when there are profiler hooks.</p>"},{"location":"clr-abi/#synchronized-methods","title":"Synchronized Methods","text":"<p>JIT32/RyuJIT only generates one epilog (and causes all returns to branch to it) when a method is synchronized. See <code>Compiler::fgAddSyncMethodEnterExit()</code>. The user code is wrapped in a try/finally. Outside/before the try body, the code initializes a boolean to false. <code>CORINFO_HELP_MON_ENTER</code> or <code>CORINFO_HELP_MON_ENTER_STATIC</code> are called, passing the lock object (the <code>this</code> pointer for instance methods or the Type object for static methods) and the address of the boolean. If the lock is acquired, the boolean is set to true (as an 'atomic' operation in the sense that a Thread.Abort/EH/GC/etc. cannot interrupt the Thread when the boolean does not match the acquired state of the lock). JIT32/RyuJIT follows the exact same logic and arguments for placing the call to <code>CORINFO_HELP_MON_EXIT</code> /  <code>CORINFO_HELP_MON_EXIT_STATIC</code> in the finally.</p>"},{"location":"clr-abi/#rejit","title":"Rejit","text":"<p>For AMD64 to support profiler attach scenarios, the JIT can be required to ensure every generated method is hot patchable (see <code>CORJIT_FLG_PROF_REJIT_NOPS</code>). The way we do this is to ensure that the first 5 bytes of code are non-interruptible and there is no branch target within those bytes (includes calls/returns). Thus the VM can stop all threads (like for a GC) and safely replace those 5 bytes with a branch to a new version of the method (presumably instrumented by a profiler). The JIT adds NOPs or increases the size of the prolog reported in the GC info to accomplish these 2 requirements.</p> <p>In a function with exception handling, only the main function is affected; the funclet prologs are not made hot patchable.</p>"},{"location":"clr-abi/#edit-and-continue","title":"Edit and Continue","text":"<p>Edit and Continue (EnC) is a special flavor of un-optimized code. The debugger has to be able to reliably remap a method state (instruction pointer and local variables) from original method code to edited method code. This puts constraints on the method stack layout performed by the JIT. The key constraint is that the addresses of the existing locals must stay the same after the edit. This constraint is required because the address of the local could have been stored in the method state.</p> <p>In the current design, the JIT does not have access to the previous versions of the method and so it has to assume the worst case. EnC is designed for simplicity, not for performance of the generated code.</p> <p>EnC is currently enabled on x86, x64 and ARM64 only, but the same principles would apply if it is ever enabled on other platforms.</p> <p>The following sections describe the various Edit and Continue code conventions that must be followed.</p>"},{"location":"clr-abi/#enc-flag-in-gcinfo","title":"EnC flag in GCInfo","text":"<p>The JIT records the fact that it has followed conventions for EnC code in GC Info. On x64/ARM64, this flag is implied by recording the size of the stack frame region preserved between EnC edits (<code>GcInfoEncoder::SetSizeOfEditAndContinuePreservedArea</code>). For x64 the size of this region is increased to include <code>RSI</code> and <code>RDI</code>, so that <code>rep stos</code> can be used for block initialization and block moves. ARM64 saves only the FP and LR registers when EnC is enabled and does not use other callee saved registers.</p> <p>To successfully perform EnC transitions the runtime needs to know the size of the stack frames it is transitioning between. For x64 code the size can be extracted from unwind codes. This is not possible for arm64 code as the frames are set up in such a way that the unwind codes do not allow to retrieve this value. Therefore, on ARM64 the GC info contains also the size of the fixed stack frame to be used for EnC purposes.</p>"},{"location":"clr-abi/#allocating-local-variables-backward","title":"Allocating local variables backward","text":"<p>This is required to preserve addresses of the existing locals when an EnC edit appends new ones. In other words, the first local must be allocated at the highest stack address. Special care has to be taken to deal with alignment. The total size of the method frame can either grow (more locals added) or shrink (fewer temps needed) after the edit. The VM zeros out newly added locals.</p>"},{"location":"clr-abi/#fixed-set-of-callee-saved-registers","title":"Fixed set of callee-saved registers","text":"<p>This eliminates need to deal with the different sets in the VM, and makes preservation of local addresses easier. There are plenty of volatile registers and so lack of non-volatile registers does not heavily impact quality of non-optimized code. x64 currently saves RBP, RSI and RDI while ARM64 saves just FP and LR.</p>"},{"location":"clr-abi/#enc-is-supported-for-methods-with-eh","title":"EnC is supported for methods with EH","text":"<p>However, EnC remap is not supported inside funclets. The stack layout of funclets does not matter for EnC.</p>"},{"location":"clr-abi/#localloc","title":"Localloc","text":"<p>Localloc is allowed in EnC code, but remap is disallowed after the method has executed a localloc instruction. VM uses the invariants above (<code>RSP == RBP</code> on x64, <code>FP + 16 == SP + stack size</code> on ARM64) to detect whether localloc was executed by the method.</p>"},{"location":"clr-abi/#security-object_1","title":"Security object","text":"<p>This does not require any special handling by the JIT on x64/arm64. (Different from x86). The security object is copied over by the VM during remap if necessary. Location of security object is found via GC info.</p>"},{"location":"clr-abi/#synchronized-methods_1","title":"Synchronized methods","text":"<p>The extra state created by the JIT for synchronized methods (lock taken flag) must be preserved during remap. The JIT stores this state in the preserved region, and increases the size of the preserved region reported in GC info accordingly.</p>"},{"location":"clr-abi/#generics_1","title":"Generics","text":"<p>EnC is supported for adding and editing generic methods and methods on generic types and generic methods on non-generic types.</p>"},{"location":"clr-abi/#system-v-x86_64-support","title":"System V x86_64 support","text":"<p>This section relates mostly to calling conventions on System V systems (such as Ubuntu Linux and Mac OS X). The general rules outlined in the System V x86_64 ABI documentation are followed with a few exceptions, described below:</p> <ol> <li>The hidden argument for by-value passed structs is always after the <code>this</code> parameter (if there is one). This is a difference with the System V ABI and affects only the internal JIT calling conventions. For PInvoke calls the hidden argument is always the first parameter since there is no <code>this</code> parameter in this case (except for the <code>CallConvMemberFunction</code> case).</li> <li>Managed structs that have no fields are always passed by-value on the stack.</li> <li>The JIT proactively generates frame register frames (with <code>RBP</code> as a frame register) in order to aid the native OS tooling for stack unwinding and the like.</li> <li>All the other internal VM contracts for PInvoke, EH, and generic support remains in place. Please see the relevant sections above for more details. Note, however, that the registers used are different on System V due to the different calling convention. For example, the integer argument registers are, in order, RDI, RSI, RDX, RCX, R8, and R9. Thus, where the first argument (typically, the <code>this</code> pointer) on Windows AMD64 goes in RCX, on System V it goes in RDI, and so forth.</li> <li>Structs with explicit layout are always passed by value on the stack.</li> <li>The following table describes register usage according to the System V x86_64 ABI</li> </ol> <pre><code>| Register      | Usage                                   | Preserved across  |\n|               |                                         | function calls    |\n|---------------|-----------------------------------------|-------------------|\n| %rax          | temporary register; with variable argu- | No                |\n|               | ments passes information about the      |                   |\n|               | number of SSE registers used;           |                   |\n|               | 1st return argument                     |                   |\n| %rbx          | callee-saved register; optionally used  | Yes               |\n|               | as base pointer                         |                   |\n| %rcx          | used to pass 4st integer argument to    | No                |\n|               | to functions                            |                   |\n| %rdx          | used to pass 3rd argument to functions  | No                |\n|               | 2nd return register                     |                   |\n| %rsp          | stack pointer                           | Yes               |\n| %rbp          | callee-saved register; optionally used  | Yes               |\n|               | as frame pointer                        |                   |\n| %rsi          | used to pass 2nd argument to functions  | No                |\n| %rdi          | used to pass 1st argument to functions  | No                |\n| %r8           | used to pass 5th argument to functions  | No                |\n| %r9           | used to pass 6th argument to functions  | No                |\n| %r10          | temporary register, used for passing a  | No                |\n|               | function's static chain pointer         |                   |\n| %r11          | temporary register                      | No                |\n| %r12-%r15     | callee-saved registers                  | Yes               |\n| %xmm0-%xmm1   | used to pass and return floating point  | No                |\n|               | arguments                               |                   |\n| %xmm2-%xmm7   | used to pass floating point arguments   | No                |\n| %xmm8-%xmm31  | temporary registers                     | No                |\n</code></pre>"},{"location":"clr-abi/#calling-convention-specifics-for-x86","title":"Calling convention specifics for x86","text":"<p>Unlike the other architectures that RyuJIT supports, the managed x86 calling convention is different than the default native calling convention. This is true for both Windows and Unix x86.</p> <p>The standard managed calling convention is a variation on the Windows x86 fastcall convention. It differs primarily in the order in which arguments are pushed on the stack.</p> <p>The only values that can be passed in registers are managed and unmanaged pointers, object references, and the built-in integer types int8, unsigned int8, int16, unsigned int16, int32, unsigned it32, native int, native unsigned int, and enums and value types with only one 4-byte integer primitive-type field. Enums are passed as their underlying type. All floating-point values and 8-byte integer values are passed on the stack. When the return type is a value type that cannot be passed in a register, the caller shall create a buffer to hold the result and pass the address of this buffer as a hidden parameter.</p> <p>Arguments are passed in left-to-right order, starting with the <code>this</code> pointer (for instance and virtual methods), followed by the return buffer pointer if needed, followed by the user-specified argument values. The first of these that can be placed in a register is put into ECX, the next in EDX, and all subsequent ones are passed on the stack. This is in contrast with the x86 native calling conventions, which push arguments onto the stack in right-to-left order.</p> <p>The return value is handled as follows:</p> <ol> <li>Floating-point values are returned on the top of the hardware FP stack.</li> <li>Integers up to 32 bits long are returned in EAX.</li> <li>64-bit integers are passed with EAX holding the least significant 32 bits and EDX holding the most significant 32 bits.</li> <li>All other cases require the use of a return buffer, through which the value is returned. See Return buffers.</li> </ol>"},{"location":"clr-abi/#control-flow-guard-cfg-support-on-windows","title":"Control Flow Guard (CFG) support on Windows","text":"<p>Control Flow Guard (CFG) is a security mitigation available in Windows. When CFG is enabled, the operating system maintains data structures that can be used to verify whether an address is to be considered a valid indirect call target. This mechanism is exposed through two different helper functions, each with different characteristics.</p> <p>The first mechanism is a validator that takes the target address as an argument and fails fast if the address is not an expected indirect call target; otherwise, it does nothing and returns. The second mechanism is a dispatcher that takes the target address in a non-standard register; on successful validation of the address, it jumps directly to the target function. Windows makes the dispatcher available only on ARM64 and x64, while the validator is available on all platforms. However, the JIT supports CFG only on ARM64 and x64, with CFG by default being disabled for these platforms. The expected use of the CFG feature is for NativeAOT scenarios that are running in constrained environments where CFG is required.</p> <p>The helpers are exposed to the JIT as standard JIT helpers <code>CORINFO_HELP_VALIDATE_INDIRECT_CALL</code> and <code>CORINFO_HELP_DISPATCH_INDIRECT_CALL</code>.</p> <p>To use the validator the JIT expands indirect calls into a call to the validator followed by a call to the validated address. For the dispatcher the JIT will transform calls to pass the target along but otherwise set up the call as normal.</p> <p>Note that \"indirect call\" here refers to any call that is not to an immediate (in the instruction stream) address. For example, even direct calls may emit indirect call instructions in JIT codegen due to e.g. tiering or if they have not been compiled yet; these are expanded with the CFG mechanism as well.</p> <p>The next sections describe the calling convention that the JIT expects from these helpers.</p>"},{"location":"clr-abi/#cfg-details-for-arm64","title":"CFG details for ARM64","text":"<p>On ARM64, <code>CORINFO_HELP_VALIDATE_INDIRECT_CALL</code> takes the call address in <code>x15</code>. In addition to the usual registers it preserves all float registers, <code>x0</code>-<code>x8</code> and <code>x15</code>.</p> <p><code>CORINFO_HELP_DISPATCH_INDIRECT_CALL</code> takes the call address in <code>x9</code>. The JIT does not use the dispatch helper by default due to worse branch predictor performance. Therefore it will expand all indirect calls via the validation helper and a manual call.</p>"},{"location":"clr-abi/#cfg-details-for-x64","title":"CFG details for x64","text":"<p>On x64, <code>CORINFO_HELP_VALIDATE_INDIRECT_CALL</code> takes the call address in <code>rcx</code>. In addition to the usual registers it also preserves all float registers, <code>rcx</code>, and <code>r10</code>; furthermore, shadow stack space is not required to be allocated.</p> <p><code>CORINFO_HELP_DISPATCH_INDIRECT_CALL</code> takes the call address in <code>rax</code> and it reserves the right to use and trash <code>r10</code> and <code>r11</code>. The JIT uses the dispatch helper on x64 whenever possible as it is expected that the code size benefits outweighs the less accurate branch prediction. However, note that the use of <code>r11</code> in the dispatcher makes it incompatible with VSD calls where the JIT must fall back to the validator and a manual call.</p>"},{"location":"clr-abi/#notes-on-memsetmemcpy","title":"Notes on Memset/Memcpy","text":"<p>Generally, <code>memset</code> and <code>memcpy</code> do not provide any guarantees of atomicity. This implies that they should only be used when the memory being modified by <code>memset</code>/<code>memcpy</code> is not observable by any other thread (including GC), or when there are no atomicity requirements according to our Memory Model. It's especially important when we modify heap containing managed pointers - those must be updated atomically, e.g. using pointer-sized <code>mov</code> instruction (managed pointers are always aligned) - see Atomic Memory Access. It's worth noting that by \"update\" it's implied \"set to zero\", otherwise, we need a write barrier.</p> <p>Examples:</p> <pre><code>struct MyStruct\n{\n    long a;\n    string b;\n}\n\nvoid Test1(ref MyStruct m)\n{\n    // We're not allowed to use memset here\n    m = default;\n}\n\nMyStruct Test2()\n{\n    // We can use memset here\n    return default;\n}\n</code></pre>"},{"location":"corelib/","title":"<code>System.Private.CoreLib</code> and calling into the runtime","text":""},{"location":"corelib/#introduction","title":"Introduction","text":"<p><code>System.Private.CoreLib.dll</code> is the assembly for defining the core parts of the type system, and a good portion of the Base Class Library in .NET Framework. It was originally named <code>mscorlib</code> in .NET Core, though many places in the code and documentation still refer to it as <code>mscorlib</code>. This document will endeavour to stick to using <code>System.Private.CoreLib</code> or CoreLib. Base data types live in this assembly, and it has a tight coupling with the CLR. Here you will learn exactly how and why CoreLib is special and the basics about calling into the CLR from managed code via QCall and FCall methods. It also discusses calling from within the CLR into managed code.</p>"},{"location":"corelib/#dependencies","title":"Dependencies","text":"<p>Since CoreLib defines base data types like <code>Object</code>, <code>Int32</code>, and <code>String</code>, CoreLib cannot depend on other managed assemblies. However, there is a strong dependency between CoreLib and the CLR. Many of the types in CoreLib need to be accessed from native code, so the layout of many managed types is defined both in managed code and in native code inside the CLR. Additionally, some fields may be defined only in Debug, Checked, or Release builds, so typically CoreLib must be compiled separately for each type of build.</p> <p><code>System.Private.CoreLib.dll</code> builds separately for 64 bit and 32 bit, and some public constants it exposes differ by bitness. By using these constants, such as <code>IntPtr.Size</code>, most libraries above CoreLib should not need to build separately for 32 bit vs. 64 bit.</p>"},{"location":"corelib/#what-makes-systemprivatecorelib-special","title":"What makes <code>System.Private.CoreLib</code> special?","text":"<p>CoreLib has several unique properties, many of which are due to its tight coupling to the CLR.</p> <ul> <li>CoreLib defines the core types necessary to implement the CLR's Virtual Object System, such as the base data types (<code>Object</code>, <code>Int32</code>, <code>String</code>, etc).</li> <li>The CLR must load CoreLib on startup to load certain system types.</li> <li>Can only have one CoreLib loaded in the process at a time, due to layout issues. Loading multiple CoreLibs would require formalizing a contract of behavior, FCall methods, and datatype layout between CLR and CoreLib, and keeping that contract relatively stable across versions.</li> <li>CoreLib's types are used heavily for native interop and managed exceptions should map correctly to native error codes/formats.</li> <li>The CLR's multiple JIT compilers may special case a small group of certain methods in CoreLib for performance reasons, both in terms of optimizing away the method (such as <code>Math.Cos(double)</code>), or calling a method in peculiar ways (such as <code>Array.Length</code>, or some implementation details on <code>StringBuilder</code> for getting the current thread).</li> <li>CoreLib will need to call into native code, via P/Invoke where appropriate, primarily into the underlying operating system or occasionally a platform adaptation layer.</li> <li>CoreLib will require calling into the CLR to expose some CLR-specific functionality, such as triggering a garbage collection, to load classes, or to interact with the type system in a non-trivial way. This requires a bridge between managed code and native, \"manually managed\" code within the CLR.</li> <li>The CLR will need to call into managed code to call managed methods, and to get at certain functionality that is only implemented in managed code.</li> </ul>"},{"location":"corelib/#interface-between-managed-and-clr-code","title":"Interface between managed and CLR code","text":"<p>To reiterate, the needs of managed code in CoreLib include:</p> <ul> <li>The ability to access fields of some managed data structures in both managed code and \"manually managed\" code within the CLR.</li> <li>Managed code must be able to call into the CLR.</li> <li>The CLR must be able to call managed code.</li> </ul> <p>To implement these, we need a way for the CLR to specify and optionally verify the layout of a managed object in native code, a managed mechanism for calling into native code, and a native mechanism for calling into managed code.</p> <p>The managed mechanism for calling into native code must also support the special managed calling convention used by <code>String</code>'s constructors, where the constructor allocates the memory used by the object (instead of the typical convention where the constructor is called after the GC allocates memory).</p> <p>The CLR provides a <code>mscorlib</code> binder internally, providing a mapping between unmanaged types and fields to managed types and fields. The binder will look up and load classes and allows the calling of managed methods. It also performs simple verification to ensure the correctness of any layout information specified in both managed and native code. The binder ensures that the managed class attempting to load exists in mscorlib, has been loaded, and the field offsets are correct. It also needs the ability to differentiate between method overloads with different signatures.</p>"},{"location":"corelib/#calling-from-managed-to-native-code","title":"Calling from managed to native code","text":"<p>Two techniques exist for calling into the CLR from managed code. FCall allows you to call directly into the CLR code, and provides a lot of flexibility in terms of manipulating objects, though it is easy to cause GC holes by not tracking object references correctly. QCall also allows you to call into the CLR via the P/Invoke, but is much harder to accidentally mis-use. FCalls are identified in managed code as extern methods with the <code>MethodImplOptions.InternalCall</code> bit set. QCalls are marked <code>static extern</code> methods similar to regular P/Invokes, but are directed toward a library called <code>\"QCall\"</code>.</p>"},{"location":"corelib/#choosing-between-fcall-qcall-pinvoke-and-writing-in-managed-code","title":"Choosing between FCall, QCall, P/Invoke, and writing in managed code","text":"<p>First, remember that you should be writing as much as possible in managed code. You avoid a raft of potential GC hole issues, you get a better debugging experience, and the code is often simpler.</p> <p>Reasons to write FCalls in the past generally fell into three camps: missing language features, better performance, or implementing unique interactions with the runtime. C# now has almost every useful language feature that you could get from C++, including unsafe code and stack-allocated buffers, and this eliminates the first two reasons for FCalls. We have ported some parts of the CLR that were heavily reliant on FCalls to managed code in the past (such as Reflection, some Encoding, and String operations) and we intend to continue this momentum.</p> <p>If the only reason you're defining a FCall method is to call a native method, you should be using P/Invoke to call the method directly. P/Invoke is the public native method interface and should be doing everything you need in a correct manner.</p> <p>If you still need to implement a feature inside the runtime, consider if there is a way to reduce the frequency of transitioning to native code. Can you write the common case in managed and only call into native for some rare corner cases? You're usually best off keeping as much as possible in managed code.</p> <p>QCalls are the preferred mechanism going forward. You should only use FCalls when you are \"forced\" to. This happens when there is common \"short path\" through the code that is important to optimize. This short path should not be more than a few hundred instructions, cannot allocate GC memory, take locks or throw exceptions (<code>GC_NOTRIGGER</code>, <code>NOTHROWS</code>). In all other circumstances, you should be using QCall.</p> <p>FCalls were specifically designed for short paths of code that must be optimized. They allowed explicit control over when erecting a frame was done. However, it is error prone and not worth the complexity for many APIs. QCalls are essentially P/Invokes into the CLR. In the event the performance of an FCall is required consider creating a QCall and marking it with <code>SuppressGCTransitionAttribute</code>.</p> <p>As a result, QCalls give you some advantageous marshaling for <code>SafeHandle</code>s automatically \u2013 your native method just takes a <code>HANDLE</code> type, and can be used without worrying whether someone will free the handle while in that method body. The resulting FCall method would need to use a <code>SafeHandleHolder</code> and may need to protect the <code>SafeHandle</code>, etc. Leveraging the P/Invoke marshaler can avoid this additional plumbing code.</p>"},{"location":"corelib/#qcall-functional-behavior","title":"QCall functional behavior","text":"<p>QCalls are very much like a normal P/Invoke from CoreLib to CLR. Unlike FCalls, QCalls will marshal all arguments as unmanaged types like a normal P/Invoke. QCall also switch to preemptive GC mode like a normal P/Invoke. These two features should make QCalls easier to write reliably compared to FCalls. QCalls are not prone to GC holes and GC starvation bugs that are common with FCalls.</p> <p>The preferred types for QCall arguments are primitive types that are efficiently handled by the P/Invoke marshaler (<code>INT32</code>, <code>LPCWSTR</code>, <code>BOOL</code>). Notice that <code>BOOL</code> is the correct boolean flavor for QCall arguments. On the other hand, <code>CLR_BOOL</code> is the correct boolean flavor for FCall arguments.</p> <p>The pointers to common unmanaged EE structures should be wrapped into handle types. This is to make the managed implementation type safe and avoid falling into unsafe C# everywhere. See AssemblyHandle in vm\\qcall.h for an example.</p> <p>Passing object references in and out of QCalls is done by wrapping a pointer to a local variable in a handle. It is intentionally cumbersome and should be avoided if reasonably possible. See the <code>StringHandleOnStack</code> in the example below. Returning objects, especially strings, from QCalls is the only common pattern where passing the raw objects is widely acceptable. (For reasoning on why this set of restrictions helps make QCalls less prone to GC holes, read the \"GC Holes, FCall, and QCall\" section below.)</p> <p>QCalls should be implemented with a C-style method signature. This makes it easier for AOT tooling in the future to connect a QCall on the managed side to the implementation on the native side.</p>"},{"location":"corelib/#qcall-example-managed","title":"QCall example - managed","text":"<p>Do not replicate the comments into your actual QCall implementation. This is for illustrative purposes.</p> <pre><code>class Foo\n{\n    // All QCalls should have the following DllImport attribute\n    [DllImport(RuntimeHelpers.QCall, EntryPoint = \"Foo_BarInternal\", CharSet = CharSet.Unicode)]\n\n    // QCalls should always be static extern.\n    private static extern bool BarInternal(int flags, string inString, StringHandleOnStack retString);\n\n    // Many QCalls have a thin managed wrapper around them to perform\n    // as much work prior to the transition as possible. An example would be\n    // argument validation which is easier in managed than native code.\n    public string Bar(int flags)\n    {\n        if (flags != 0)\n            throw new ArgumentException(\"Invalid flags\");\n\n        string retString = null;\n        // The strings are returned from QCalls by taking address\n        // of a local variable using StringHandleOnStack\n        if (!BarInternal(flags, this.Id, new StringHandleOnStack(ref retString)))\n            FatalError();\n\n        return retString;\n    }\n}\n</code></pre>"},{"location":"corelib/#qcall-example-unmanaged","title":"QCall example - unmanaged","text":"<p>Do not replicate the comments into your actual QCall implementation.</p> <p>The QCall entrypoint has to be registered in tables in vm\\qcallentrypoints.cpp using the <code>DllImportEntry</code> macro. See \"Registering your QCall or FCall Method\" below.</p> <pre><code>// All QCalls should be free functions and tagged with QCALLTYPE and extern \"C\"\nextern \"C\" BOOL QCALLTYPE Foo_BarInternal(int flags, LPCWSTR wszString, QCall::StringHandleOnStack retString)\n{\n    // All QCalls should have QCALL_CONTRACT.\n    // It is alias for THROWS; GC_TRIGGERS; MODE_PREEMPTIVE.\n    QCALL_CONTRACT;\n\n    // Optionally, use QCALL_CHECK instead and the expanded form of the contract\n    // if you want to specify preconditions:\n    // CONTRACTL {\n    //     QCALL_CHECK;\n    //     PRECONDITION(wszString != NULL);\n    // } CONTRACTL_END;\n\n    // The only line between QCALL_CONTRACT and BEGIN_QCALL\n    // should be the return value declaration if there is one.\n    BOOL retVal = FALSE;\n\n    // The body has to be enclosed in BEGIN_QCALL/END_QCALL macro.\n    // It is necessary for exception handling.\n    BEGIN_QCALL;\n\n    // Argument validation would ideally be in managed, but in some cases\n    // needs to be done in native. If argument validation is done in\n    // managed asserting in native is warranted.\n    _ASSERTE(flags != 0);\n\n    // No need to worry about GC moving strings passed into QCall.\n    // Marshalling pins them for us.\n    printf(\"%S\\n\", wszString);\n\n    // This is the most efficient way to return strings back\n    // to managed code. No need to use StringBuilder.\n    retString.Set(L\"Hello\");\n\n    // You can not return from inside of BEGIN_QCALL/END_QCALL.\n    // The return value has to be passed out in helper variable.\n    retVal = TRUE;\n\n    END_QCALL;\n\n    return retVal;\n}\n</code></pre>"},{"location":"corelib/#fcall-functional-behavior","title":"FCall functional behavior","text":"<p>FCalls allow more flexibility in terms of passing object references around, but with higher code complexity and more opportunities to make mistakes. Additionally, for any FCall of non-trivial length, explicitly poll for whether a garbage collection must occur. Failing to do so will lead to starvation issues if managed code repeatedly calls the FCall method in a tight loop, because FCalls execute while the thread only allows the GC to run in a cooperative manner.</p> <p>FCalls require a lot of boilerplate code, too much to describe here. Refer to fcall.h for details.</p>"},{"location":"corelib/#gc-holes-fcall-and-qcall","title":"GC holes, FCall, and QCall","text":"<p>A more complete discussion on GC holes can be found in the CLR Code Guide. Look for \"Is your code GC-safe?\". This tailored discussion motivates some of the reasons why FCall and QCall have some of their strange conventions.</p> <p>Object references passed as parameters to FCall methods are not GC-protected, meaning that if a GC occurs, those references will point to the old location in memory of an object, not the new location. For this reason, FCalls usually follow the discipline of accepting something like <code>StringObject*</code> as their parameter type, then explicitly converting that to a <code>STRINGREF</code> before doing operations that may trigger a GC. If you expect to use an object reference later, you must GC protect object references before triggering a GC.</p> <p>Failing to properly report an <code>OBJECTREF</code> or to update an interior pointer is commonly referred to as a \"GC hole\", because the <code>OBJECTREF</code> class will do some validation that it points to a valid object every time you dereference it in Debug and Checked builds. When an <code>OBJECTREF</code> pointing to an invalid object is dereferenced, an assert will trigger saying something like \"Detected an invalid object reference. Possible GC hole?\". This assert is unfortunately easy to hit when writing \"manually managed\" code.</p> <p>Note that QCall's programming model is restrictive to sidestep GC holes by forcing you to pass in the address of an object reference on the stack. This guarantees that the object reference is GC protected by the JIT's reporting logic, and that the actual object reference will not move because it is not allocated in the GC heap. QCall is our recommended approach, precisely because it makes GC holes harder to write.</p>"},{"location":"corelib/#fcall-epilog-walker-for-x86","title":"FCall epilog walker for x86","text":"<p>The managed stack walker needs to be able to find its way from FCalls. It is relative easy on newer platforms that define conventions for stack unwinding as part of the ABI. The stack unwinding conventions are not defined by an ABI for x86. The runtime works around this by implementing an epilog walker. The epilog walker computes the FCall return address and callee save registers by simulating the FCall execution. This imposes limits on what constructs are allowed in the FCall implementation.</p> <p>Complex constructs like stack allocated objects with destructors or exception handling in the FCall implementation may confuse the epilog walker. This can lead to GC holes or crashes during stack walking. There is no comprehensive list of what constructs should be avoided to prevent this class of bugs. An FCall implementation that is fine one day may break with the next C++ compiler update. We depend on stress runs and code coverage to find bugs in this area.</p>"},{"location":"corelib/#fcall-example-managed","title":"FCall example \u2013 managed","text":"<p>Here's a real-world example from the <code>String</code> class:</p> <pre><code>public partial sealed class String\n{\n    [MethodImpl(MethodImplOptions.InternalCall)]\n    private extern string? IsInterned();\n\n    public static string? IsInterned(string str)\n    {\n        if (str == null)\n        {\n            throw new ArgumentNullException(nameof(str));\n        }\n\n        return str.IsInterned();\n    }\n}\n</code></pre>"},{"location":"corelib/#fcall-example-unmanaged","title":"FCall example \u2013 unmanaged","text":"<p>The FCall entrypoint has to be registered in tables in vm\\ecalllist.h using <code>FCFuncEntry</code> macro. See \"Registering your QCall or FCall Method\".</p> <p>This example shows an FCall method that takes a managed object (<code>Object*</code>) as a raw pointer. These raw inputs are considered \"unsafe\" and must be validated or converted if they\u2019re used in a GC-sensitive context.</p> <pre><code>FCIMPL1(FC_BOOL_RET, ExceptionNative::IsImmutableAgileException, Object* pExceptionUNSAFE)\n{\n    FCALL_CONTRACT;\n\n    ASSERT(pExceptionUNSAFE != NULL);\n\n    OBJECTREF pException = (OBJECTREF) pExceptionUNSAFE;\n\n    FC_RETURN_BOOL(CLRException::IsPreallocatedExceptionObject(pException));\n}\nFCIMPLEND\n</code></pre>"},{"location":"corelib/#registering-your-qcall-or-fcall-method","title":"Registering your QCall or FCall method","text":"<p>The CLR must know the name of your QCall and FCall methods, both in terms of the managed class and method names, as well as which native methods to call. For FCalls, registration is done in ecalllist.h, with two arrays. The first array maps namespace and class names to an array of function elements. That array of function elements then maps individual method names and signatures to function pointers.</p> <p>Say we defined an FCall method for <code>String.IsInterned()</code>, in the example above. First, we need to ensure that we have an array of function elements for the String class.</p> <pre><code>// Note these have to remain sorted by name:namespace pair\n    ...\n    FCClassElement(\"String\", \"System\", gStringFuncs)\n    ...\n</code></pre> <p>Second, we must then ensure that <code>gStringFuncs</code> contains a proper entry for <code>IsInterned</code>. Note that if a method name has multiple overloads then we can specify a signature:</p> <pre><code>FCFuncStart(gStringFuncs)\n    ...\n    FCFuncElement(\"IsInterned\", AppDomainNative::IsStringInterned)\n    ...\nFCFuncEnd()\n</code></pre> <p>QCalls are registered in qcallentrypoints.cpp in the <code>s_QCall</code> array with the <code>DllImportEntry</code> macro as follows:</p> <pre><code>static const Entry s_QCall[] =\n{\n    ...\n    DllImportEntry(MyQCall),\n    ...\n};\n</code></pre>"},{"location":"corelib/#naming-convention","title":"Naming convention","text":"<p>FCalls and QCalls should not be publicly exposed. Instead wrap the actual FCall or QCall and provide a API approved name.</p> <p>The internal FCall or QCall should use the \"Internal\" suffix to disambiguate the name of the FCall or QCall from public entry point (e.g. the public entry point does error checking and then calls shared worker function with exactly same signature). This is no different from how you would deal with this situation in pure managed code in BCL.</p>"},{"location":"corelib/#types-with-a-managedunmanaged-duality","title":"Types with a managed/unmanaged duality","text":"<p>Certain managed types must have a representation available in both managed and native code. You could ask whether the canonical definition of a type is in managed code or native code within the CLR, but the answer doesn't matter \u2013 the key thing is they must both be identical. This will allow the CLR's native code to access fields within a managed object in a fast and efficient manner. There is a more complex way of using essentially the CLR's equivalent of Reflection over <code>MethodTable</code>s and <code>FieldDesc</code>s to retrieve field values, but this doesn't perform as well as desired and isn't very usable. For commonly used types, it makes sense to declare a data structure in native code and keep the two in sync.</p> <p>The CLR provides a binder for this purpose. After you define your managed and native classes, you should provide some clues to the binder to help ensure that the field offsets remain the same to quickly spot when someone accidentally adds a field to only one definition of a type.</p> <p>In corelib.h, use macros ending in \"_U\" to describe a type, the name of fields in managed code, and the name of fields in a corresponding native data structure. Additionally, you can specify a list of methods, and reference them by name when you attempt to call them later.</p> <pre><code>DEFINE_CLASS_U(SAFE_HANDLE,         Interop,                SafeHandle,         SafeHandle)\nDEFINE_FIELD(SAFE_HANDLE,           HANDLE,                 handle)\nDEFINE_FIELD_U(SAFE_HANDLE,         STATE,                  _state,                     SafeHandle,            m_state)\nDEFINE_FIELD_U(SAFE_HANDLE,         OWNS_HANDLE,            _ownsHandle,                SafeHandle,            m_ownsHandle)\nDEFINE_FIELD_U(SAFE_HANDLE,         INITIALIZED,            _fullyInitialized,          SafeHandle,            m_fullyInitialized)\nDEFINE_METHOD(SAFE_HANDLE,          GET_IS_INVALID,         get_IsInvalid,              IM_RetBool)\nDEFINE_METHOD(SAFE_HANDLE,          RELEASE_HANDLE,         ReleaseHandle,              IM_RetBool)\nDEFINE_METHOD(SAFE_HANDLE,          DISPOSE,                Dispose,                    IM_RetVoid)\nDEFINE_METHOD(SAFE_HANDLE,          DISPOSE_BOOL,           Dispose,                    IM_Bool_RetVoid)\n</code></pre> <p>Then, you can use the <code>REF&lt;T&gt;</code> template to create a type name like <code>SAFEHANDLEREF</code>. All the error checking from <code>OBJECTREF</code> is built into the <code>REF&lt;T&gt;</code> template, and you can freely dereference this <code>SAFEHANDLEREF</code> and use fields off of it in native code. You still must GC protect these references.</p>"},{"location":"corelib/#calling-into-managed-code-from-unmanaged-code","title":"Calling into managed code from unmanaged code","text":"<p>Clearly there are places where the CLR must call into managed code from native. For this purpose, we have added a <code>MethodDescCallSite</code> class to handle a lot of plumbing for you. Conceptually, all you need to do is find the <code>MethodDesc*</code> for the method you want to call, find a managed object for the \"this\" pointer (if you're calling an instance method), pass in an array of arguments, and deal with the return value. Internally, you'll need to potentially toggle your thread's state to allow the GC to run in preemptive mode, etc.</p> <p>Here's a simplified example. Note how this instance uses the binder described in the previous section to call <code>SafeHandle</code>'s virtual <code>ReleaseHandle</code> method.</p> <pre><code>void SafeHandle::RunReleaseMethod(SafeHandle* psh)\n{\n    CONTRACTL {\n        THROWS;\n        GC_TRIGGERS;\n        MODE_COOPERATIVE;\n    } CONTRACTL_END;\n\n    SAFEHANDLEREF sh(psh);\n\n    GCPROTECT_BEGIN(sh);\n\n    MethodDescCallSite releaseHandle(s_pReleaseHandleMethod, METHOD__SAFE_HANDLE__RELEASE_HANDLE, (OBJECTREF*)&amp;sh, TypeHandle(), TRUE);\n\n    ARG_SLOT releaseArgs[] = { ObjToArgSlot(sh) };\n    if (!(BOOL)releaseHandle.Call_RetBool(releaseArgs)) {\n        MDA_TRIGGER_ASSISTANT(ReleaseHandleFailed, ReportViolation)(sh-&gt;GetTypeHandle(), sh-&gt;m_handle);\n    }\n\n    GCPROTECT_END();\n}\n</code></pre>"},{"location":"corelib/#interactions-with-other-subsystems","title":"Interactions with other subsystems","text":""},{"location":"corelib/#debugger","title":"Debugger","text":"<p>One limitation of FCalls today is that you cannot easily debug both managed code and FCalls easily in Visual Studio's Interop (or mixed mode) debugging. Setting a breakpoint today in an FCall and debugging with Interop debugging just doesn't work. This most likely won't be fixed.</p>"},{"location":"corelib/#physical-architecture","title":"Physical architecture","text":"<p>When the CLR starts up, CoreLib is loaded by a method called <code>SystemDomain::LoadBaseSystemClasses()</code>. Here, the base data types and other similar classes (like <code>Exception</code>) are loaded, and appropriate global pointers are set up to refer to CoreLib's types.</p> <p>For FCalls, look in fcall.h for infrastructure, and ecalllist.h to properly inform the runtime about your FCall method.</p> <p>For QCalls, look in qcall.h for associated infrastructure, and qcallentrypoints.cpp to properly inform the runtime about your QCall method.</p> <p>More general infrastructure and some native type definitions can be found in object.h. The binder uses <code>mscorlib.h</code> to associate managed and native classes.</p>"},{"location":"dac-notes/","title":"Data Access Component (DAC) Notes","text":"<p>Date: 2007</p> <p>Debugging managed code requires special knowledge of managed objects and constructs. For example, objects have various kinds of header information in addition to the data itself. Objects may move in memory as the garbage collector does its work. Getting type information may require help from the loader. Retrieving the correct version of a function that has undergone an edit-and-continue or getting information for a function emitted through reflection requires the debugger to be aware of EnC version numbers and metadata. The debugger must be able to distinguish AppDomains and assemblies. The code in the VM directory embodies the necessary knowledge of these managed constructs. This essentially means that APIs to retrieve information about managed code and data must run some of the same algorithms that the execution engine itself runs.</p> <p>Debuggers can operate either in-process or out-of-process. A debugger that runs in-process requires a live data target (the debuggee). In this case, the runtime has been loaded and the target is running. A helper thread in the debuggee runs code from the execution engine to compute the information the debugger needs. Because the helper thread runs in the target process, it has ready access to the target's address space and the runtime code. All the computation occurs in the target process. This is a simple way to get the information the debugger needs to be able to represent managed constructs in a meaningful way. Nevertheless, an in-process debugger has certain limitations. For example, if the debuggee is not currently running (as is the case when the debuggee is a dump file), the runtime is not loaded (and may not even be available on the machine). In this case, the debugger has no way to execute runtime code to get the information it needs.</p> <p>Historically, the CLR debugger has operated in process. A debugger extension, SOS (Son of Strike) or Strike (in the early CLR days) can be used to inspect managed code. Starting with .NET Framework 4, the debugger runs out-of-process. The CLR debugger APIs provide much of the functionality of SOS along with other functionality that SOS does not provide. Both SOS and the CLR debugging APIs use the Data Access Component (DAC) to implement out-of-process debugging. The DAC is conceptually a subset of the runtime's execution engine code that runs out-of-process. This means that it can operate on a dump file, even on a machine that has no runtime installed. Its implementation consists mainly of a set of macros and templates, combined with conditional compilation of the execution engine's code. When the runtime is built, both clr.dll and mscordacwks.dll. For CoreCLR builds, the binaries are slightly different: coreclr.dll and msdaccore.dll. The file names also differ when built for other operating systems, like OS X. To inspect the target, the DAC can read its memory to get the inputs for the VM code in mscordacwks. It can then run the appropriate functions in the host to compute the information needed about a managed construct and finally return the results to the debugger.</p> <p>Notice that the DAC reads the memory of the target process. It's important to realize that the debugger and the debuggee are separate processes with separate address spaces. Thus it is important to make a clear distinction between target memory and host memory. Using a target address in code running in the host process would have completely unpredictable and generally incorrect results. When using the DAC to retrieve memory from the target, it is important to be very careful to use addresses from the correct address space. Furthermore, sometimes the target addresses are strictly used as data. In this case, it would be just as incorrect to use a host address. For example, to display information about a managed function, we might want to list its starting address and size. Here, it is important to provide the target address. When writing code in the VM that the DAC will run, one needs to correctly choose when to use host and target addresses.</p> <p>The DAC infrastructure (the macros and templates that control how host or target memory is accessed) supplies certain conventions that distinguish which pointers are host addresses and which are target addresses. When a function is DACized (i.e., use the DAC infrastructure to make the function work out of process), host pointers of type <code>T</code> are declared to be of type <code>T *</code>. Target pointers are of type <code>PTR_T</code>. Remember though, that the concept of host versus target is only meaningful for the DAC. In a non-DAC build, we have only a single address space. The host and the target are the same: the CLR. If we declare a local variable of either type <code>T *</code> _or of type <code>PTR_T</code> in a VM function, it will be a \"host pointer\". When we are executing code in clr.dll (coreclr.dll), there is absolutely no difference between a local variable of type <code>T *</code> and a local variable of type <code>PTR_T</code>. If we execute the function compiled into mscordacwks.dll (msdaccore.dll) from the same source, the variable declared to be of type <code>T *</code> will be a true host pointer, with the debugger as the host. If you think about it, this is obvious. Nevertheless it can become confusing when we start passing these pointers to other VM functions. When we are DACizing a function (i.e., changing <code>T *</code> to <code>PTR_T</code>, as appropriate), we sometimes need to trace a pointer back to its point of origin to determine whether it should be a host or target type.</p> <p>When one has no understanding of the DAC, it's easy to find the use of the DAC infrastructure annoying. The <code>TADDR</code>s and <code>PTR_this</code> and <code>dac_casts</code>, etc. seem to clutter the code and make it harder to understand. With just a little work, though, you'll find that these are not really difficult to learn. Keeping host and target addresses explicitly different is really a form of strong typing. The more diligent we are, the easier it becomes to ensure our code is correct.</p> <p>Because the DAC potentially operates on a dump, the part of the VM sources we build in mscordacwks.dll (msdaccore.dll) must be non-invasive. Specifically, we usually don't want to do anything that would cause writing to the target's address space, nor can we execute any code that might cause an immediate garbage collection. (If we can defer the GC, it may be possible to allocate.) Note that the host state is always mutated (temporaries, stack or local heap values); it is only mutating the target space that is problematic. To enforce this, we do two things: code factoring and conditional compilation. In an ideal world, we would factor the VM code so that we would strictly isolate invasive actions in functions that are separate from non-invasive functions.</p> <p>Unfortunately, we have a large code base, most of which we wrote without ever thinking about the DAC at all. We have a significant number of functions with \"find or create\" semantics and many other functions that have some parts that just do inspection and other parts that write to the target. Sometimes we control this with a flag passed into the function. This is common in loader code, for example. To avoid having to complete the immense job of refactoring all the VM code before we can use the DAC, we have a second method to prevent executing invasive code from out of process. We have a defined pre-processor constant, <code>DACCESS_COMPILE</code> that we use to control what parts of the code we compile into the DAC. We would like to use the <code>DACCESS_COMPILE</code> constant as little as we can, so when we DACize a new code path, we prefer to refactor whenever possible. Thus, a function that has \"find or create\" semantics should become two functions: one that tries to find the information and a wrapper that calls this and creates if the find fails. That way, the DAC code path can call the find function directly and avoid the creation.</p>"},{"location":"dac-notes/#how-does-the-dac-work","title":"How does the DAC work?","text":"<p>As discussed, the DAC works by marshaling the data it needs and running code in the mscordacwks.dll (msdaccore.dll) module. It marshals data by reading from the target address space to get a target value, and then storing it in the host address space where the functions in mscordacwks can operate on it. This happens only on demand, so if the mscordacwks functions never need a target value, the DAC will not marshal it.</p>"},{"location":"dac-notes/#marshaling-principles","title":"Marshaling Principles","text":"<p>The DAC maintains a cache of data that it reads. This avoids the overhead of reading the same values repeatedly. Of course, if the target is live, the values will potentially change. We can only assume the cached values are valid as long as the debuggee remains stopped. Once we allow the target to continue execution, we must flush the DAC cache. The DAC will retrieve the values again when the debugger stops the target for further inspection. The entries in the DAC cache are of type <code>DAC_INSTANCE</code>. This contains (among other data) the target address, the size of the data and space for the marshaled data itself. When the DAC marshals data, it returns the address of the marshaled data part of this entry as the host address.</p> <p>When the DAC reads a value from the target, it marshals the value as a chunk of bytes of a given size (determined by its type). By keeping the target address as a field in the cache entries, it maintains a mapping between the target address and the host address (the address in the cache). Between any stop and continue of a debugger session, the DAC will marshal each value requested only once, as long as subsequent accesses use the same type. (If we reference the target address by two different types, the size may be different, so the DAC will create a new cache entry for the new type). If the value is already in the cache, the DAC will be able to look it up by its target address. That means we can correctly compare two host pointers for (in)equality as long as we have accessed both pointers using the same type. This identity of pointers does not hold across type conversions however. Furthermore, we have no guarantee that values marshaled separately will maintain the same spatial relationship in the cache that they do in the target, so it is incorrect to compare two host pointers for less-than or greater-than relationships. Object layout must be identical in host and target, so we can access fields in an object in the cache using the same offsets we use in the target. Remember that any pointer fields in a marshaled object will be target addresses (generally declared as data members of a <code>PTR</code> type). If we need the values at those addresses, the DAC must marshal them to the host before dereferencing them.</p> <p>Because we build this dll from the same sources that we use to build mscorwks.dll (coreclr.dll), the mscordacwks.dll (msdaccore.dll) build that the debugger uses must match the mscorwks build exactly. You can see that this is obviously true if you consider that between builds we might add or remove a field from a type we use. The size for the object in mscorwks would then be different from the size in mscordacwks and the DAC could not marshal the object correctly. This has a ramification that's obvious when you think about it, but easy to overlook. We cannot have fields in objects that exist only in DAC builds or only in non-DAC builds. Thus, a declaration such as the following would lead to incorrect behavior.</p> <pre><code>class Foo\n{\n    ...\n    int nCount;\n\n    // DON'T DO THIS!! Object layout must match in DAC builds\n    #ifndef DACCESS_COMPILE\n\n        DWORD dwFlags;\n\n    #endif\n\n    PTR_Bar pBar;\n    ...\n};\n</code></pre>"},{"location":"dac-notes/#marshaling-specifics","title":"Marshaling Specifics","text":"<p>DAC marshaling works through a collection of typedefs, macros and templated types that generally have one meaning in DAC builds and a different meaning in non-DAC builds. You can find these declarations in src\\inc\\daccess.h. You will also find a long comment at the beginning of this file that explains the details necessary to write code that uses the DAC.</p> <p>An example may be helpful in understanding how marshaling works. The common debugging scenario is represented in the following block diagram:</p> <p></p> <p>The debugger in this figure could be Visual Studio, MDbg, WinDbg, etc. The debugger interfaces with the CLR debugger interface (DBI) APIs to get the information it needs. Information that must come from the target goes through the DAC. The debugger implements the data target, which is responsible for implementing a <code>ReadVirtual</code> function to read memory in the target. The dotted line in the diagram represents the process boundary.</p> <p>Suppose the debugger needs to display the starting address of an ngen'ed method in the managed application that it has gotten from the managed stack. We will assume that the debugger has already gotten an instance of <code>ICorDebugFunction</code> back from the DBI. It will begin by calling the DBI API <code>ICorDebugFunction::GetNativeCode</code>. This calls into the DAC through the DAC/DBI interface function <code>GetNativeCodeInfo</code>, passing in the domain file and metadata token for the function. The following code fragment is a simplification of the actual function, but it illustrates marshaling without introducing extraneous details.</p> <pre><code>void DacDbiInterfaceImpl::GetNativeCodeInfo(TADDR taddrDomainFile,\n            mdToken functionToken,\n            NativeCodeFunctionData * pCodeInfo)\n{\n    ...\n\n    DomainFile * pDomainFile = dac_cast&lt;PTR_DomainFile&gt;(taddrDomainFile);\n    Module * pModule = pDomainFile-&gt;GetCurrentModule();\n\n    MethodDesc* pMethodDesc = pModule-&gt;LookupMethodDef (functionToken);\n    pCodeInfo-&gt;pNativeCodeMethodDescToken = pMethodDesc;\n\n    // if we are loading a module and trying to bind a previously set breakpoint, we may not have\n    // a method desc yet, so check for that situation\n    if(pMethodDesc != NULL)\n    {\n        pCodeInfo-&gt;startAddress = pMethodDesc-&gt;GetNativeCode();\n        ...\n    }\n}\n</code></pre> <p>The first step is to get the module in which the managed function resides. The <code>taddrDomainFile</code> parameter we pass in represents a target address, but we will need to be able to dereference it here. This means we need the DAC to marshal the value. The <code>dac_cast</code> operator will construct a new instance of <code>PTR_DomainFile</code> with a target address equal to the value of <code>domainFileTaddr</code>. When we assign this to <code>pDomainFile</code>, we have an implicit conversion to the host pointer type. This conversion operator is a member of the <code>PTR</code> type and this is where the marshaling occurs. The DAC first searches its cache for the target address. If it doesn't find it, it reads the data from the target for the marshaled <code>DomainFile</code> instance and copies it to the cache. Finally, it returns the host address of the marshaled value.</p> <p>Now we can call <code>GetCurrentModule</code> on this host instance of the <code>DomainFile</code>. This function is a simple accessor that returns <code>DomainFile::m_pModule</code>. Notice that it returns a <code>Module *</code>, which will be a host address. The value of <code>m_pModule</code> is a target address (the DAC will have copied the <code>DomainFile</code> instance as raw bytes). The type for the field is <code>PTR_Module</code>, however, so when the function returns it, the DAC will automatically marshal it as part of the conversion to <code>Module *</code>. That means the return value is a host address. Now we have the correct module and a method token, so we have all the information we need to get the <code>MethodDesc</code>.</p> <pre><code>Module * DomainFile::GetCurrentModule()\n{\n    LEAF_CONTRACT;\n    SUPPORTS_DAC;\n    return m_pModule;\n}\n</code></pre> <p>In this simplified version of the code, we are assuming that the method token is a method definition. The next step, then, is to call the <code>LookupMethodDef</code> function on the <code>Module</code> instance.</p> <pre><code>inline MethodDesc *Module::LookupMethodDef(mdMethodDef token)\n{\n    WRAPPER_CONTRACT;\n    SUPPORTS_DAC;\n    ...\n    return dac_cast&lt;PTR_MethodDesc&gt;(GetFromRidMap(&amp;m_MethodDefToDescMap, RidFromToken(token)));\n}\n</code></pre> <p>This uses the <code>RidMap</code> to lookup the <code>MethodDesc</code>. If you look at the definition for this function, you will see that it returns a <code>TADDR</code>:</p> <pre><code>TADDR GetFromRidMap(LookupMap *pMap, DWORD rid)\n{\n    ...\n\n    TADDR result = pMap-&gt;pTable[rid];\n    ...\n    return result;\n}\n</code></pre> <p>This represents a target address, but it's not really a pointer; it's simply a number (although it represents an address). The problem is that <code>LookupMethodDef</code> needs to return the address of a <code>MethodDesc</code> that we can dereference. To accomplish this, the function uses a <code>dac_cast</code> to <code>PTR_MethodDesc</code> to convert the <code>TADDR</code> to a <code>PTR_MethodDesc</code>. You can think of this as the target address space form of a cast from <code>void *</code> to <code>MethodDesc *</code>. In fact, this code would be slightly cleander if <code>GetFromRidMap</code> returned a <code>PTR_VOID</code> (with pointer semantics) instead of a <code>TADDR</code> (with integer semantics). Again, the type conversion implicit in the return statement ensures that the DAC marshals the object (if necessary) and returns the host address of the <code>MethodDesc</code> in the DAC cache.</p> <p>The assignment statement in <code>GetFromRidMap</code> indexes an array to get a particular value. The <code>pMap</code> parameter is the address of a structure field from the <code>MethodDesc</code>. As such, the DAC will have copied the entire field into the cache when it marshaled the <code>MethodDesc</code> instance. Thus, <code>pMap</code>, which is the address of this struct, is a host pointer. Dereferencing it does not involve the DAC at all. The <code>pTable</code> field, however, is a <code>PTR_TADDR</code>. What this tells us is that <code>pTable</code> is an array of target addresses, but its type indicates that it is a marshaled type. This means that <code>pTable</code> will be a target address as well. We dereference it with the overloaded indexing operator for the <code>PTR</code> type. This will get the target address of the array and compute the target address of the element we want. The last step of indexing marshals the array element back to a host instance in the DAC cache and returns its value. We assign the element (a <code>TADDR</code>) to the local variable result and return it.</p> <p>Finally, to get the code address, the DAC/DBI interface function will call <code>MethodDesc::GetNativeCode</code>. This function returns a value of type <code>PCODE</code>. This type is a target address, but one that we cannot dereference (it is just an alias of <code>TADDR</code>) and one that we use specifically to specify a code address. We store this value on the <code>ICorDebugFunction</code> instance and return it to the debugger.</p>"},{"location":"dac-notes/#ptr-types","title":"PTR Types","text":"<p>Because the DAC marshals values from the target address space to the host address space, understanding how the DAC handles target pointers is fundamental. We collectively refer to the fundamental types used for marshaling these as \"PTR types.\" You will see that daccess.h defines two classes: <code>__TPtrBase</code>, which has several derived types, and <code>__GlobalPtr</code>. We don't use these types directly; we use them only indirectly through a number of macros. Each of these contains a single data member to give us the target address of the value. For <code>__TPtrBase</code>, this is a full address. For <code>__GlobalPtr</code>, it is a relative address, referenced from a DAC global base location. The \"T\" in <code>__TPtrBase</code> stands for \"target\". As you can guess, we use types derived from <code>__TPtrBase</code> for pointers that are data members or locals and we use <code>__GlobalPtr</code> for globals and statics.</p> <p>In practice, we use these types only through macros. The introductory comment in daccess.h has examples of the use of all of these. What is interesting about these macros is that they will expand to declare instantiated types from these marshaling templates in DAC builds, but are no-ops in non-DAC builds. For example, the following definition declares <code>PTR_MethodTable</code> as a type to represent method table pointers (note that the convention is to name these types with a prefix of <code>PTR_</code>):</p> <pre><code>typedef DPTR(class MethodTable) PTR_MethodTable;\n</code></pre> <p>In a DAC build, the <code>DPTR</code> macro will expand to declare a <code>__DPtr&lt;MethodTable&gt;</code> type named <code>PTR_MethodTable</code>. In a non-DAC build, the macro simply declares <code>PTR_MethodTable</code> to be <code>MethodTable *</code>. This implies that the DAC functionality does not result in any behavior change or performance degradation in non-DAC builds.</p> <p>Even better, in a DAC build, the DAC will automatically marshal variables, data members, or return values declared to be of type <code>PTR_MethodTable</code>, as we saw in the example in the last section. The marshaling is completely transparent. The <code>__DPtr</code> type has overloaded operator functions to redefine pointer dereferencing and array indexing, and a conversion operator to cast to the host pointer type. These operations determine whether the requested value is already in the cache, from whence the operators will return them immediately, or whether it is necessary to read from the target and load the value into the cache before returning it. If you are interested in understanding the details, the function responsible for these cache operations is <code>DacInstantiateTypeByAddressHelper</code>.</p> <p><code>PTR</code> types defined with <code>DPTR</code> are the most common in the runtime, but we also have <code>PTR</code> types for global and static pointers, restricted-use arrays, pointers to variable-sized objects, and pointers to classes with virtual functions that we may need to call from mscordacwks.dll (msdaccore.dll). Most of these are rare and you can refer to daccess.h to learn more about them if you need them.</p> <p>The <code>GPTR</code> and <code>VPTR</code> macros are common enough to warrant special mention here. Both the way we use these and their external behavior is quite similar to <code>DPTR</code>s. Again, marshaling is automatic and transparent. The <code>VPTR</code> macro declares a marshaled pointer type for a class with virtual functions. This special macro is necessary because the virtual function table is essentially an implicit extra field. The DAC has to marshal this separately, since the function addresses are all target addresses that the DAC must convert to host addresses. Treating these classes in this way means that the DAC automatically instantiates the correct implementation class, making casts between base and derived types unnecessary. When you declare a <code>VPTR</code> type, you must also list it in vptr_list.h. <code>__GlobalPtr</code> types provide base functionality to marshal both global variables and static data members through the <code>GPTR</code>, <code>GVAL</code>, <code>SPTR</code> and <code>SVAL</code> macros. The implementation of global variables is almost identical to that of static fields (both use the <code>__GlobalPtr</code> class) and require the addition of an entry in dacvars.h. Global functions used in the DAC do not require macros at the implementation site, but they must be declared in the gfunc_list.h header to have their addresses be automatically available to the DAC. The comments in daccess.h and dacvars.h provide more details about declaring these types.</p> <p>Global and static values and pointers are interesting because they form the entry points to the target address space (all other uses of the DAC require you to have a target address already). Many of the globals in the runtime are already DACized. It occasionally becomes necessary to make a previously un-DACized (or a newly introduced) global available to the DAC. By using the appropriate macros and dacvars.h entry, you enable the dac table machinery (in dactable.cpp) to save the address of the global into a table that is exported from coreclr.dll. The DAC uses this table at run-time to determine where to look in the target address space when the code accesses a global.</p>"},{"location":"dac-notes/#val-types","title":"VAL Types","text":"<p>In addition to pointer types, the DAC must also marshal static and global values (as opposed to values referenced by static or global pointers). For this we have a collection of macros <code>?VAL_*</code>. We use <code>GVAL_*</code> for global values, and <code>SVAL_*</code> for static values. The comment in the daccess.h file has a table showing how to use the various forms of these and includes instructions for declaring global and static values (and global and static pointers) that we will use in DACized code.</p>"},{"location":"dac-notes/#pure-addresses","title":"Pure Addresses","text":"<p>The <code>TADDR</code> and <code>PCODE</code> types we introduced in the example of DAC operation are pure target addresses. These are actually integer types, rather than pointers. This prevents code in the host from incorrectly dereferencing them. The DAC does not treat them as pointers either. Specifically, because we have no type or size information no dereferencing or marshalling can occur. We use these primarily in two situations: when we are treating a target address as pure data and when we need to do pointer arithmetic with target addresses (although we can also do pointer arithmetic with <code>PTR</code> types). Of course, because <code>TADDR</code>s have no type information for the target locations they specify, when we perform address arithmetic, we need to factor in the size explicitly.</p> <p>We also have one special class of <code>PTR</code>s that don't involve marshaling: <code>PTR_VOID</code> and <code>PTR_CVOID</code>. These are the target equivalents of <code>void *</code> and <code>const void *</code>, respectively. Because <code>TADDR</code>s are simply numbers, they don't have pointer semantics, which means that if we DACize code by converting <code>void *</code> to <code>TADDR</code> (as was often the case in the past), we often need extra casts and other changes, even in code that does not compile for the DAC. Using <code>PTR_VOID</code> makes it easier and cleaner to DACize code that uses <code>void *</code> by preserving the semantics expected for <code>void *</code>. If we DACize a function that uses <code>PTR_VOID</code> or <code>PTR_CVOID</code>, we can't directly marshal data from these addresses, since we have no idea how much data we would need to read. This means we can't dereference them (or even do pointer arithmetic), but this is identical to the semantics of <code>void *</code>. As is the case for <code>void *</code>, we generally cast them to a more specific <code>PTR</code> type when we need to use them. We also have a <code>PTR_BYTE</code> type, which is a standard marshaled target pointer (that supports pointer arithmetic, etc.). In general, when we DACize code, <code>void *</code> becomes <code>PTR_VOID</code> and <code>BYTE *</code> becomes <code>PTR_BYTE</code>, just as you would expect. daccess.h has explanatory comments that provide more details about the use and semantics of the <code>PTR_VOID</code> type.</p> <p>Occasionally, legacy code stores a target address in a host pointer type such as <code>void *</code>. This is always a bug and makes it extremely difficult to reason about the code. It will also break when we support cross-platform, where the pointer types are different sizes. In DAC builds, the <code>void *</code> type is a host pointer which should never contain a target address. Using <code>PTR_VOID</code> instead allows us to indicate that a void pointer type is a target address. We are trying to eliminate all such uses, but some are quite pervasive in the code and will take a while to eliminate entirely.</p>"},{"location":"dac-notes/#conversions","title":"Conversions","text":"<p>In earlier CLR versions, we used C-style type casting, macros, and constructors to cast between types. For example, in <code>MethodIterator::Next</code>, we have the following:</p> <pre><code>if (methodCold)\n{\n    PTR_CORCOMPILE_METHOD_COLD_HEADER methodColdHeader\n                = PTR_CORCOMPILE_METHOD_COLD_HEADER((TADDR)methodCold);\n\n    if (((TADDR)methodCode) == PTR_TO_TADDR(methodColdHeader-&gt;hotHeader))\n    {\n        // Matched the cold code\n        m_pCMH = PTR_CORCOMPILE_METHOD_COLD_HEADER((TADDR)methodCold);\n        ...\n</code></pre> <p>Both methodCold and methodCode are declared as <code>BYTE *</code>, but in fact hold target addresses. In line 4, methodCold is casted to a <code>TADDR</code> and used as the argument to the constructor for <code>PTR_CORCOMPILE_METHOD_COLD_HEADER</code>. At this point, <code>methodColdHeader</code> is explicitly a target address. In line 6, there is another C-style cast for <code>methodCode</code>. The hotHeader field of <code>methodColdHeader</code> is of type <code>PTR_CORCOMPILE_METHOD_HEADER</code>. The macro <code>PTR_TO_TADDR</code> extracts the raw target address from this <code>PTR</code> type and assigns it to <code>methodCode</code>. Finally, in line 9,  another instance of type <code>PTR_CORCOMPILE_METHOD_COLD_HEADER</code> is constructed. Again, <code>methodCold</code> is casted to <code>TADDR</code> to pass to this constructor.</p> <p>If this code seems overly complex and confusing to you, that's good. In fact it is. Worse, it provides no protection for the separation of host and target addresses. From the declarations of <code>methodCold</code> and <code>methodCode</code>, there is no particular reason to interpret them as target addresses at all. If these pointers were dereferenced in DAC builds as if they really were host pointers, the process would probably AV. This snippet demonstrates that any arbitrary pointer type (as opposed to a <code>PTR</code> type) can be casted to a <code>TADDR</code>. Given that these two variables always hold target addresses, they should be of type <code>PTR_BYTE</code>, rather than <code>BYTE *</code>.</p> <p>There is also a disciplined means to cast between different <code>PTR</code> types: <code>dac_cast</code>. The <code>dac_cast</code> operator is the DAC-aware version of the C++ <code>static_cast</code> operator (which the CLR coding conventions stipulate instead of C-style casts when casting pointer types). The <code>dac_cast</code> operator will do any of the following things:</p> <ol> <li>Create a <code>PTR</code> type from a <code>TADDR</code></li> <li>Convert one <code>PTR</code> type to another</li> <li>Create a <code>PTR</code> from a host instance previously marshaled to the DAC cache</li> <li>Extract the <code>TADDR</code> from a <code>PTR</code> type</li> <li>Get a <code>TADDR</code> from a host instance previously marshaled to the DAC cache</li> </ol> <p>Now, assuming both methodCold and methodCode are declared to be of type <code>PTR_BYTE</code>, the code above can be rewritten as follows.</p> <pre><code>if (methodCold)\n{\n    PTR_CORCOMPILE_METHOD_COLD_HEADER methodColdHeader\n                = dac_cast&lt;PTR_CORCOMPILE_METHOD_COLD_HEADER&gt;(methodCold);\n\n    if (methodCode == methodColdHeader-&gt;hotHeader)\n    {\n        // Matched the cold code\n        m_pCMH = methodColdHeader;\n</code></pre> <p>You might argue that this code still seems complex and confusing, but at least we have significantly reduced the number of casts and constructors. We have also used constructs that maintain the separation between host and target pointers, so we have made the code safer. In particular, <code>dac_cast</code> will often generate compiler or run-time errors if we try to do the wrong thing. In general, <code>dac_cast</code> should be used for conversions.</p>"},{"location":"dac-notes/#dacizing","title":"DACizing","text":""},{"location":"dac-notes/#when-do-you-need-to-dacize","title":"When do you need to DACize?","text":"<p>Whenever you add a new feature, you will need to consider its debuggability needs and DACize the code to support your feature. You must also ensure that any other changes, such as bug fixes or code clean-up, conform to the DAC rules when necessary. Otherwise, the changes will break the debugger or SOS. If you are simply modifying existing code (as opposed to implementing a new feature), you will generally be able to determine that you need to worry about the DAC when a function you modify includes a <code>SUPPORTS_DAC</code> contract. This contract has a few variants such as <code>SUPPORTS_DAC_WRAPPER</code> and <code>LEAF_DAC_CONTRACT</code>. You can find comments explaining the differences in contract.h. If you see a number of DAC-specific types in the function, you should assume the code will run in DAC builds.</p> <p>DACizing ensures that code in the engine will work correctly with the DAC. It is important to use the DAC correctly to marshal values from the target to the host. Target addresses used incorrectly from the host (or vice versa) may reference unmapped addresses. If addresses are mapped, the values will be completely unrelated to the values expected. As a result, DACizing mostly involves ensuring that we use <code>PTR</code> types for all values that the DAC needs to marshal. Another major task is to ensure that we do not allow invasive code to execute in DAC builds. In practice, this means that we must sometimes refactor code or add <code>DACCESS_COMPILE</code> preprocessor directives. We also want to be sure that we add the appropriate <code>SUPPORTS_DAC</code> contract. The use of this contract signals to developers that the function works with the DAC. This is important for two reasons:</p> <ol> <li>If we later call it from some other <code>SUPPORTS_DAC</code> function, we know that it is DAC-safe and we don't need to worry about DACizing it.</li> <li>If we make modifications to the function, we need to make sure that they are DAC-safe. If we add a call to another function from this one, we also need to ensure that it is DAC-safe or that we only make the call in non-DAC builds.</li> </ol>"},{"location":"exceptions/","title":"What Every Dev needs to Know About Exceptions in the Runtime","text":"<p>Date: 2005</p> <p>When talking about \"exceptions\" in the CLR, there is an important distinction to keep in mind. There are managed exceptions, which are exposed to applications through mechanisms like C#'s try/catch/finally, with all of the runtime machinery to implement them. And then there is the use of exceptions inside the runtime itself. Most runtime developers seldom need to think about how to build and expose the managed exception model. But every runtime developer needs to understand how exceptions are used in the implementation of the runtime. When there is a need to keep the distinction clear, this document will refer to managed exceptions that a managed application may throw or catch, and will refer to the CLR's internal exceptions that are used by the runtime for its own error handling. Mostly, though, this document is about the CLR's internal exceptions.</p>"},{"location":"exceptions/#where-do-exceptions-matter","title":"Where do exceptions matter?","text":"<p>Exceptions matter almost everywhere. They matter the most in functions that throw or catch exceptions, because that code must be written explicitly to throw the exception, or to catch and properly handle an exception. Even if a particular function doesn't itself throw an exception, it may well call one that does, and so that particular function must be written to behave correctly when an exception is thrown through it. The judicious use of holders can greatly ease writing such code correctly.</p>"},{"location":"exceptions/#why-are-clr-internal-exceptions-different","title":"Why are CLR internal exceptions different?","text":"<p>The CLR's internal exceptions are much like C++ exceptions, but not exactly. CoreCLR can be built for Mac OSX, for Linux, for BSD, and for Windows. The OS and compiler differences dictate that we can't just use standard C++ try/catch. In addition, the CLR internal exceptions provide features similar to the managed \"finally\" and \"fault\".</p> <p>With the help of some macros, it is possible to write exception handling code that is almost as easy to write and to read as standard C++.</p>"},{"location":"exceptions/#catching-an-exception","title":"Catching an Exception","text":""},{"location":"exceptions/#ex_try","title":"EX_TRY","text":"<p>The basic macros are, of course, EX_TRY / EX_CATCH / EX_END_CATCH, and in use they look like this:</p> <pre><code>EX_TRY\n  // Call some function.  Maybe it will throw an exception.\n  Bar();\nEX_CATCH\n  // If we're here, something failed.\n  m_finalDisposition = terminallyHopeless;\n  RethrowTransientExceptions();\nEX_END_CATCH\n</code></pre> <p>The EX_TRY macro simply introduces the try block, and is much like the C++ \"try\", except that it also includes an opening brace, \"{\".</p>"},{"location":"exceptions/#ex_catch","title":"EX_CATCH","text":"<p>The EX_CATCH macro ends the try block, including the closing brace, \"}\", and begins the catch block. Like the EX_TRY, it also starts the catch block with an opening brace.</p> <p>And here is the big difference from C++ exceptions: the CLR developer doesn't get to specify what to catch. In fact, this set of macros catches everything, including non-C++ exceptions like AV or a managed exception. If a bit of code needs to catch just one exception, or a subset, then it will need to catch, examine the exception, and rethrow anything that isn't relevant.</p> <p>It bears repeating that the EX_CATCH macro catches everything. This behaviour is frequently not what a function needs. The next two sections discuss more about how to deal with exceptions that shouldn't have been caught.</p>"},{"location":"exceptions/#get_exception-get_throwable","title":"GET_EXCEPTION() &amp; GET_THROWABLE()","text":"<p>How, then, does a CLR developer discover just what has been caught, and determine what to do? There are several options, depending on just what the requirement is.</p> <p>First, whatever the (C++) exception that is caught, it will be delivered as an instance of some class derived from the global Exception class. Some of these derived classes are pretty obvious, like OutOfMemoryException. Some are somewhat domain specific, like EETypeLoadException. And some of these are just wrapper classes around another system's exceptions, like CLRException (has an OBJECTHANDLE to reference any managed exception) or HRException (wraps an HRESULT). If the original exception was not derived from Exception, the macros will wrap it up in something that is.  (Note that all of these exceptions are system-provided and well known. New exception classes shouldn't be added without involving the Core Execution Engine Team!)</p> <p>Next, there is always an HRESULT associated with a CLR internal exception. Sometimes, as with HRException, the value came from some COM source, but internal errors and Win32 api failures also have HRESULTS.</p> <p>Finally, because almost any exception inside the CLR could possibly be delivered back to managed code, there is a mapping from the internal exceptions back to the corresponding managed exceptions. The managed exception won't necessarily be created, but there is always the possibility of obtaining it.</p> <p>So, given these features, how does the CLR developer categorize an exception?</p> <p>Frequently, all that is needed to categorize is the HRESULT that corresponds to the exception, and this is extremely easy to get:</p> <pre><code>HRESULT hr = GET_EXCEPTION()-&gt;GetHR();\n</code></pre> <p>More information is often most conveniently available through the managed exception object. And if the exception will be delivered back to managed code, whether immediately, or cached for later, the managed object is, of course, required. And the exception object is just as easy to get. Of course, it is a managed objectref, so all the usual rules apply:</p> <pre><code>OBJECTREF throwable = NULL;\nGCPROTECT_BEGIN(throwable);\n// . . .\nEX_TRY\n    // . . . do something that might throw\nEX_CATCH\n    throwable = GET_THROWABLE();\n    RethrowTransientExceptions();\nEX_END_CATCH\n// . . . do something with throwable\nGCPROTECT_END()\n</code></pre> <p>Sometimes, there is no avoiding a need for the C++ exception object, though this is mostly inside the exception implementation. If it is important exactly what the C++ exception type is, there is a set of lightweight RTTI-like functions that help categorize exceptions. For instance,</p> <pre><code>Exception *pEx = GET_EXCEPTION();\nif (pEx-&gt;IsType(CLRException::GetType())) {/* ... */}\n</code></pre> <p>would tell whether the exception is (or derives from) CLRException.</p>"},{"location":"exceptions/#rethrowtransientexceptions","title":"RethrowTransientExceptions","text":"<p>In the example above, \"RethrowTransientExceptions\" is a macro in the <code>EX_CATCH</code> block; it is one of three pre-defined macros that can be thought of \"exception disposition\". Here are the macros, and their meanings:</p> <ul> <li>RethrowTerminalExceptions. A better name would be \"RethrowThreadAbort\", which is what this macro does.</li> <li>RethrowTransientExceptions. The best definition of a \"transient\" exception is one that might not occur if tried again, possibly in a different context. These are the transient exceptions:</li> <li>COR_E_THREADABORTED</li> <li>COR_E_THREADINTERRUPTED</li> <li>COR_E_THREADSTOP</li> <li>COR_E_APPDOMAINUNLOADED</li> <li>E_OUTOFMEMORY</li> <li>HRESULT_FROM_WIN32(ERROR_COMMITMENT_LIMIT)</li> <li>HRESULT_FROM_WIN32(ERROR_NOT_ENOUGH_MEMORY)</li> <li>(HRESULT)STATUS_NO_MEMORY</li> <li>COR_E_STACKOVERFLOW</li> <li>MSEE_E_ASSEMBLYLOADINPROGRESS</li> </ul> <p>The CLR developer with doubts about which macro to use should probably pick RethrowTransientExceptions.</p> <p>In every case, however, the developer writing an EX_CATCH block needs to think hard about which exception should be caught, and should catch only those exceptions. And, because the macros catch everything anyway, the only way to not catch an exception is to rethrow it.</p>"},{"location":"exceptions/#ex_catch_hresult","title":"EX_CATCH_HRESULT","text":"<p>Sometimes all that is needed is the HRESULT corresponding to an exception, particularly when the code is in an interface from COM. For these cases, EX_CATCH_HRESULT is simpler than writing a while EX_CATCH block. A typical case would look like this:</p> <pre><code>HRESULT hr;\nEX_TRY\n  // code\nEX_CATCH_HRESULT (hr)\n\nreturn hr;\n</code></pre> <p>However, while very tempting, it is not always correct. The EX_CATCH_HRESULT catches all exceptions, saves the HRESULT, and swallows the exception. So, unless that exception swallowing is what the function really needs, EX_CATCH_HRESULT is not appropriate.</p>"},{"location":"exceptions/#ex_rethrow","title":"EX_RETHROW","text":"<p>As noted above, the exception macros catch all exceptions; the only way to catch a specific exception is to catch all, and rethrow all but the one(s) of interest. So, if, after an exception is caught, examined, possibly logged, and so forth, it shouldn't be caught, it may be re-thrown. EX_RETHROW will re-raise the same exception.</p>"},{"location":"exceptions/#not-catching-an-exception","title":"Not catching an exception","text":"<p>It's frequently the case that a bit of code doesn't need to catch an exception, but does need to perform some sort of cleanup or compensating action, Holders are frequently just the thing for this scenario, but not always. For the times that holders aren't adequate, the CLR has two variations on a \"finally\" block.</p>"},{"location":"exceptions/#ex_try_for_finally","title":"EX_TRY_FOR_FINALLY","text":"<p>When there is a need for some sort of compensating action as code exits, a finally may be appropriate. There is a set of macros to implement a try/finally in the CLR:</p> <pre><code>EX_TRY_FOR_FINALLY\n  // code\nEX_FINALLY\n  // exit and/or backout code\nEX_END_FINALLY\n</code></pre> <p>Important : The EX_TRY_FOR_FINALLY macros are built with SEH, rather than C++ EH, and the C++ compiler doesn't allow SEH and C++ EH to be mixed in the same function. Locals with auto-destructors require C++ EH for their destructor to run. Therefore, any function with EX_TRY_FOR_FINALLY can't have EX_TRY, and can't have any local variable with an auto-destructor.</p>"},{"location":"exceptions/#ex_hook","title":"EX_HOOK","text":"<p>Frequently there is a need for compensating code, but only when an exception is thrown. For these cases, EX_HOOK is similar to EX_FINALLY, but the \"hook\" clause only runs when there is an exception. The exception is automatically rethrown at the end of the \"hook\" clause.</p> <pre><code>EX_TRY\n  // code\nEX_HOOK\n  // code to run when an exception escapes the \"code\" block.\nEX_END_HOOK\n</code></pre> <p>This construct is somewhat better than simply EX_CATCH with EX_RETHROW, because it will rethrow a non-stack-overflow, but will catch a stack overflow exception (and unwind the stack) and then throw a new stack overflow exception.</p>"},{"location":"exceptions/#throwing-an-exception","title":"Throwing an Exception","text":"<p>Throwing an Exception in the CLR is generally a matter of calling</p> <pre><code>COMPlusThrow ( &lt; args &gt; )\n</code></pre> <p>There are a number of overloads, but the idea is to pass the \"kind\" of the exception to COMPlusThrow. The list of \"kinds\" is generated by a set of macros operating on rexcep.h, and the various \"kinds\" are kAmbiguousMatchException, kApplicationException, and so forth. Additional arguments (for the overloads) specify resources and substitution text. Generally, the right \"kind\" is selected by looking for other code that reports a similar error.</p> <p>There are some pre-defined convenience variations:</p>"},{"location":"exceptions/#complusthrowoom","title":"COMPlusThrowOOM();","text":"<p>Defers to ThrowOutOfMemory(), which throws the C++ OOM exception. This will throw a pre-allocated exception, to avoid the problem of being out of memory trying to throw an out of memory exception!</p> <p>When getting the managed exception object for this exception, the runtime will first try to allocate a new managed object <sup>[1]</sup>, and if that fails, will return a pre-allocated, shared, global out of memory exception object.</p> <p>[1] After all, if it was a request for a 2gb array that failed, a simple object may be fine.</p>"},{"location":"exceptions/#complusthrowhrhresult-thebadhr","title":"COMPlusThrowHR(HRESULT theBadHR);","text":"<p>There are a number of overloads, in case you have an IErrorInfo, etc. There is some surprisingly complicated code to figure out what kind of exception corresponds to a particular HRESULT.</p>"},{"location":"exceptions/#complusthrowwin32-complusthrowwin32hr","title":"COMPlusThrowWin32(); / COMPlusThrowWin32(hr);","text":"<p>Basically throws an HRESULT_FROM_WIN32(GetLastError())</p>"},{"location":"exceptions/#complusthrowso","title":"COMPlusThrowSO();","text":"<p>Throws a Stack Overflow (SO) Exception. Note that this is not a hard SO, but rather an exception we throw when proceeding might lead to a hard SO.</p> <p>Like OOM, this throws a pre-allocated C++ SO exception object. Unlike OOM, when retrieving the managed object, the runtime always returns the pre-allocated, shared, global stack overflow exception object.</p>"},{"location":"exceptions/#complusthrowargumentnull","title":"COMPlusThrowArgumentNull()","text":"<p>A helper for throwing an \"argument foo must not be null\" exception.</p>"},{"location":"exceptions/#complusthrowargumentoutofrange","title":"COMPlusThrowArgumentOutOfRange()","text":"<p>As it sounds.</p>"},{"location":"exceptions/#complusthrowargumentexception","title":"COMPlusThrowArgumentException()","text":"<p>Yet another flavor of invalid argument exception.</p>"},{"location":"exceptions/#complusthrowinvalidcastexceptionthfrom-thto","title":"COMPlusThrowInvalidCastException(thFrom, thTo)","text":"<p>Given type handles to from and to types of the attempted cast, the helper creates a nicely formatted exception message.</p>"},{"location":"exceptions/#ex_throw","title":"EX_THROW","text":"<p>This is a low-level throw construct that is not generally needed in normal code. Many of the COMPlusThrowXXX functions use EX_THROW internally, as do other specialized ThrowXXX functions.  It is best to minimize direct use of EX_THROW, simply to keep the nitty-gritty details of the exception mechanism as well encapsulated as possible. But when none of the higher-level Throw functions work, it is fine to use EX_THROW.</p> <p>The macro takes two arguments, the type of exception to be thrown (some sub-type of the C++ Exception class), and a parenthesized list of arguments to the exception type's constructor.</p>"},{"location":"exceptions/#using-seh-directly","title":"Using SEH directly","text":"<p>There are a few situations where it is appropriate to use SEH directly. In particular, SEH is the only option if some processing is needed on the first pass, that is, before the stack is unwound. The filter code in an SEH __try/__except can do anything, in addition to deciding whether to handle an exception. Debugger notifications is an area that sometimes needs first pass handling.</p> <p>Filter code needs to be written very carefully. In general, the filter code must be prepared for any random, and likely inconsistent, state. Because the filter runs on the first pass, and dtors run on the second pass, holders won't have run yet, and will not have restored their state.</p>"},{"location":"exceptions/#pal_try-pal_except-pal_except_filter-pal_finally-pal_endtry","title":"PAL_TRY / PAL_EXCEPT, PAL_EXCEPT_FILTER, PAL_FINALLY / PAL_ENDTRY","text":"<p>When a filter is needed, the PAL_TRY family is the portable way to write one in the CLR. Because the filter uses SEH directly, it is incompatible with C++ EH in the same function, and so there can't be any holders in the function.</p> <p>Again, these should be rare.</p>"},{"location":"exceptions/#__try-__except-__finally","title":"__try / __except, __finally","text":"<p>There isn't a good reason to use these directly in the CLR.</p>"},{"location":"exceptions/#exceptions-and-gc-mode","title":"Exceptions and GC mode","text":"<p>Throwing an exception with COMPlusThrowXXX() doesn't affect the GC mode, and is safe in any mode. As the exception unwinds back to the EX_CATCH, any holders that were on the stack will be unwound, releasing their resources and resetting their state. By the time that execution resumes in the EX_CATCH, the holder-protected state will have been restored to what it was at the time of the EX_TRY.</p>"},{"location":"exceptions/#transitions","title":"Transitions","text":"<p>Considering managed code, the CLR, COM servers, and other native code, there are many possible transitions between calling conventions, memory management, and, of course, exception handling mechanisms. Regarding exceptions, it is fortunate for the CLR developer that most of these transitions are either completely outside of the runtime, or are handled automatically.  There are three transitions that are a daily concern for a CLR developer. Anything else is an advanced topic, and those who need to know about them, are well aware that they need to know!</p>"},{"location":"exceptions/#managed-code-into-the-runtime","title":"Managed code into the runtime","text":"<p>This is the \"fcall\", \"jit helper\", and so forth. The typical way that the runtime reports errors back to managed code is through a managed exception. So, if an fcall function, directly or indirectly, raises a managed exception, that's perfectly fine. The normal CLR managed exception implementation will \"do the right thing\" and look for an appropriate managed handler.</p> <p>On the other hand, if an fcall function can do anything that might throw a CLR internal exception (one of the C++ exceptions), that exception must not be allowed to leak back out to managed code. To handle this case, the CLR has the UnwindAndContinueHandler (UACH), which is a set of code to catch the C++ EH exceptions, and re-raise them as managed exceptions.</p> <p>Any runtime function that is called from managed code, and might throw a C++ EH exception, must wrap the throwing code in INSTALL_UNWIND_AND_CONTINUE_HANDLER / UNINSTALL_UNWIND_AND_CONTINUE_HANDLER. There is a non-trivial amount of overhead to installing a UACH, so they shouldn't be used everywhere. One technique that is used in performance critical code is to run without a UACH, and install one just before throwing an exception.</p> <p>When a C++ exception is thrown, and there is a missing UACH, the typical failure will be a Contract Violation of \"GC_TRIGGERS called in a GC_NOTRIGGER region\" in CPFH_RealFirstPassHandler. To fix these, look for managed to runtime transitions, and check for INSTALL_UNWIND_AND_CONTINUE_HANDLER.</p>"},{"location":"exceptions/#runtime-code-into-managed-code","title":"Runtime code into managed code","text":"<p>The transition from the runtime into managed code has highly platform-dependent requirements. On 32-bit Windows platforms, the CLR's managed exception code requires that \"COMPlusFrameHandler\" is installed just before entering managed code. These transitions are handled by highly specialized helper functions, which take care of the appropriate exception handlers. It is very unlikely that any typical new calls into managed would use any other way in. In the event that the COMPlusFrameHandler were missing, the most likely effect would be that exception handling code in the target managed code simply wouldn't be executed \u2013 no finally blocks, and no catch blocks.</p>"},{"location":"exceptions/#runtime-code-into-external-native-code","title":"Runtime code into external native code","text":"<p>Calls from the runtime into other native code (the OS, the CRT, and other DLLs) may need particular attention. The cases that matter are those in which the external code might cause an exception. The reason that this is a problem comes from the implementation of the EX_TRY macros, and in particular how they translate or wrap non-Exceptions into Exceptions. With C++ EH, it is possible to catch any and all exceptions (via \"catch(...)\"), but only by giving up all information about what has been caught. When catching an Exception*, the macros have the exception object to examine, but when catching anything else, there is nothing to examine, and the macros must guess what the actual exception is. And when the exception comes from outside of the runtime, the macros will always guess wrong.</p> <p>The current solution is to wrap the call to external code in a \"callout filter\". The filter will catch the external exception, and translate it into SEHException, one of the runtime's internal exceptions. This filter is predefined, and is simple to use. However, using a filter means using SEH, which of course precludes using C++ EH in the same function. To add a callout filter to a function that uses C++ EH will require splitting a function in two.</p> <p>To use the callout filter, instead of this:</p> <pre><code>length = SysStringLen(pBSTR);\n</code></pre> <p>write this:</p> <pre><code>BOOL OneShot = TRUE;\nstruct Param {\n    BSTR*  pBSTR;\n    int length;\n};\nstruct Param param;\nparam.pBSTR = pBSTR;\n\nPAL_TRY(Param*, pParam, &amp;param)\n{\n  pParam-&gt;length = SysStringLen(pParam-&gt;pBSTR);\n}\nPAL_EXCEPT_FILTER(CallOutFilter, &amp;OneShot)\n{\n  _ASSERTE(!\"CallOutFilter returned EXECUTE_HANDLER.\");\n}\nPAL_ENDTRY;\n</code></pre> <p>A missing callout filter on a call that raises an exception will always result in the wrong exception being reported in the runtime. The type that is incorrectly reported isn't even always deterministic; if there is already some managed exception \"in flight\", then that managed exception is what will be reported. If there is no current exception, then OOM will be reported. On a checked build there are asserts that usually fire for a missing callout filter. These assert messages will include the text \"The runtime may have lost track of the type of an exception\".</p>"},{"location":"exceptions/#miscellaneous","title":"Miscellaneous","text":"<p>There are actually a lot of macros involved in EX_TRY. Most of them should never, ever, be used outside of the macro implementations.</p>"},{"location":"garbage-collection/","title":"Garbage Collection Design","text":"<p>Author: Maoni Stephens (@maoni0) - 2015</p> <p>Note: See The Garbage Collection Handbook to learn more about garbage collection topics in general; for specific knowledge on the CLR GC please refer to the Pro .NET Memory Management book. Both referenced in the resources section at the end of this document.</p>"},{"location":"garbage-collection/#component-architecture","title":"Component Architecture","text":"<p>The 2 components that belong to GC are the allocator and the collector. The allocator is responsible for getting more memory and triggering the collector when appropriate. The collector reclaims garbage, or the memory of objects that are no longer in use by the program.</p> <p>There are other ways that the collector can get called, such as manually calling GC.Collect or the finalizer thread receiving an asynchronous notification of the low memory (which triggers the collector).</p>"},{"location":"garbage-collection/#design-of-allocator","title":"Design of Allocator","text":"<p>The allocator gets called by the allocation helpers in the Execution Engine (EE), with the following information:</p> <ul> <li>Size requested</li> <li>Thread allocation context</li> <li>Flags that indicate things like whether this is a finalizable object or not</li> </ul> <p>The GC does not have special treatment for different kinds of object types. It consults the EE to get the size of an object.</p> <p>Based on the size, the GC divides objects into 2 categories: small objects (&lt; 85,000 bytes) and large objects (&gt;= 85,000 bytes). In principle, small and large objects can be treated the same way but since compacting large objects is more expensive GC makes this distinction.</p> <p>When the GC gives out memory to the allocator, it does so in terms of allocation contexts. The size of an allocation context is defined by the allocation quantum.</p> <ul> <li>Allocation contexts are smaller regions of a given heap segment that are each dedicated for use by a given thread. On a single-processor (meaning 1 logical processor) machine, a single context is used, which is the generation 0 allocation context.</li> <li>The Allocation quantum is the size of memory that the allocator allocates each time it needs more memory, in order to perform object allocations within an allocation context. The allocation is typically 8k and the average size of managed objects are around 35 bytes, enabling a single allocation quantum to be used for many object allocations.</li> </ul> <p>Large objects do not use allocation contexts and quantums. A single large object can itself be larger than these smaller regions of memory. Also, the benefits (discussed below) of these regions are specific to smaller objects. Large objects are allocated directly to a heap segment.</p> <p>The allocator is designed to achieve the following:</p> <ul> <li>Triggering a GC when appropriate: The allocator triggers a GC when the allocation budget (a threshold set by the collector) is exceeded or when the allocator can no longer allocate on a given segment. The allocation budget and managed segments are discussed in more detail later.</li> <li>Preserving object locality: Objects allocated together on the same heap segment will be stored at virtual addresses close to each other.</li> <li>Efficient cache usage: The allocator allocates memory in allocation quantum units, not on an object-by-object basis. It zeroes out that much memory to warm up the CPU cache because there will be objects immediately allocated in that memory. The allocation quantum is usually 8k.</li> <li>Efficient locking: The thread affinity of allocation contexts and quantums guarantee that there is only ever a single thread writing to a given allocation quantum. As a result, there is no need to lock for object allocations, as long as the current allocation context is not exhausted.</li> <li>Memory integrity: The GC always zeroes out the memory for newly allocated objects to prevent object references pointing at random memory.</li> <li>Keeping the heap crawlable: The allocator makes sure to make a free object out of left over memory in each allocation quantum. For example, if there is 30 bytes left in an allocation quantum and the next object is 40 bytes, the allocator will make the 30 bytes a free object and get a new allocation quantum.</li> </ul>"},{"location":"garbage-collection/#allocation-apis","title":"Allocation APIs","text":"<pre><code> Object* GCHeap::Alloc(size_t size, DWORD flags);\n Object* GCHeap::Alloc(alloc_context* acontext, size_t size, DWORD flags);\n</code></pre> <p>The above functions can be used to allocate both small objects and large objects. There is also a function to allocate directly on LOH:</p> <pre><code> Object* GCHeap::AllocLHeap(size_t size, DWORD flags);\n</code></pre>"},{"location":"garbage-collection/#design-of-the-collector","title":"Design of the Collector","text":""},{"location":"garbage-collection/#goals-of-the-gc","title":"Goals of the GC","text":"<p>The GC strives to manage memory extremely efficiently and require very little effort from people who write \"managed code\". Efficient means:</p> <ul> <li>GCs should occur often enough to avoid the managed heap containing a significant amount (by ratio or absolute count) of unused but allocated objects (garbage), and therefore use memory unnecessarily.</li> <li>GCs should happen as infrequently as possible to avoid using otherwise useful CPU time, even though frequent GCs would result in lower memory usage.</li> <li>A GC should be productive. If GC reclaims a small amount of memory, then the GC (including the associated CPU cycles) was wasted.</li> <li>Each GC should be fast. Many workloads have low latency requirements.</li> <li>Managed code developers shouldn't need to know much about the GC to achieve good memory utilization (relative to their workload). \u2013 The GC should tune itself to satisfy different memory usage patterns.</li> </ul>"},{"location":"garbage-collection/#logical-representation-of-the-managed-heap","title":"Logical representation of the managed heap","text":"<p>The CLR GC is a generational collector which means objects are logically divided into generations. When a generation N is collected, the survived objects are now marked as belonging to generation N+1. This process is called promotion. There are exceptions to this when we decide to demote or not promote.</p> <p>For small objects the heap is divided into 3 generations: gen0, gen1 and gen2. For large objects there's one generation \u2013 gen3. Gen0 and gen1 are referred to as ephemeral (objects lasting for a short time) generations.</p> <p>For the small object heap, the generation number represents the age \u2013 gen0 being the youngest generation. This doesn't mean all objects in gen0 are younger than any objects in gen1 or gen2. There are exceptions which will be explained below. Collecting a generation means collecting objects in that generation and all of its younger generations.</p> <p>In principle large objects can be handled the same way as small objects but since compacting large objects is very expensive, they are treated differently. There is only one generation for large objects and they are always collected with gen2 collections due to performance reasons. Both gen2 and gen3 can be big, and collecting ephemeral generations (gen0 and gen1) needs to have a bounded cost.</p> <p>Allocations are made in the youngest generation \u2013 for small objects this means always gen0 and for large objects this means gen3 since there's only one generation.</p>"},{"location":"garbage-collection/#physical-representation-of-the-managed-heap","title":"Physical representation of the managed heap","text":"<p>The managed heap is a set of managed heap segments. A heap segment is a contiguous block of memory that is acquired by the GC from the OS. Heap segments can be small, large, or pinned object segments depending on what they contain. On each heap the heap segments are chained together. There is at least one small object segment and one large segment - they are reserved when CLR is loaded. There is also a NonGC heap that contains ro (readonly) segments.</p> <p>There's always only one ephemeral segment in each small object heap, which is where gen0 and gen1 live. This segment may or may not include gen2 objects. In addition to the ephemeral segment, there can be zero, one or more additional segments, which will be gen2 segments since they only contain gen2 objects.</p> <p>There are 1 or more segments on the large object heap.</p> <p>A heap segment is consumed from the lower address to the higher address, which means objects of lower addresses on the segment are older than those of higher addresses. Again there are exceptions that will be described below.</p> <p>Heap segments can be acquired as needed. They are  deleted when they don't contain any live objects, however the initial segment on the heap will always exist. For each heap, one segment at a time is acquired, which is done during a GC for small objects and during allocation time for large objects. This design provides better performance because large objects are only collected with gen2 collections (which are relatively expensive).</p> <p>Heap segments are chained together in order of when they were acquired. The last segment in the chain is always the ephemeral segment. Collected segments (no live objects) can be reused instead of deleted and instead become the new ephemeral segment. Segment reuse is only implemented for small object heap. Each time a large object is allocated, the whole large object heap is considered. Small object allocations only consider the ephemeral segment.</p>"},{"location":"garbage-collection/#the-allocation-budget","title":"The allocation budget","text":"<p>The allocation budget is a logical concept associated with each generation. It is a size limit that triggers a GC for that generation when exceeded.</p> <p>The budget is a property set on the generation mostly based on the survival rate of that generation. If the survival rate is high, the budget is made larger with the expectation that there will be a better ratio of dead to live objects next time there is a GC for that generation.</p>"},{"location":"garbage-collection/#determining-which-generation-to-collect","title":"Determining which generation to collect","text":"<p>When a GC is triggered, the GC must first determine which generation to collect. Besides the allocation budget there are other factors that must be considered:</p> <ul> <li>Fragmentation of a generation \u2013 if a generation has high fragmentation, collecting that generation is likely to be productive.</li> <li>If the memory load on the machine is too high, the GC may collect   more aggressively if that's likely to yield free space. This is important to   prevent unnecessary paging (across the machine).</li> <li>If the ephemeral segment is running out of space, the GC may do more aggressive ephemeral collections (meaning doing more gen1's) to avoid acquiring a new heap segment.</li> </ul>"},{"location":"garbage-collection/#the-flow-of-a-gc","title":"The flow of a GC","text":""},{"location":"garbage-collection/#mark-phase","title":"Mark phase","text":"<p>The goal of the mark phase is to find all live objects.</p> <p>The benefit of a generational collector is the ability to collect just part of the heap instead of having to look at all of the objects all the time. When  collecting the ephemeral generations, the GC needs to find out which objects are live in these generations, which is information reported by the EE. Besides the objects kept live by the EE, objects in older generations can also keep objects in younger generations live by making references to them.</p> <p>The GC uses cards for the older generation marking. Cards are set by JIT helpers during assignment operations. If the JIT helper sees an object in the ephemeral range it will set the byte that contains the card representing the source location. During ephemeral collections, the GC can look at the set cards for the rest of the heap and only look at the objects that these cards correspond to.</p>"},{"location":"garbage-collection/#plan-phase","title":"Plan phase","text":"<p>The plan phase simulates a compaction to determine the effective result. If compaction is productive the GC starts an actual compaction; otherwise it sweeps.</p>"},{"location":"garbage-collection/#relocate-phase","title":"Relocate phase","text":"<p>If the GC decides to compact, which will result in moving objects, then  references to these objects must be updated. The relocate phase needs to find all references that point to objects that are in the generations being collected. In contrast, the mark phase only consults live objects so it doesn't need to consider weak references.</p>"},{"location":"garbage-collection/#compact-phase","title":"Compact phase","text":"<p>This phase is very straight forward since the plan phase already calculated the new addresses the objects should move to. The compact phase will copy the objects there.</p>"},{"location":"garbage-collection/#sweep-phase","title":"Sweep phase","text":"<p>The sweep phase looks for the dead space in between live objects. It creates free objects in place of these dead spaces. Adjacent dead objects are made into one free object. It places all of these free objects onto the freelist.</p>"},{"location":"garbage-collection/#code-flow","title":"Code Flow","text":"<p>Terms:</p> <ul> <li>WKS GC: Workstation GC</li> <li>SVR GC: Server GC</li> </ul>"},{"location":"garbage-collection/#functional-behavior","title":"Functional Behavior","text":""},{"location":"garbage-collection/#wks-gc-with-concurrent-gc-off","title":"WKS GC with concurrent GC off","text":"<ol> <li>User thread runs out of allocation budget and triggers a GC.</li> <li>GC calls SuspendEE to suspend managed threads.</li> <li>GC decides which generation to condemn.</li> <li>Mark phase runs.</li> <li>Plan phase runs and decides if a compacting GC should be done.</li> <li>If so relocate and compact phase runs. Otherwise, sweep phase runs.</li> <li>GC calls RestartEE to resume managed threads.</li> <li>User thread resumes running.</li> </ol>"},{"location":"garbage-collection/#wks-gc-with-concurrent-gc-on","title":"WKS GC with concurrent GC on","text":"<p>This illustrates how a background GC is done.</p> <ol> <li>User thread runs out of allocation budget and triggers a GC.</li> <li>GC calls SuspendEE to suspend managed threads.</li> <li>GC decides if background GC should be run.</li> <li>If so background GC thread is woken up to do a background    GC. Background GC thread calls RestartEE to resume managed threads.</li> <li>Managed threads continue allocating while the background GC does its work.</li> <li>User thread may run out of allocation budget and trigger an    ephemeral GC (what we call a foreground GC). This is done in the same    fashion as the \"WKS GC with concurrent GC off\" flavor.</li> <li>Background GC calls SuspendEE again to finish with marking and then    calls RestartEE to start the concurrent sweep phase while user threads    are running.</li> <li>Background GC is finished.</li> </ol>"},{"location":"garbage-collection/#svr-gc-with-concurrent-gc-off","title":"SVR GC with concurrent GC off","text":"<ol> <li>User thread runs out of allocation budget and triggers a GC.</li> <li>Server GC threads are woken up and call SuspendEE to suspend    managed threads.</li> <li>Server GC threads do the GC work (same phases as in workstation GC    without concurrent GC).</li> <li>Server GC threads call RestartEE to resume managed threads.</li> <li>User thread resumes running.</li> </ol>"},{"location":"garbage-collection/#svr-gc-with-concurrent-gc-on","title":"SVR GC with concurrent GC on","text":"<p>This scenario is the same as WKS GC with concurrent GC on, except the non background GCs are done on SVR GC threads.</p>"},{"location":"garbage-collection/#physical-architecture","title":"Physical Architecture","text":"<p>This section is meant to help you follow the code flow.</p> <p>User thread runs out of space in an allocation context and gets a new one via try_allocate_more_space.</p> <p>try_allocate_more_space calls GarbageCollectGeneration when it needs to trigger a GC.</p> <p>Given WKS GC with concurrent GC off, GarbageCollectGeneration is done all on the user thread that triggered the GC. The code flow is:</p> <pre><code> GarbageCollectGeneration()\n {\n     SuspendEE();\n     garbage_collect();\n     RestartEE();\n }\n\n garbage_collect()\n {\n     generation_to_condemn();\n     gc1();\n }\n\n gc1()\n {\n     mark_phase();\n     plan_phase();\n }\n\n plan_phase()\n {\n     // actual plan phase work to decide to\n     // compact or not\n     if (compact)\n     {\n         relocate_phase();\n         compact_phase();\n     }\n     else\n         make_free_lists();\n }\n</code></pre> <p>Given WKS GC with concurrent GC on (default case), the code flow for a background GC is</p> <pre><code> GarbageCollectGeneration()\n {\n     SuspendEE();\n     garbage_collect();\n     RestartEE();\n }\n\n garbage_collect()\n {\n     generation_to_condemn();\n     // decide to do a background GC\n     // wake up the background GC thread to do the work\n     do_background_gc();\n }\n\n do_background_gc()\n {\n     init_background_gc();\n     start_c_gc ();\n\n     //wait until restarted by the BGC.\n     wait_to_proceed();\n }\n\n bgc_thread_function()\n {\n     while (1)\n     {\n         // wait on an event\n         // wake up\n         gc1();\n     }\n }\n\n gc1()\n {\n     background_mark_phase();\n     background_sweep();\n }\n</code></pre>"},{"location":"garbage-collection/#resources","title":"Resources","text":"<ul> <li>.NET CLR GC Implementation</li> <li>The Garbage Collection Handbook: The Art of Automatic Memory Management</li> <li>Garbage collection (Wikipedia)</li> <li>Pro .NET Memory Management</li> <li>.NET GC Internals video series</li> </ul>"},{"location":"guide-for-porting/","title":"Guide for porting .NET to a new processor architecture","text":"<p>This document is broken up into 2 major sections.</p> <ol> <li> <p>The various porting stages of porting the .NET Runtime</p> </li> <li> <p>A technical discussion of the major components affected by a port to a new     architecture</p> </li> </ol>"},{"location":"guide-for-porting/#porting-stages-and-steps","title":"Porting stages and steps","text":"<p>Porting the .NET Runtime to a new architecture typically follows along the following path.</p> <p>As engineering continues along the development path, it is best if the logic can be placed into the main branch of the runtime as soon as possible. This will have 2 major effects.</p> <ol> <li> <p>Individual commits are easier to review.</p> </li> <li> <p>Not all approaches for fixing problems will always be considered acceptable.     It is plausible that a change may not ever be acceptable to take into     the upstream git repo, and discovering such issues early can avoid large     amounts of sunk cost.</p> </li> <li> <p>When some change is made which breaks other platforms, it will be relatively     simple to identify the break. If changes are held until after all changes     are complete and the product is fully functional, this work is likely to be     much more difficult.</p> </li> </ol>"},{"location":"guide-for-porting/#stage-1-initial-bring-up","title":"Stage 1 Initial Bring Up","text":"<p>Porting .NET to a new platform starts with porting CoreCLR to a new architecture.</p> <p>The process follows the following strategy</p> <ul> <li> <p>Add a new target architecture to the build environment, and make it build.</p> </li> <li> <p>Determine if there is sufficient incentive to bring up the interpreter, or     if simply making the jit handle the new architecture is cheaper. The     interpreter in the CLR is currently only used for bring up scenarios, and is     not maintained as generally working. It is expected that the interpreter     will take 1-2 months to enable for an engineer familiar with the CoreCLR     codebase. A functional interpreter allows the porting team to have a set of     engineers which focus exclusively on the JIT and a set which focuses on the     VM portion of the runtime.</p> </li> <li> <p>Build up a set of scripts that will run the coreclr tests. The normal     routine for running coreclr tests is XUnit, which is only suitable once the     framework is mostly functional. These scripts will evolve during the     development effort to support ever increasing needs of development. This set     of scripts will be expected to do the following tasks.</p> <ul> <li> <p>Run a subset of the tests. Tests are arranged in a directory structure     by category, so this subsetting mechanism will only need to be a     directory structure system.</p> </li> <li> <p>Some set of tests will need to be excluded on a test by test basis. Once     the product is ready to ship, most of these disabled tests will need to     have been re-enabled, but there are tests which will be disabled for     months/years as the product is brought up to quality.</p> </li> <li> <p>Produce crash or core dumps. The failure mode of many tests during this     phase will be a crash. A test running tool that captures core dumps will     make these issues easier to diagnose.</p> </li> <li> <p>Produce bucketized lists of failures. Generally the approach is to group     by assertion, and if there is a crash, group by callstack of crash.</p> </li> </ul> </li> <li> <p>The first test category to focus on is the JIT category, to bring up the     general ability to run .NET code. Most of these tests are very simple, but     getting some code to work is a prerequisite for handling more complex     scenarios. When doing initial bringup, configuring the Gen0 budget of the GC     to be a large number so that the GC does not attempt to run during most     tests is very useful. (Set <code>DOTNET_GCgen0size=99999999</code>)</p> </li> <li> <p>Once basic code is executing, the focus shifts to enabling the GC to work.     In this initial phase, the correct choice is to enable conservative GC     tracking via the <code>FEATURE_CONSERVATIVE_GC</code> macro. This feature will make     garbage collection largely function correctly, but it is not suitable for     production use of .NET, and can under certain circumstances trigger     unbounded memory use.</p> </li> <li> <p>Once basic GC works, and basic JIT functionality is present, work can fan     out into all of the various features of the runtime. Of particular interest     to engineers porting the runtime are the EH, stackwalking, and interop     portions of the test suite.</p> </li> <li> <p>During this phase, porting the SOS plugin from the     https://github.com/dotnet/diagnostics will be very useful. The various     commands available via that tool such as dumpmt, dumpdomain and such are     regularly useful to developers attempting to port the runtime.</p> </li> </ul>"},{"location":"guide-for-porting/#stage-2-expand-scenario-coverage","title":"Stage 2 Expand scenario coverage","text":"<ul> <li> <p>Once the coreclr tests are largely passing, the next step is to enable     XUnit. At this time the clr is probably mostly capable of running XUnit     tests, and adding testing using the libraries tests will require XUnit to     work well.</p> </li> <li> <p>Once XUnit is functional, bring up the libraries set of tests. There     is quite a lot of the CoreCLR codebase that is largely only tested by the     libraries test suites.</p> </li> <li> <p>Engineers should also begin to attempt real scenario tests at this point,     such as ASP.NET Core applications. If the libraries test suites work, then     ASP.NET Core should as well.</p> </li> </ul>"},{"location":"guide-for-porting/#stage-3-focus-on-performance","title":"Stage 3 Focus on performance","text":"<ul> <li> <p>Throughput performance at this time is likely to be not that great. There     are three major opportunities to improve performance at this stage.</p> <ul> <li> <p>Replace conservative GC with precise GC.</p> </li> <li> <p>Tune the assembly stubs to be high performance on the platform,     and implement optional assembly stubs where hand-written assembly would     be faster than the equivalent C++ code.</p> </li> <li> <p>Improve the code generated by the JIT.</p> </li> </ul> </li> <li> <p>Up until this point, engineers have probably been using the JIT for all code     instead of bringing the Ready To Run compiler (crossgen/crossgen2) into     usage on the platform. Implementing the ahead of the time compiler starts to     be useful at this time to improve startup performance.</p> </li> </ul>"},{"location":"guide-for-porting/#stage-4-focus-on-stress","title":"Stage 4 Focus on stress","text":"<ul> <li> <p>Stress testing the system is necessary to provide confidence that the system     really works.</p> </li> <li> <p>See the various test passes done in CI, but most critically GCStress testing     is needed. See documentation around use of the DOTNET_GCStress environment     variable.</p> </li> </ul>"},{"location":"guide-for-porting/#stage-5-productization","title":"Stage 5 productization","text":"<ul> <li> <p>Productization is about making the runtime able to run shipped effectively     on a platform.</p> </li> <li> <p>This document does not attempt to list out the work here as it is largely     specific to the platform in use and the opinions of numerous stakeholders.</p> </li> </ul>"},{"location":"guide-for-porting/#design-issues","title":"Design issues","text":"<p>These large architecture specific design issues will have substantial impact on both the JIT and VM.</p> <ol> <li> <p>Calling convention rules \u2013 Caller pop vs Callee pop, HFA arguments,     structure argument passing rules, etc. CoreCLR is designed to utilize a     broadly similar ABI to the OS api. Managed to managed calls typically have a     small set of tweaks or extensions to the ABI for VM efficiency purposes, but it     is generally intended that the ABI of managed code and the ABI of native     code are very similar. (This is not a hard requirement, and on Windows X86     the runtime supports a managed to managed abi as well as 3 separate native     abis for interop, but this scheme is generally not recommended.) See the     CLR-ABI document for how the existing architectures work.     Ensure that the CLR-ABI document is updated with all the requisite details     and special cases of the new platform. When defining the behavior of a new     processor architecture abi for CoreCLR, we must maintain that:</p> <ol> <li> <p>The <code>this</code> pointer is always passed in the same register regardless of     other parameters.</p> </li> <li> <p>Various stub types will require an extra \"secret\" parameter. Perf     details typically drive exactly where these are placed.</p> </li> <li> <p>When executing managed code it must be possible to hijack the return     address. Current implementations require that the return address always     be on the stack to do so, although this is a known performance     deficiency for RISC platforms on arm64.</p> </li> </ol> </li> <li> <p>Architecture specific relocation information (to represent generation of     relocations for use by load, store, jmp and call instructions) See     https://learn.microsoft.com/windows/win32/debug/pe-format#coff-relocations-object-only     for the sort of details that need to be defined.</p> </li> <li> <p>Behavior and accessibility of processor single step features from within a     process. On Unix the CLR debugger uses an in process thread to single step     through functions.</p> </li> <li> <p>Unwind information. CoreCLR uses Windows style unwind data internally, even     on Unix platforms. A Windows style unwind structure must be defined. In     addition, it is possible to enable generation of DWARF data for exposure     through the GDB JIT     https://sourceware.org/gdb/onlinedocs/gdb/JIT-Interface.html . This     support is conditional on an #ifdef, but has been used in the past to     support bring up of new platforms.</p> </li> <li> <p>EH Funclets. .NET requires a 2 pass exception model in order to properly     support exception filters. This substantially differs from the typical     Itanium ABI model which is used on most Linux architectures</p> </li> <li> <p>OS behavior with Signals. Especially exactly where the reported instruction     pointer is located.</p> </li> <li> <p>Little vs big endian. While .NET runtimes have been ported to big endian in     the past (notable examples include Mono support various game consoles, and     POWER, and XNA support on Xbox360) there are no current ports of CoreCLR to     a big endian platform.</p> </li> </ol>"},{"location":"guide-for-porting/#components-of-the-runtime-affected-by-a-port-to-a-new-architecture","title":"Components of the Runtime affected by a port to a new architecture","text":"<p>This a list of the notable architecture specific components of the .NET runtime. The list is not complete, but covers most of the areas where work will need to be done.</p> <p>Notable components</p> <ol> <li> <p>The JIT. The jit maintains the largest concentration of architecture     specific logic in the stack. This is not surprising. See Porting RyuJit     for guidance.</p> </li> <li> <p>The CLR PAL. When porting to a non-Windows OS, the PAL will be the first component     that needs to be ported.</p> </li> <li> <p>The CLR VM. The VM is a mix of completely architecture neutral logic, and     very machine specific paths.</p> </li> <li> <p>The unwinder. The unwinder is used to unwind stacks on non-Windows platforms.     It is located in https://github.com/dotnet/runtime/tree/main/src/coreclr/unwinder.</p> </li> <li> <p>System.Private.CoreLib/System.Reflection. There is little to no architecture     specific work here that is necessary for bringup. Nice-to-have work involves     adding support for the architecture in the     System.Reflection.ImageFileMachine enum, and the ProcessorArchitecture enum,     and logic that manipulates it.</p> </li> <li> <p>PE File format changes to add a new architecture. Also, the C# compiler likely     also needs a new switch to generate machine specific code for the new     architecture.</p> </li> <li> <p>Crossgen/Crossgen2 - As the AOT compilers that produce machine specific logic     from general purpose MSIL, these will be needed to improve startup performance.</p> </li> <li> <p>R2RDump - This allows diagnosing issues in pre-compiled code.</p> </li> <li> <p>coredistools - Necessary for GCStress (if determining instruction boundaries is     non-trivial), as well as for SuperPMI asm diffs for JIT development.</p> </li> <li> <p>debug and diagnostics components - The managed debugger and profiler are beyond     the scope of this document.</p> </li> </ol>"},{"location":"guide-for-porting/#clr-pal","title":"CLR PAL","text":"<p>The PAL provides a similar to Win32 api as the CLR codebase was originally designed to run on Windows platforms. Mostly the PAL is concerned with OS independence, but there are also architecture specific components.</p> <ol> <li> <p>pal.h - Contains architecture specific details for handling unwinding scenarios     such as <code>CONTEXT</code> / <code>_KNONVOLATILE_CONTEXT_POINTERS</code>/ <code>_RUNTIME_FUNCTION</code>.</p> </li> <li> <p>Unwinding support in <code>seh-unwind.cpp</code></p> </li> <li> <p>context.cpp - Which manipulates and captures register contexts</p> </li> <li> <p>jitsupport.cpp - Depending on how the features of the CPU are exposed, there    may need to be code to call OS apis to gather information about CPU features.</p> </li> <li> <p>pal arch directory - https://github.com/dotnet/runtime/tree/main/src/coreclr/pal/src/arch    This directory primarily contains assembly stubs for architecture specific    handling of signals and exceptions.</p> </li> </ol> <p>In addition to the PAL source code, there is a comprehensive set of PAL tests located in https://github.com/dotnet/runtime/tree/main/src/coreclr/pal/tests.</p>"},{"location":"guide-for-porting/#clr-vm","title":"CLR VM","text":"<p>The VM support for architecture specific logic is encoded in a variety of different ways.</p> <ol> <li> <p>Entirely architecture specific components. These are held in an architecture     specific folder.</p> </li> <li> <p>Features which are only enabled on certain architectures. E.g. <code>FEATURE_HFA</code>.</p> </li> <li> <p>Ad-hoc #if blocks used for specific architectures. As needed these are     added. The general goal is to keep these to a minimum, but difficulty here     is primarily driven by what special behavior the processor architecture     requires.</p> </li> </ol> <p>My recommendation would be to look at how Arm64 is implemented in the VM for the most up to date model of how to implement a CPU architecture.</p>"},{"location":"guide-for-porting/#architecture-specific-components","title":"Architecture Specific Components","text":"<p>There are a variety of architecture specific components that all architectures must implement.</p> <ol> <li> <p>Assembly Stubs</p> </li> <li> <p><code>cgencpu.h</code> (CPU specific header defining stubs and miscellaneous other CPU     specific details.)</p> </li> <li> <p>VSD call stub generation (virtualcallstubcpu.hpp and associated logic)</p> </li> <li> <p>Precode/Prestub/Jumpstub generation</p> </li> <li> <p><code>callingconventions.h</code>/<code>argdestination.h</code> Provides an implementation of the ABI used by VM     components. The implementation made architecture specific via a long series of     C preprocessor macros.</p> </li> <li> <p><code>gcinfodecoder.h</code> The GC info format is architecture specific as it holds    information about which specific registers hold GC data. The implementation    is generally simplified to be defined in terms of register numbers, but if    the architecture has more registers available for use than existing architectures    then the format will need extension.</p> </li> </ol>"},{"location":"guide-for-porting/#assembly-stubs","title":"Assembly Stubs","text":"<p>There are many reasons for which the runtime requires various assembly stubs. Here is an annotated list of the stubs implemented for Unix on Arm64.</p> <ol> <li> <p>Only Performance. Some stubs have alternative implementations in C++ code     which are used if there isn't an assembly stub. As compilers have gotten     better, it has become more reasonable to just use the C++ versions. Often     the biggest performance cost/win is due to fast paths being written that do     not require setting up a stack frame. Most of the casting helpers fall in     this category.</p> <ol> <li><code>JIT_Stelem_Ref</code> \u2013 very slightly faster version of     <code>JIT_Stelem_Ref_Portable</code>.</li> </ol> </li> <li> <p>General purpose correctness. Some helpers adjust the abi of whatever they     call in interesting ways, manipulate/parse the \"secret\" arguments, or do     other not quite compilable to standardized C concepts.</p> <ol> <li> <p><code>CallDescrWorkerInternal</code> \u2013 Needed to support VM to managed function     calls. Necessary for all applications as this is how the main method is     called.</p> </li> <li> <p><code>PInvokeImportThunk</code> \u2013 Needed to support saving off a set of arguments to     a p/invoke so that the runtime can find the actual target. Also uses one     of the secret arguments (Used by all p/invoke methods)</p> </li> <li> <p><code>PrecodeFixupThunk</code> \u2013 Needed to convert the secret argument from a     FixupPrecode* to a MethodDesc*. This function exists to reduce the     code size of FixupPrecodes as there are (Used by many managed methods)</p> </li> <li> <p><code>ThePreStub</code> - Needed to support saving off a set of arguments to the     stack so that the runtime can find or jit the right target method.     (Needed for any jitted method to execute Used by all managed methods)</p> </li> <li> <p><code>ThePreStubPatch</code> \u2013 Exists to provide a reliable spot for the managed     debugger to put a breakpoint.</p> </li> <li> <p>GC Write Barriers \u2013 These are used to provide the GC with information     about what memory is being updated. The existing implementations of     these are all complex, and there are a number of controls where the     runtime can adjust to tweak the behavior of the barrier in various ways.     Some of these adjustments involve modifying the code to inject     constants, or even wholesale replacements of various bits and pieces. To     achieve high performance, all of these features must work; however, to     achieve bringup supporting a simple GC, focus on the case of the single     heap workstation GC. Additionally, the     FEATURE_MANUALLY_MANAGED_CARD_BUNDLES and     FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP can be implemented as     performance needs require.</p> </li> <li> <p><code>ComCallPreStub</code>/ <code>COMToCLRDispatchHelper</code> /<code>GenericComCallStub</code> - not     necessary for non-Windows platforms at this time</p> </li> <li> <p><code>TheUMEntryPrestub</code>/ <code>UMThunkStub</code> - used to enter the runtime from     non-managed code through entrypoints generated from the     Marshal.GetFunctionPointerForDelegate api.</p> </li> <li> <p><code>OnHijackTripThread</code> - needed for thread suspension to support GC + other     suspension requiring events. This is typically not needed for very early     stage bringup of the product, but will be needed for any decent size     application</p> </li> <li> <p><code>CallEHFunclet</code> \u2013 Used to call catch, finally and fault funclets. Behavior     is specific to exactly how funclets are implemented.</p> </li> <li> <p><code>CallEHFilterFunclet</code> \u2013 Used to call filter funclets. Behavior is specific     to exactly how funclets are implemented.</p> </li> <li> <p><code>ResolveWorkerChainLookupAsmStub</code>/ <code>ResolveWorkerAsmStub</code> Used for virtual     stub dispatch (virtual call support for interface, and some virtual     methods). These work in tandem with the logic in virtualcallstubcpu.h to     implement the logic described in Virtual Stub Dispatch</p> </li> <li> <p><code>ProfileEnter</code>/ <code>ProfileLeave</code>/ <code>ProfileTailcall</code> \u2013 Used to call function     entry/exit profile functions acquired through the ICorProfiler     interface. Used in VERY rare circumstances. It is reasonable to wait to     implement these until the final stages of productization. Most profilers     do not use this functionality.</p> </li> <li> <p><code>JIT_PInvokeBegin</code>/<code>JIT_PInvokeEnd</code> \u2013 Leave/enter the managed runtime state. Necessary     for ReadyToRun pre-compiled pinvoke calls, so that they do not cause GC     starvation</p> </li> <li> <p><code>VarargPInvokeStub</code>/ <code>GenericPInvokeCalliHelper</code> Used to support calli     pinvokes. It is expected that C# 8.0 will increase use of this feature.     Today use of this feature on Unix requires hand-written IL. On Windows     this feature is commonly used by C++/CLI</p> </li> </ol> </li> </ol>"},{"location":"guide-for-porting/#cgencpuh","title":"cgencpu.h","text":"<p>This header is included by various code in the VM directory. It provides a large set of functionality that is architecture specific, including but not limited to</p> <ol> <li> <p>Defines that are architecture specific specifying the sizes of various data     structures the VM should create, and such</p> </li> <li> <p>Defines which specify which of various jit helpers should be replaced with     asm functions instead of the portable C++ implementations</p> </li> <li> <p>The CalleeSavedRegisters, ArgumentRegisters, and FloatArgumentRegisters as     needed to describe the calling convention for the platform</p> </li> <li> <p>The ClrFlushInstructionCache function. If the architecture doesn't actually     need to manually flush the icache, then this function is empty.</p> </li> <li> <p>Various functions for decoding and manipulating jump instructions. These are     used by various stub routines to predict where code will go, and to produce     simple jump stubs.</p> </li> <li> <p>The StubLinkerCpu class for the architecture. Each Architecture defines its     own StubLinkerCpu api surface and uses it to produce VM generated code.     There is a small set of apis that are called from general purpose vm code     (EmitComputedInstantiatingMethodStub, EmitShuffleThunkshared) across     multiple architectures, and then there are the individual assembly     instruction emission functions which are architecture specific. The     StubLinker is used to generate complex stubs, where the set of assembly     instructions emitted varies from stub to stub.</p> </li> <li> <p>Various stub data structures. Many very simple stubs are not emitted via an     emission of a stream of bytes, but instead are exceptionally regular, and     are effectively the same instructions for each different stub, only with     slightly different data members. Instead of using the StubLinker mechanism,     the VM instead has structures that represent the entirety of the stub and     its associate data, and fill in the assembly instructions and data fields     with a normal constructor call setting magic numbers. In addition to being     executable, these stubs are often parsed to determine exactly what a given     function is, what it is doing, where control flow will lead to, etc.</p> </li> </ol>"},{"location":"guide-for-porting/#virtualcallstubcpuh","title":"virtualcallstubcpu.h","text":"<p>This header is used to provide implementation of various stubs as used by virtual stub dispatch. These stubs are the lookup, resolver, and dispatch stubs as described in Virtual Stub Dispatch. These are maintained in a separate file from the rest of cgencpu.h for historical reasons, and for reasons of size (there is quite a lot of logic here.)</p>"},{"location":"guide-for-porting/#systemprivatecorelib","title":"System.Private.CoreLib","text":""},{"location":"guide-for-porting/#initial-bring-up","title":"Initial Bring up","text":"<p>In System.Private.CoreLib there is no work necessary for initial bring up.</p>"},{"location":"guide-for-porting/#complete-support","title":"Complete support","text":"<p>Complete support involves changing the publicly visible api surface of the product. Doing so is a process handled via public issues on GitHub and discussions with the api review board.</p> <ul> <li> <p>Adding support for the architecture to the     System.Reflection.ImageFileMachine enum, and     System.Reflection.ProcessorArchitecture enum as well as related logic</p> </li> <li> <p>Adding support for architecture specific intrinsics such as SIMD     instructions, or other non-standard api surface.</p> </li> </ul>"},{"location":"ilc-architecture/","title":"ILC Compiler Architecture","text":"<p>Author: Michal Strehovsky (@MichalStrehovsky) - 2018</p> <p>ILC (IL Compiler) is an ahead of time compiler that transforms programs in CIL (Common Intermediate Language) into a target language or instruction set to be executed on a stripped down CoreCLR runtime. The input to ILC is the common instruction format generated by popular managed language compilers such as C#, VB.NET, or F#. The output of ILC is native code for the target platform, along with data structures required to support executing the code on the target runtime. With a bit of stretch, one could say that ILC is an ahead of time native compiler for C#.</p> <p>Traditionally, CIL has been compiled \"just in time\" (JIT). What this means is that the translation from CIL to the instruction set executable on the target runtime environment happened on an as-needed basis when the native code became necessary to continue execution of the program (e.g. on a first call to a CIL method). An ahead of time compiler tries to prepare the code and data structures in advance - before the program starts executing. The major advantages of having native code and data structures required for the code to execute available in advance are significant improvements to program startup time and working set.</p> <p>In a fully ahead of time compiled environment, the compiler is responsible for generating code and data structures for everything that might be needed at runtime - the presence of the original CIL instructions or program metadata (names of methods and their signature, for example) is no longer necessary after compilation. One important aspect to keep in mind is that ahead of time compilation does not preclude just in time compilation: one could imagine mixed modes of execution where some parts of the application are compiled ahead of time, while others are compiled just in time, or interpreted. ILC needs to support such modes of operations, since both have their advantages and disadvantages. We have prototyped such modes of execution in the past.</p>"},{"location":"ilc-architecture/#goals","title":"Goals","text":"<ul> <li>Compile CIL and produce native code for the target platform</li> <li>Generate essential data structures that the runtime requires to execute managed native code (exception handling and GC information for methods, data structures describing types, their GC layout and vtables, interface dispatch maps, etc.)</li> <li>Generate optional data structures the base class libraries require to provide rich managed APIs to user code (data structures that support reflection, interop, textual stack trace information, type loading at runtime, etc.)</li> <li>Support optional inputs from a whole program analysis step to influence compilation</li> <li>Support generating executable files and static/dynamic libraries (with a flat C-style public API surface)</li> <li>Support multiple modes of compilation:</li> <li>Single-file output:<ul> <li>All input assemblies merged into a single object file generated by ILC (merging managed assemblies happens in ILC). This mode allows maximum optimization.</li> <li>Generating multiple object files that are merged using a platform linker into a single executable (merging happens in the native linker after ILC runs). This mode allows incremental compilation at the cost of inhibiting optimizations in ILC.</li> </ul> </li> <li>Multi-file output (one or more input assemblies generating one or more dynamic libraries that link against each other dynamically). This mode allows code/data sharing among several executables or dynamic libraries, but inhibits many optimizations.</li> <li>Multi-threaded compilation</li> <li>Generate native debug information for the target platform to allow debugging with native debuggers</li> <li>Generate outputs in the platform's native object file format (<code>.obj</code> and <code>.o</code> files)</li> <li>Have a defined behavior when input is incomplete (e.g. assemblies missing, or an assembly version mismatch)</li> </ul>"},{"location":"ilc-architecture/#ilc-composition","title":"ILC composition","text":"<p>ILC is composed of roughly 3 parts: the compilation driver, the compiler, and the code generation backends.</p>"},{"location":"ilc-architecture/#compilation-driver","title":"Compilation driver","text":"<p>The role of the compilation driver is to parse command line arguments, set up the compiler, and run the compilation. The process of setting up the compiler involves configuring a <code>CompilationBuilder</code>. The compilation builder exposes methods that let the driver configure and compose various pieces of the compiler as directed by the command line arguments. These components influence what gets compiled and how the compilation happens. Eventually the driver constructs a <code>Compilation</code> object that provides methods to run the compilation, inspect the results of the compilation, and write the outputs to a file on disk.</p> <p>Related classes: <code>CompilationBuilder</code>, <code>ICompilation</code></p>"},{"location":"ilc-architecture/#compiler","title":"Compiler","text":"<p>Compiler is the core component that the rest of the document is going to talk about. It's responsible for running the compilation process and generating data structures for the target runtime and target base class libraries.</p> <p>The compiler tries to stay policy free as to what to compile, what data structures to generate, and how to do the compilation. The specific policies are supplied by the compilation driver as part of configuring the compilation.</p>"},{"location":"ilc-architecture/#code-generation-backends","title":"Code generation backends","text":"<p>ILC is designed to support multiple code generation backends that target the same runtime. What this means is that we have a model where there are common parts within the compiler (determining what needs to be compiled, generating data structures for the underlying runtime), and parts that are specific for a target environment. The common parts (the data structure layout) are not target specific - the target specific differences are limited to general questions, such as \"does the target have representation for relative pointers?\", but the basic shape of the data structures is the same, no matter the target platform. ILC currently supports following codegen backends (with varying levels of completeness):</p> <ul> <li>RyuJIT: native code generator also used as the JIT compiler in CoreCLR. This backend supports x64, arm64, arm32 on Windows, Linux, macOS, and BSD.</li> <li>LLVM: the LLVM backend is currently used to generate WebAssembly code in connection with Emscripten. Lives in NativeAOT-LLVM branch.</li> </ul> <p>This document describes the common parts of the compiler that are applicable to all codegen backends.</p> <p>In the past, ILC supported following backends:</p> <ul> <li>CppCodegen: a portable code generator that translates CIL into C++ code. This supports rapid platform bringup (instead of having to build a code generator for a new CPU architecture, it relies on the C++ compiler the platform likely already has). Portability comes at certain costs. This codegen backend wasn't brought over from the now archived CoreRT repo.</li> </ul> <p>Related project files: ILCompiler.LLVM.csproj, ILCompiler.RyuJit.csproj</p>"},{"location":"ilc-architecture/#dependency-analysis","title":"Dependency analysis","text":"<p>The core concept driving the compilation in ILC is dependency analysis. Dependency analysis is the process of determining the set of runtime artifacts (method code bodies and various data structures) that need to be generated into the output object file. Dependency analysis builds a graph where each vertex either * represents an artifact that will be part of the output file (such as \"compiled method body\" or \"data structure describing a type at runtime\") - this is an \"object node\", or * captures certain abstract characteristic of the compiled program (such as \"the program contains a virtual call to the <code>Object.GetHashCode</code> method\") - a general \"dependency node\". General dependency nodes do not physically manifest themselves as bytes in the output, but they usually have edges that (transitively) lead to object nodes that do form parts of the output.</p> <p>The edges of the graph represent a \"requires\" relationship. The compilation process corresponds to building this graph and determining what nodes are part of the graph.</p> <p>Related classes: <code>DependencyNodeCore&lt;&gt;</code>, <code>ObjectNode</code></p> <p>Related project files: ILCompiler.DependencyAnalysisFramework.csproj</p>"},{"location":"ilc-architecture/#dependency-expansion-process","title":"Dependency expansion process","text":"<p>The compilation starts with a set of nodes in the dependency graph called compilation roots. The roots are specified by the compilation driver and typically contain the <code>Main()</code> method, but the exact set of roots depends on the compilation mode: the set of roots will be different when we're e.g. building a library, or when we're doing a multi-file compilation, or when we're building a single file app.</p> <p>The process begins by looking at the list of the root nodes and establishing their dependencies (dependent nodes). Once the dependencies are known, the compilation moves on to inspecting the dependencies of the dependencies, and so on, until all dependencies are known and marked within the dependency graph. When that happens, the compilation is done.</p> <p>The expansion of the graph is required to stay within the limits of a compilation group. Compilation group is a component that controls how the dependency graph is expanded. The role of it is best illustrated by contrasting a multifile and single file compilation: in a single file compilation, all methods and types that are statically reachable from the roots become part of the dependency graph, irrespective of the input assembly that defines them. In a multi-file compilation, some of the compilation happens as part of a different unit of work: the methods and types that are not part of the current unit of work shouldn't have their dependencies examined and they should not be a part of the dependency graph.</p> <p>The advantage of having the two abstractions (compilation roots, and a class that controls how the dependency graph is expanded) is that the core compilation process can be completely unaware of the specific compilation mode (e.g. whether we're building a library, or whether we're doing a multi-file compilation). The details are fully wrapped behind the two abstractions and give us a great expressive power for defining or experimenting with new compilation modes, while keeping all the logic in a single place. For example, we support a single method compilation mode where we compile only one method. This mode is useful for troubleshooting code generation. The compilation driver can define additional compilation modes (e.g. a mode that compiles a single type and all the associated methods) without having to change the compiler itself.</p> <p>Related classes: <code>ICompilationRootProvider</code>, <code>CompilationModuleGroup</code></p>"},{"location":"ilc-architecture/#dependency-types","title":"Dependency types","text":"<p>The dependency graph analysis can work with several kinds of dependencies between the nodes: * Static dependencies: these are the most common. If node A is part of the dependency graph and it declares it requires node B, node B also becomes part of the dependency graph. * Conditional dependencies: Node A declares that it depends on node B, but only if node C is part of the graph. If that's the case, node B will become part of the graph if both A and C are in the graph. * Dynamic dependencies: These are quite expensive to have in the system, so we only use them rarely. They let the node inspect other nodes within the graph and inject nodes based on their presence. They are pretty much only used for analysis of generic virtual methods.</p> <p>To show how the dependency graph looks like in real life let's look at an example of how an (optional) optimization within the compiler around virtual method usage tracking works:</p> <pre><code>abstract class Foo\n{\n    public abstract void VirtualMethod();\n    public virtual void UnusedVirtualMethod() { }\n}\n\nclass Bar : Foo\n{\n    public override void VirtualMethod() { }\n    public override void UnusedVirtualMethod() { }\n}\n\nclass Baz : Foo\n{\n    public override void VirtualMethod() { }\n}\n\nclass Program\n{\n    static int Main()\n    {\n        Foo f = new Bar();\n        f.VirtualMethod();\n        return f is Baz ? 0 : 100;\n    }\n}\n</code></pre> <p>The dependency graph for the above program would look something like this:</p> <p></p> <p>The rectangle-shaped nodes represent method bodies, the oval-shaped nodes represent types, the dashed rectangles represent virtual method use, and the dotted oval-shaped node is an unconstructed type. The dashed edges are conditional dependencies, with the condition marked on the label.</p> <ul> <li><code>Program::Main</code> creates a new instance of <code>Bar</code>. For that, it will allocate an object on the GC heap and call a constructor to initialize it. Therefore, it needs the data structure that represents the <code>Bar</code> type and <code>Bar</code>'s default constructor. The method then calls <code>VirtualMethod</code>. Even though from this simple example we know what specific method body this will end up calling (we can devirtualize the call in our head), we can't know in general, so we say <code>Program::Main</code> also depends on \"Virtual method use of Foo::VirtualMethod\". The last line of the program performs a type check. To do the type check, the generated code needs to reference a data structure that represents type <code>Baz</code>. The interesting thing about a type check is that we don't need to generate a full data structure describing the type, only enough to be able to tell if the cast succeeds. So we say <code>Program::Main</code> also depends on \"unconstructed type data structure\" for <code>Baz</code>.</li> <li>The data structure that represents type <code>Bar</code> has two important kinds of dependencies. It depends on its base type (<code>Foo</code>) - a pointer to it is required to make casting work - and it also contains the vtable. The entries in the vtable are conditional - if a virtual method is never called, we don't need to place it in the vtable. As a result of the situation in the graph, the method body for <code>Bar::VirtualMethod</code> is going to be part of the graph, but <code>Bar::UnusedVirtualMethod</code> will not, because it's conditioned on a node that is not present in the graph.</li> <li>The data structure that represents <code>Baz</code> is a bit different from <code>Bar</code>. We call this an \"unconstructed type\" structure. Unconstructed type structures don't contain a vtable, and therefore <code>Baz</code> is missing a virtual method use dependency for <code>Baz::VirtualMethod</code> conditioned on the use of <code>Foo::VirtualMethod</code>.</li> </ul> <p>Notice how using conditional dependencies helped us avoid compiling method bodies for <code>Foo::UnusedVirtualMethod</code> and <code>Bar::UnusedVirtualMethod</code> because the virtual method is never used. We also avoided generating <code>Baz::VirtualMethod</code>, because <code>Baz</code> was never allocated within the program. We generated the data structure that represents <code>Baz</code>, but because the data structure was only generated for the purposes of casting, it doesn't have a vtable that would pull <code>Baz::VirtualMethod</code> into the dependency graph.</p> <p>Note that while \"constructed\" and \"unconstructed\" type nodes are modelled separately in the dependency graph, at the object writing time they get coalesced into one. If the graph has a type in both the unconstructed and constructed form, only the constructed form will be emitted into the executable and places referring to the unconstructed form will be redirected to the constructed form, to maintain type identity.</p> <p>Related compiler switches: <code>--dgmllog</code> serializes the dependency graph into an XML file. The XML file captures all the nodes in the graph, but only captures the first edge leading to the node (knowing the first edge is enough for most purposes). <code>--fulllog</code> generates an even bigger XML file that captures all the edges.</p> <p>Related tools: Dependency analysis viewer is a tool that listens to ETW events generated by all the ILC compiler processes on the machine and lets you interactively explore the graph.</p>"},{"location":"ilc-architecture/#object-writing","title":"Object writing","text":"<p>The final phase of compilation is writing out the outputs. The output of the compilation depends on the target environment but will typically be some sort of object file. An object file typically consists of blobs of code or data with links (or relocations) between them, and symbols: named locations within a blob. The relocations point to symbols, either defined within the same object file, or in a different module.</p> <p>While the object file format is highly target specific, the compiler represents dependency nodes that have object data associated with them the same way irrespective of the target - with the <code>ObjectNode</code> class. <code>ObjectNode</code> class allows its children to specify the section where to place their data (code, read only data, uninitialized data, etc.), and crucially, the data itself (represented by the <code>ObjectData</code> class returned from <code>GetObjectData</code> method).</p> <p>On a high level, the role of the object writer is to go over all the marked <code>ObjectNode</code>s in the graph, retrieve their data, defined symbols, and relocations to other symbols, and store them in the object file.</p> <p>NativeAOT compiler contains multiple object writers: * Native object writer based on LLVM that is capable of producing Windows PE, Linux ELF, and macOS Mach-O file formats * Native object writer based on LLVM for WebAssembly * Ready to run object writer that generates mixed CIL/native executables in the ready to run format for CoreCLR</p> <p>Related command line arguments: <code>--map</code> produces a map of all the object nodes that were emitted into the object file.</p>"},{"location":"ilc-architecture/#optimization-pluggability","title":"Optimization pluggability","text":"<p>An advantage of a fully ahead of time compiled environment is that the compiler can make closed world assumptions about the code being compiled. For example: lacking the ability to load arbitrary CIL at runtime (either through <code>Assembly.Load</code>, or <code>Reflection.Emit</code>), if the compiler sees that there's only one type implementing an interface, it can replace all the interface calls in the program with direct calls, and apply additional optimizations enabled by it, such as inlining. If the target environment allowed dynamic code, such optimization would be invalid.</p> <p>The compiler is structured to allow such optimizations, but remains policy-free as to when the optimization should be applied. This allow both fully AOT compiled and mixed (JIT/interpreted) code execution strategies. The policies are always captured in an abstract class or an interface, the implementation of which is selected by the compilation driver and passed to the compilation builder. This allows a great degree of flexibility and gives a lot of power to influence the compilation from the compilation driver, without hardcoding the conditions when the optimization is applicable into the compiler.</p> <p>An example of such policy is the virtual method table (vtable) generation policy. The compiler can build vtables two ways: lazily, or by reading the type's metadata and generating a vtable slot for every new virtual method present in the type's method list. The depenceny analysis example graph a couple sections above was describing how conditional dependencies can be used to track what vtable slots and virtual method bodies we need to generate for a program to work. This is an example of an optimization that requires closed world assumptions. The policy is captured in a <code>VTableSliceProvider</code> class and allows the driver to select the vtable generation policy per type. This allows the compilation driver a great degree of control to fine tune when the optimization is allowed to happen (e.g. even in the presence of a JIT, we could still allow this optimization to happen on types that are not visible/accessible from the non-AOT compiled parts of the program or through reflection).</p> <p>The policies that can be configured in the driver span a wide range of areas: generation of reflection metadata, devirtualization, generation of vtables, generation of stack trace metadata for <code>Exception.ToString</code>, generation of debug information, the source of IL for method bodies, etc.</p>"},{"location":"ilc-architecture/#il-scanning","title":"IL scanning","text":"<p>Another component of ILC is the IL scanner. IL scanning is an optional step that can be executed before the compilation. In many ways, the IL scanning acts as another compilation with a null/dummy code generation backend. The IL scanner scans the IL of all the method bodies that become part of the dependency graph starting from the roots and expands their dependencies. The IL scanner ends up building the same dependency graph a code generation backend would, but the nodes in the graph that represent method bodies don't have any machine code instructions associated with them. This process is relatively fast since there's no code generation involved, but the resulting graph contains a lot of valuable insights into the compiled program. The dependency graph built by the IL scanner is a strict superset of the graph built by a real compilation since the IL scanner doesn't model optimizations such as inlining and devirtualization.</p> <p>The results of the IL scanner are input into the subsequent compilation process. For example, the IL scanner can use the lazy vtable generation policy to build vtables with just the slots needed, and assign slot numbers to each slot in the vtable at the end of scanning. The vtable layouts computed lazily during scanning can then be used by the real compilation process to inline vtable lookups at the callsites. Inlining the vtable lookup at the callsite would not be possible with a lazy vtable generation policy because the exact slot assignments of lazy vtables aren't stable until the compilation is done.</p> <p>The IL scanning process is optional and the compilation driver can skip it if compilation throughput is more important than runtime code quality.</p> <p>Related classes: <code>ILScanner</code></p>"},{"location":"ilc-architecture/#coupling-with-the-base-class-libraries","title":"Coupling with the base class libraries","text":"<p>The compiler has a certain level of coupling with the underlying base class library (the <code>System.Private.*</code> libraries within the repo). The coupling is twofold: * Binary format of the generated data structures * Expectations about the existence of certain methods within the core library</p> <p>Examples of the binary formats generated by the compiler and used by the base class libraries would be the format of the data structure that represents a type at runtime (<code>MethodTable</code>), or the blob of bytes that describes non-essential information about the type (such as the type name, or a list of methods). These data structures form a contract and allow the managed code in the base class library to provide rich services to user code through library APIs at runtime (such as the reflection APIs). Generation of some of these data structures is optional, but for some it's mandatory because they're required to execute any managed code.</p> <p>The compiler also needs to call into some well-known entrypoints within the base class library to support the generated code. The base class library needs to define these methods. Examples of such entrypoints would be various helpers to throw <code>OverflowException</code> during mathematical operations, <code>IndexOutOfRangeException</code> during array access, or various helpers to aid in generating p/invoke marshalling code (e.g. converting a UTF-16 string to ANSI and back before/after invoking the native method).</p> <p>One interesting thing to point out is that the coupling of the compiler with the base class libraries is relatively loose (there are only few mandatory parts). This allows different base class libraries to be used with ILC. Such base class libraries could look quite different from what regular .NET developers are used to (e.g. a <code>System.Object</code> that doesn't have a <code>ToString</code> method) but could allow using type safe code in environments where regular .NET would be considered \"too heavy\". Various experiments with such lightweight code have been done in the past, and some of them even shipped as part of the Windows operating system.</p> <p>Example of such alternative base class library is Test.CoreLib. The <code>Test.CoreLib</code> library provides a very minimal API surface. This, coupled with the fact that it requires almost no initialization, makes it a great assistant in bringing NativeAOT to new platforms.</p>"},{"location":"ilc-architecture/#compiler-generated-method-bodies","title":"Compiler-generated method bodies","text":"<p>Besides compiling the code provided by the user in the form of input assemblies, the compiler also needs to compile various helpers generated within the compiler. The helpers are used to lower some of the higher-level .NET constructs into concepts that the underlying code generation backend understands. These helpers are emitted as IL code on the fly, without being physically backed by IL in an assembly on disk. Having the higher level concepts expressed as regular IL helps avoid having to implement the higher-level concept in each code generation backend (we only must do it once because IL is the thing all backends understand).</p> <p>The helpers serve various purposes such as: * Helpers to support invoking delegates * Helpers that support marshalling parameters and return values for P/Invoke * Helpers that support <code>ValueType.GetHashCode</code> and <code>ValueType.Equals</code> * Helpers that support reflection: e.g. <code>Assembly.GetExecutingAssembly</code></p> <p>Related classes: <code>ILEmitter</code>, <code>ILStubMethod</code></p> <p>Related ILC command line switches: <code>--ildump</code> to dump all generated IL into a file and map debug information to it (allows source stepping through the generated IL at runtime).</p>"},{"location":"intro-to-clr/","title":"Introduction to the Common Language Runtime (CLR)","text":"<p>By Vance Morrison (@vancem) - 2007</p> <p>What is the Common Language Runtime (CLR)? To put it succinctly:</p> <p>The Common Language Runtime (CLR) is a complete, high level virtual machine designed to support a broad variety of programming languages and interoperation among them.</p> <p>Phew, that was a mouthful.  It also in and of itself is not very illuminating.  The statement above is useful however, because it is the first step in taking the large and complicated piece of software known as the CLR and grouping its features in an understandable way.  It gives us a \"10,000 foot\" view of the runtime from which we can understand the broad goals and purpose of the runtime.  After understanding the CLR at this high level, it is easier to look more deeply into sub-components without as much chance of getting lost in the details.</p>"},{"location":"intro-to-clr/#the-clr-a-very-rare-complete-programming-platform","title":"The CLR: A (very rare) Complete Programming Platform","text":"<p>Every program has a surprising number of dependencies on its runtime environment.  Most obviously, the program is written in a particular programming language, but that is only the first of many assumptions a programmer weaves into the program.  All interesting programs need some runtime library that allows them to interact with the other resources of the machine (such as user input, disk files, network communications, etc).  The program also needs to be converted in some way (either by interpretation or compilation) to a form that the native hardware can execute directly.  These dependencies of a program are so numerous, interdependent and diverse that implementers of programming languages almost always defer to other standards to specify them.  For example, the C++ language does not specify the format of a C++ executable.  Instead, each C++ compiler is bound to a particular hardware architecture (e.g., X86) and to an operating system environment (e.g., Windows, Linux, or Mac OS), which describes the format of the executable file format and specifies how it will be loaded.  Thus, programmers don't make a \"C++ executable,\" but rather a \"Windows X86 executable\" or a \"Power PC Mac OS executable.\"</p> <p>While leveraging existing hardware and operating system standards is usually a good thing, it has the disadvantage of tying the specification to the level of abstraction of the existing standards.  For example, no common operating system today has the concept of a garbage-collected heap.  Thus, there is no way to use existing standards to describe an interface that takes advantage of garbage collection (e.g., passing strings back and forth, without worrying about who is responsible for deleting them).  Similarly, a typical executable file format provides just enough information to run a program but not enough information for a compiler to bind other binaries to the executable.  For example, C++ programs typically use a standard library (on Windows, called msvcrt.dll) which contains most of the common functionality (e.g., printf), but the existence of that library alone is not enough.  Without the matching header files that go along with it (e.g., stdio.h), programmers can't use the library.  Thus, existing executable file format standards cannot be used both to describe a file format that can be run and to specify other information or binaries necessary to make the program complete.</p> <p>The CLR fixes problems like these by defining a very complete specification (standardized by ECMA) containing the details you need for the COMPLETE lifecycle of a program, from construction and binding through deployment and execution.  Thus, among other things, the CLR specifies:</p> <ul> <li>A GC-aware virtual machine with its own instruction set (called the Common Intermediate Language (CIL)) used to specify the primitive operations that programs perform.  This means the CLR is not dependent on a particular type of CPU.</li> <li>A rich meta data representation for program declarations (e.g., types, fields, methods, etc), so that compilers generating other executables have the information they need to call functionality from 'outside'.</li> <li>A file format that specifies exactly how to lay the bits down in a file, so that you can properly speak of a CLR EXE that is not tied to a particular operating system or computer hardware.</li> <li>The lifetime semantics of a loaded program, the mechanism by which one CLR EXE file can refer to another CLR EXE and the rules on how the runtime finds the referenced files at execution time.</li> <li>A class library that leverages the features that the CLR provides (e.g., garbage collection, exceptions, or generic types) to give access both to basic functionality (e.g., integers, strings, arrays, lists, or dictionaries) as well as to operating system services (e.g., files, network, or user interaction).</li> </ul>"},{"location":"intro-to-clr/#multi-language-support","title":"Multi-language Support","text":"<p>Defining, specifying and implementing all of these details is a huge undertaking, which is why complete abstractions like the CLR are very rare.  In fact, the vast majority of such reasonably complete abstractions were built for single languages.  For example, the Java runtime, the Perl interpreter or the early version of the Visual Basic runtime offer similarly complete abstraction boundaries.  What distinguishes the CLR from these earlier efforts is its multi-language nature.  With the possible exception of Visual Basic (because it leverages the COM object model), the experience within the language is often very good, but interoperating with programs written in other languages is very difficult at best.  Interoperation is difficult because these languages can only communicate with \"foreign\" languages by using the primitives provided by the operating system.  Because the OS abstraction level is so low (e.g., the operating system has no concept of a garbage-collected heap), needlessly complicated techniques are necessary.  By providing a COMMON LANGUAGE RUNTIME, the CLR allows languages to communicate with each other with high-level constructs (e.g., GC-collected structures), easing the interoperation burden dramatically.</p> <p>Because the runtime is shared among many languages, it means that more resources can be put into supporting it well.  Building good debuggers and profilers for a language is a lot of work, and thus they exist in a full-featured form only for the most important programming languages.  Nevertheless, because languages that are implemented on the CLR can reuse this infrastructure, the burden on any particular language is reduced substantially.  Perhaps even more important, any language built on the CLR immediately has access to all the class libraries built on top of the CLR.  This large (and growing) body of (debugged and supported) functionality is a huge reason why the CLR has been so successful.</p> <p>In short, the runtime is a complete specification of the exact bits one has to put in a file to create and run a program.  The virtual machine that runs these files is at a high level appropriate for implementing a broad class of programming languages.  This virtual machine, along with an ever growing body of class libraries that run on that virtual machine, is what we call the common language runtime (CLR).</p>"},{"location":"intro-to-clr/#the-primary-goal-of-the-clr","title":"The Primary Goal of the CLR","text":"<p>Now that we have basic idea what the CLR is, it is useful to back up just a bit and understand the problem the runtime was meant to solve.  At a very high level, the runtime has only one goal:</p> <p>The goal of the CLR is to make programming easy.</p> <p>This statement is useful for two reasons.  First, it is a very useful guiding principle as the runtime evolves.  For example, fundamentally only simple things can be easy, so adding user visible complexity to the runtime should always be viewed with suspicion.  More important than the cost/benefit ratio of a feature is its added exposed complexity/weighted benefit over all scenarios ratio.  Ideally, this ratio is negative (that is, the new feature reduces complexity by removing restrictions or by generalizing existing special cases); however, more typically it is kept low by minimizing the exposed complexity and maximizing the number of scenarios to which the feature adds value.</p> <p>The second reason this goal is so important is that ease of use is the fundamental reason for the CLR's success.  The CLR is not successful because it is faster or smaller than writing native code (in fact, well-written native code often wins).  The CLR is not successful because of any particular feature it supports (like garbage collection, platform independence, object-oriented programming or versioning support).  The CLR is successful because all of those features, as well as numerous others, combine to make programming significantly easier than it would be otherwise.  Some important but often overlooked ease of use features include:</p> <ol> <li>Simplified languages (e.g., C# and Visual Basic are significantly simpler than C++)</li> <li>A dedication to simplicity in the class library (e.g., we only have one string type, and it is immutable; this greatly simplifies any API that uses strings)</li> <li>Strong consistency in the naming in the class library (e.g., requiring APIs to use whole words and consistent naming conventions)</li> <li>Great support in the tool chain needed to create an application (e.g., Visual Studio makes building CLR applications very simple, and Intellisense makes finding the right types and methods to create the application very easy).</li> </ol> <p>It is this dedication to ease of use (which goes hand in hand with simplicity of the user model) that stands out as the reason for the success of the CLR.  Oddly, some of the most important ease-of-use features are also the most \"boring.\" For example, any programming environment could apply consistent naming conventions, yet actually doing so across a large class library is quite a lot of work.  Often such efforts conflict with other goals (such as retaining compatibility with existing interfaces), or they run into significant logistical concerns (such as the cost of renaming a method across a very large code base).  It is at times like these that we have to remind ourselves about our number-one overarching goal of the runtime and ensure that we have our priorities straight to reach that goal.</p>"},{"location":"intro-to-clr/#fundamental-features-of-the-clr","title":"Fundamental Features of the CLR","text":"<p>The runtime has many features, so it is useful to categorize them as follows:</p> <ol> <li>Fundamental features \u2013 Features that have broad impact on the design of other features.  These include:<ol> <li>Garbage Collection</li> <li>Memory Safety and Type Safety</li> <li>High level support for programming languages.</li> </ol> </li> <li>Secondary features \u2013 Features enabled by the fundamental features that may not be required by many useful programs:<ol> <li>Program isolation with AppDomains</li> <li>Program Security and sandboxing</li> </ol> </li> <li>Other Features \u2013 Features that all runtime environments need but that do not leverage the fundamental features of the CLR.  Instead, they are the result of the desire to create a complete programming environment.  Among them are:<ol> <li>Versioning</li> <li>Debugging/Profiling</li> <li>Interoperation</li> </ol> </li> </ol>"},{"location":"intro-to-clr/#the-clr-garbage-collector-gc","title":"The CLR Garbage Collector (GC)","text":"<p>Of all the features that the CLR provides, the garbage collector deserves special notice.  Garbage collection (GC) is the common term for automatic memory reclamation.  In a garbage-collected system, user programs no longer need to invoke a special operator to delete memory.  Instead the runtime automatically keeps track of all references to memory in the garbage-collected heap, and from time-to-time, it will traverse these references to find out which memory is still reachable by the program.  All other memory is garbage and can be reused for new allocations.</p> <p>Garbage collection is a wonderful user feature because it simplifies programming.  The most obvious simplification is that most explicit delete operations are no longer necessary.  While removing the delete operations is important, the real value to the programmer is a bit more subtle:</p> <ol> <li>Garbage collection simplifies interface design because you no longer have to carefully specify which side of the interface is responsible for deleting objects passed across the interface.  For example, CLR interfaces simply return strings; they don't take string buffers and lengths.  This means they don't have to deal with the complexity of what happens when the buffers are too small.  Thus, garbage collection allows ALL interfaces in the runtime to be simpler than they otherwise would be.</li> <li>Garbage collection eliminates a whole class of common user mistakes.  It is frightfully easy to make mistakes concerning the lifetime of a particular object, either deleting it too soon (leading to memory corruption), or too late (unreachable memory leaks).  Since a typical program uses literally MILLIONS of objects, the probability for error is quite high.  In addition, tracking down lifetime bugs is very difficult, especially if the object is referenced by many other objects.  Making this class of mistakes impossible avoids a lot of grief.</li> </ol> <p>Still, it is not the usefulness of garbage collection that makes it worthy of special note here.  More important is the simple requirement it places on the runtime itself:</p> <p>Garbage collection requires ALL references to the GC heap to be tracked.</p> <p>While this is a very simple requirement, it in fact has profound ramifications for the runtime.  As you can imagine, knowing where every pointer to an object is at every moment of program execution can be quite difficult.  We have one mitigating factor, though.  Technically, this requirement only applies to when a GC actually needs to happen (thus, in theory we don't need to know where all GC references are all the time, but only at the time of a GC).  In practice, however, this mitigation doesn't completely apply because of another feature of the CLR:</p> <p>The CLR supports multiple concurrent threads of execution with a single process.</p> <p>At any time some other thread of execution might perform an allocation that requires a garbage collection.  The exact sequence of operations across concurrently executing threads is non-deterministic.  We can't tell exactly what one thread will be doing when another thread requests an allocation that will trigger a GC.  Thus, GCs can really happen any time.  Now the CLR does NOT need to respond immediately to another thread's desire to do a GC, so the CLR has a little \"wiggle room\" and doesn't need to track GC references at all points of execution, but it does need to do so at enough places that it can guarantee \"timely\" response to the need to do a GC caused by an allocation on another thread.</p> <p>What this means is that the CLR needs to track all references to the GC heap almost all the time.  Since GC references may reside in machine registers, in local variables, statics, or other fields, there is quite a bit to track.  The most problematic of these locations are machine registers and local variables because they are so intimately related to the actual execution of user code.  Effectively, what this means is that the machine code that manipulates GC references has another requirement: it must track all the GC references that it uses.  This implies some extra work for the compiler to emit the instructions to track the references.</p> <p>To learn more, check out the Garbage Collector design document.</p>"},{"location":"intro-to-clr/#the-concept-of-managed-code","title":"The Concept of \"Managed Code\"","text":"<p>Code that does the extra bookkeeping so that it can report all of its live GC references \"almost all the time\" is called managed code (because it is \"managed\" by the CLR).  Code that does not do this is called unmanaged code.  Thus all code that existed before the CLR is unmanaged code, and in particular, all operating system code is unmanaged.</p>"},{"location":"intro-to-clr/#the-stack-unwinding-problem","title":"The stack unwinding problem","text":"<p>Clearly, because managed code needs the services of the operating system, there will be times when managed code calls unmanaged code.  Similarly, because the operating system originally started the managed code, there are also times when unmanaged code calls into managed code.  Thus, in general, if you stop a managed program at an arbitrary location, the call stack will have a mixture of frames created by managed code and frames created by unmanaged code.</p> <p>The stack frames for unmanaged code have no requirements on them over and above running the program.  In particular, there is no requirement that they can be unwound at runtime to find their caller.  What this means is that if you stop a program at an arbitrary place, and it happens to be in a unmanaged method, there is no way in general<sup>[1]</sup> to find who the caller was.  You can only do this in the debugger because of extra information stored in the symbolic information (PDB file).  This information is not guaranteed to be available (which is why you sometimes don't get good stack traces in a debugger).  This is quite problematic for managed code, because any stack that can't be unwound might in fact contain managed code frames (which contain GC references that need to be reported).</p> <p>Managed code has additional requirements on it: not only must it track all the GC references it uses during its execution, but it must also be able to unwind to its caller.  Additionally, whenever there is a transition from managed code to unmanaged code (or the reverse), managed code must also do additional bookkeeping to make up for the fact that unmanaged code does not know how to unwind its stack frames.  Effectively, managed code links together the parts of the stack that contain managed frames.  Thus, while it still may be impossible to unwind the unmanaged stack frames without additional information, it will always be possible to find the chunks of the stack that correspond to managed code and to enumerate the managed frames in those chunks.</p> <p>[1] More recent platform ABIs (application binary interfaces) define conventions for encoding this information, however there is typically not a strict requirement for all code to follow them.</p>"},{"location":"intro-to-clr/#the-world-of-managed-code","title":"The \"World\" of Managed Code","text":"<p>The result is that special bookkeeping is needed at every transition to and from managed code.  Managed code effectively lives in its own \"world\" where execution can't enter or leave unless the CLR knows about it.  The two worlds are in a very real sense distinct from one another (at any point in time the code is in the managed world or the unmanaged world).  Moreover, because the execution of managed code is specified in a CLR format (with its Common Intermediate Language (CIL)), and it is the CLR that converts it to run on the native hardware, the CLR has much more control over exactly what that execution does.  For example, the CLR could change the meaning of what it means to fetch a field from an object or call a function.  In fact, the CLR does exactly this to support the ability to create MarshalByReference objects.  These appear to be ordinary local objects, but in fact may exist on another machine.  In short, the managed world of the CLR has a large number of execution hooks that it can use to support powerful features which will be explained in more detail in the coming sections.</p> <p>In addition, there is another important ramification of managed code that may not be so obvious.  In the unmanaged world, GC pointers are not allowed (since they can't be tracked), and there is a bookkeeping cost associated with transitioning from managed to unmanaged code.  What this means is that while you can call arbitrary unmanaged functions from managed code, it is often not pleasant to do so.  Unmanaged methods don't use GC objects in their arguments and return types, which means that any \"objects\" or \"object handles\" that those unmanaged functions create and use need to be explicitly deallocated.  This is quite unfortunate.  Because these APIs can't take advantage of CLR functionality such as exceptions or inheritance, they tend to have a \"mismatched\" user experience compared to how the interfaces would have been designed in managed code.</p> <p>The result of this is that unmanaged interfaces are almost always wrapped before being exposed to managed code developers.  For example, when accessing files, you don't use the Win32 CreateFile functions provided by the operating system, but rather the managed System.IO.File class that wraps this functionality.  It is in fact extremely rare that unmanaged functionality is exposed to users directly.</p> <p>While this wrapping may seem to be \"bad\" in some way (more code that does not seem to do much), it is in fact good because it actually adds quite a bit of value.  Remember it was always possible to expose the unmanaged interfaces directly; we chose to wrap the functionality.  Why?  Because the overarching goal of the runtime is to make programming easy, and typically the unmanaged functions are not easy enough.  Most often, unmanaged interfaces are not designed with ease of use in mind, but rather are tuned for completeness.  Anyone looking at the arguments to CreateFile or CreateProcess would be hard pressed to characterize them as \"easy.\" Luckily, the functionality gets a \"facelift\" when it enters the managed world, and while this makeover is often very \"low tech\" (requiring nothing more complex than renaming, simplification, and organizing the functionality), it is also profoundly useful.  One of the very important documents created for the CLR is the Framework Design Guidelines.  This 800+ page document details best practices in making new managed class libraries.</p> <p>Thus, we have now seen that managed code (which is intimately involved with the CLR) differs from unmanaged code in two important ways:</p> <ol> <li>High Tech: The code lives in a distinct world, where the CLR controls most aspects of program execution at a very fine level (potentially to individual instructions), and the CLR detects when execution enters and exits managed code.  This enables a wide variety of useful features.</li> <li>Low Tech: The fact that there is a transition cost when going from managed to unmanaged code, as well as the fact that unmanaged code cannot use GC objects encourages the practice of wrapping most unmanaged code in a managed fa\u00e7ade.  This means interfaces can get a \"facelift\" to simplify them and to conform to a uniform set of naming and design guidelines that produce a level of consistency and discoverability that could have existed in the unmanaged world, but does not.</li> </ol> <p>Both of these characteristics are very important to the success of managed code.</p>"},{"location":"intro-to-clr/#memory-and-type-safety","title":"Memory and Type Safety","text":"<p>One of the less obvious but quite far-reaching features that a garbage collector enables is that of memory safety.  The invariant of memory safety is very simple: a program is memory safe if it accesses only memory that has been allocated (and not freed).  This simply means that you don't have \"wild\" (dangling) pointers that are pointing at random locations (more precisely, at memory that was freed prematurely).  Clearly, memory safety is a property we want all programs to have.  Dangling pointers are always bugs, and tracking them down is often quite difficult.</p> <p>A GC is necessary to provide memory safety guarantees</p> <p>One can quickly see how a garbage collector helps in ensuring memory safety because it removes the possibility that users will prematurely free memory (and thus access memory that was not properly allocated).  What may not be so obvious is that if you want to guarantee memory safety (that is make it impossible for programmers to create memory-unsafe programs), practically speaking you can't avoid having a garbage collector.  The reason for this is that non-trivial programs need heap style (dynamic) memory allocations, where the lifetime of the objects is essentially under arbitrary program control (unlike stack-allocated, or statically-allocated memory, which has a highly constrained allocation protocol).  In such an unconstrained environment, the problem of determining whether a particular explicit delete statement is correct becomes impossible to predict by program analysis.  Effectively, the only way you have to determine if a delete is correct is to check it at runtime.  This is exactly what a GC does (checks to see if memory is still live).  Thus, for any programs that need heap-style memory allocations, if you want to guarantee memory safety, you need a GC.</p> <p>While a GC is necessary to ensure memory safety, it is not sufficient.  The GC will not prevent the program from indexing off the end of an array or accessing a field off the end of an object (possible if you compute the field's address using a base and offset computation).  However, if we do prevent these cases, then we can indeed make it impossible for a programmer to create memory-unsafe programs.</p> <p>While the common intermediate language (CIL) does have operators that can fetch and set arbitrary memory (and thus violate memory safety), it also has the following memory-safe operators and the CLR strongly encourages their use in most programming:</p> <ol> <li>Field-fetch operators (LDFLD, STFLD, LDFLDA) that fetch (read), set and take the address of a field by name.</li> <li>Array-fetch operators (LDELEM, STELEM, LDELEMA) that fetch, set and take the address of an array element by index.  All arrays include a tag specifying their length.  This facilitates an automatic bounds check before each access.</li> </ol> <p>By using these operators instead of the lower-level (and unsafe) memory-fetch operators in user code, as well as avoiding other unsafe CIL operators (e.g., those that allow you to jump to arbitrary, and thus possibly bad locations) one could imagine building a system that is memory-safe but nothing more.  The CLR does not do this, however.  Instead the CLR enforces a stronger invariant: type safety.</p> <p>For type safety, conceptually each memory allocation is associated with a type.  All operators that act on memory locations are also conceptually tagged with the type for which they are valid.  Type safety then requires that memory tagged with a particular type can only undergo operations allowed for that type.  Not only does this ensure memory safety (no dangling pointers), it also allows additional guarantees for each individual type.</p> <p>One of the most important of these type-specific guarantees is that the visibility attributes associated with a type (and in particular with fields) are enforced.  Thus, if a field is declared to be private (accessible only by the methods of the type), then that privacy will indeed be respected by all other type-safe code.  For example, a particular type might declare a count field that represents the count of items in a table.  Assuming the fields for the count and the table are private, and assuming that the only code that updates them updates them together, there is now a strong guarantee (across all type-safe code) that the count and the number of items in the table are indeed in sync.  When reasoning about programs, programmers use the concept of type safety all the time, whether they know it or not.  The CLR elevates type-safety from being simply a programming language/compiler convention, to something that can be strictly enforced at run time.</p>"},{"location":"intro-to-clr/#verifiable-code-enforcing-memory-and-type-safety","title":"Verifiable Code - Enforcing Memory and Type Safety","text":"<p>Conceptually, to enforce type safety, every operation that the program performs has to be checked to ensure that it is operating on memory that was typed in a way that is compatible with the operation.  While the system could do this all at runtime, it would be very slow.  Instead, the CLR has the concept of CIL verification, where a static analysis is done on the CIL (before the code is run) to confirm that most operations are indeed type-safe.  Only when this static analysis can't do a complete job are runtime checks necessary.  In practice, the number of run-time checks needed is actually very small.  They include the following operations:</p> <ol> <li>Casting a pointer to a base type to be a pointer to a derived type (the opposite direction can be checked statically)</li> <li>Array bounds checks (just as we saw for memory safety)</li> <li>Assigning an element in an array of pointers to a new (pointer) value.  This particular check is only required because CLR arrays have liberal casting rules (more on that later...)</li> </ol> <p>Note that the need to do these checks places requirements on the runtime.  In particular:</p> <ol> <li>All memory in the GC heap must be tagged with its type (so the casting operator can be implemented).  This type information must be available at runtime, and it must be rich enough to determine if casts are valid (e.g., the runtime needs to know the inheritance hierarchy).  In fact, the first field in every object on the GC heap points to a runtime data structure that represents its type.</li> <li>All arrays must also have their size (for bounds checking).</li> <li>Arrays must have complete type information about their element type.</li> </ol> <p>Luckily, the most expensive requirement (tagging each heap item) was something that was already necessary to support garbage collection (the GC needs to know what fields in every object contain references that need to be scanned), so the additional cost to provide type safety is low.</p> <p>Thus, by verifying the CIL of the code and by doing a few run-time checks, the CLR can ensure type safety (and memory safety).  Nevertheless, this extra safety exacts a price in programming flexibility.  While the CLR does have general memory fetch operators, these operators can only be used in very constrained ways for the code to be verifiable.  In particular, all pointer arithmetic will fail verification today.  Thus many classic C or C++ conventions cannot be used in verifiable code; you must use arrays instead.  While this constrains programming a bit, it really is not bad (arrays are quite powerful), and the benefits (far fewer \"nasty\" bugs), are quite real.</p> <p>The CLR strongly encourages the use of verifiable, type-safe code.  Even so, there are times (mostly when dealing with unmanaged code) that unverifiable programming is needed.  The CLR allows this, but the best practice here is to try to confine this unsafe code as much as possible.  Typical programs have only a very small fraction of their code that needs to be unsafe, and the rest can be type-safe.</p>"},{"location":"intro-to-clr/#high-level-features","title":"High Level Features","text":"<p>Supporting garbage collection had a profound effect on the runtime because it requires that all code must support extra bookkeeping.  The desire for type-safety also had a profound effect, requiring that the description of the program (the CIL) be at a high level, where fields and methods have detailed type information.  The desire for type safety also forces the CIL to support other high-level programming constructs that are type-safe.  Expressing these constructs in a type-safe manner also requires runtime support.  The two most important of these high-level features are used to support two essential elements of object oriented programming: inheritance and virtual call dispatch.</p>"},{"location":"intro-to-clr/#object-oriented-programming","title":"Object Oriented Programming","text":"<p>Inheritance is relatively simple in a mechanical sense.  The basic idea is that if the fields of type <code>derived</code> are a superset of the fields of type <code>base</code>, and <code>derived</code> lays out its fields so the fields of <code>base</code> come first, then any code that expects a pointer to an instance of <code>base</code> can be given a pointer to an instance of <code>derived</code> and the code will \"just work\".  Thus, type <code>derived</code> is said to inherit from <code>base</code>, meaning that it can be used anywhere <code>base</code> can be used.  Code becomes polymorphic because the same code can be used on many distinct types.  Because the runtime needs to know what type coercions are possible, the runtime must formalize the way inheritance is specified so it can validate type safety.</p> <p>Virtual call dispatch generalizes inheritance polymorphism.  It allows base types to declare methods that will be overridden by derived types.  Code that uses variables of type <code>base</code> can expect that calls to virtual methods will be dispatched to the correct overridden method based on the actual type of the object at run time.  While such run-time dispatch logic could have been implemented using primitive CIL instructions without direct support in the runtime, it would have suffered from two important disadvantages</p> <ol> <li>It would not be type safe (mistakes in the dispatch table are catastrophic errors)</li> <li>Each object-oriented language would likely implement a slightly different way of implementing its virtual dispatch logic.  As result, interoperability among languages would suffer (one language could not inherit from a base type implemented in another language).</li> </ol> <p>For this reason, the CLR has direct support for basic object-oriented features.  To the degree possible, the CLR tried to make its model of inheritance \"language neutral,\" in the sense that different languages might still share the same inheritance hierarchy.  Unfortunately, that was not always possible.  In particular, multiple inheritance can be implemented in many different ways.  The CLR chose not to support multiple inheritance on types with fields, but does support multiple inheritance from special types (called interfaces) that are constrained not to have fields.</p> <p>It is important to keep in mind that while the runtime supports these object-oriented concepts, it does not require their use.  Languages without the concept of inheritance (e.g., functional languages) simply don't use these facilities.</p>"},{"location":"intro-to-clr/#value-types-and-boxing","title":"Value Types (and Boxing)","text":"<p>A profound, yet subtle aspect of object oriented programming is the concept of object identity: the notion that objects (allocated by separate allocation calls) can be distinguished, even if all their field values are identical.  Object identity is strongly related to the fact that objects are accessed by reference (pointer) rather than by value.  If two variables hold the same object (their pointers address the same memory), then updates to one of the variables will affect the other variable.</p> <p>Unfortunately, the concept of object identity is not a good semantic match for all types.  In particular, programmers don't generally think of integers as objects.  If the number '1' was allocated at two different places, programmers generally want to consider those two items equal, and certainly don't want updates to one of those instances affecting the other.  In fact, a broad class of programming languages called `functional languages' avoid object identity and reference semantics altogether.</p> <p>While it is possible to have a \"pure\" object oriented system, where everything (including integers) is an object (Smalltalk-80 does this), a certain amount of implementation \"gymnastics\" is necessary to undo this uniformity to get an efficient implementation.  Other languages (Perl, Java, JavaScript) take a pragmatic view and treat some types (like integers) by value, and others by reference.  The CLR also chose a mixed model, but unlike the others, allowed user-defined value types.</p> <p>The key characteristics of value types are:</p> <ol> <li>Each local variable, field, or array element of a value type has a distinct copy of the data in the value.</li> <li>When one variable, field or array element is assigned to another, the value is copied.</li> <li>Equality is always defined only in terms of the data in the variable (not its location).</li> <li>Each value type also has a corresponding reference type which has only one implicit, unnamed field.  This is called its boxed value.  Boxed value types can participate in inheritance and have object identity (although using the object identity of a boxed value type is strongly discouraged).</li> </ol> <p>Value types very closely model the C (and C++) notion of a struct (or C++ class).  Like C you can have pointers to value types, but the pointers are a type distinct from the type of the struct.</p>"},{"location":"intro-to-clr/#exceptions","title":"Exceptions","text":"<p>Another high-level programming construct that the CLR directly supports is exceptions.  Exceptions are a language feature that allows programmers to throw an arbitrary object at the point that a failure occurs.  When an object is thrown, the runtime searches the call stack for a method that declares that it can catch the exception.  If such a catch declaration is found, execution continues from that point.  The usefulness of exceptions is that they avoid the very common mistake of not checking if a called method fails.  Given that exceptions help avoid programmer mistakes (thus making programming easier), it is not surprising that the CLR supports them.</p> <p>As an aside, while exceptions avoid one common error (not checking for failure), they do not prevent another (restoring data structures to a consistent state in the event of a failure).  This means that after an exception is caught, it is difficult in general to know if continuing execution will cause additional errors (caused by the first failure).  This is an area where the CLR is likely to add value in the future.  Even as currently implemented, however, exceptions are a great step forward (we just need to go further).</p>"},{"location":"intro-to-clr/#parameterized-types-generics","title":"Parameterized Types (Generics)","text":"<p>Previous to version 2.0 of the CLR, the only parameterized types were arrays.  All other containers (such as hash tables, lists, queues, etc.), all operated on a generic Object type.  The inability to create List, or Dictionary certainly had a negative performance effect because value types needed to be boxed on entry to a collection, and explicit casting was needed on element fetch.  Nevertheless, that is not the overriding reason for adding parameterized types to the CLR.  The main reason is that parameterized types make programming easier. <p>The reason for this is subtle.  The easiest way to see the effect is to imagine what a class library would look like if all types were replaced with a generic Object type.  This effect is not unlike what happens in dynamically typed languages like JavaScript.  In such a world, there are simply far more ways for a programmer to make incorrect (but type-safe) programs.  Is the parameter for that method supposed to be a list? a string? an integer? any of the above? It is no longer obvious from looking at the method's signature.  Worse, when a method returns an Object, what other methods can accept it as a parameter? Typical frameworks have hundreds of methods; if they all take parameters of type Object, it becomes very difficult to determine which Object instances are valid for the operations the method will perform.  In short, strong typing helps a programmer express their intent more clearly, and allows tools (e.g., the compiler) to enforce their intent.  This results in a big productivity boost.</p> <p>These benefits do not disappear just because the type gets put into a List or a Dictionary, so clearly parameterized types have value.  The only real question is whether parameterized types are best thought of as a language specific feature which is \"compiled out\" by the time CIL is generated, or whether this feature should have first class support in the runtime.  Either implementation is certainly possible.  The CLR team chose first class support because without it, parameterized types would be implemented different ways by different languages.  This would imply that interoperability would be cumbersome at best.  In addition, expressing programmer intent for parameterized types is most valuable at the interface of a class library.  If the CLR did not officially support parameterized types, then class libraries could not use them, and an important usability feature would be lost.</p>"},{"location":"intro-to-clr/#programs-as-data-reflection-apis","title":"Programs as Data (Reflection APIs)","text":"<p>The fundamentals of the CLR are garbage collection, type safety, and high-level language features.  These basic characteristics forced the specification of the program (the CIL) to be fairly high level.  Once this data existed at run time (something not true for C or C++ programs), it became obvious that it would also be valuable to expose this rich data to end programmers.  This idea resulted in the creation of the System.Reflection interfaces (so-called because they allow the program to look at (reflect upon) itself).  This interface allows you to explore almost all aspects of a program (what types it has, the inheritance relationship, and what methods and fields are present).  In fact, so little information is lost that very good \"decompilers\" for managed code are possible (e.g., NET Reflector).  While those concerned with intellectual property protection are aghast at this capability (which can be fixed by purposefully destroying information through an operation called obfuscating the program), the fact that it is possible is a testament to the richness of the information available at run time in managed code.</p> <p>In addition to simply inspecting programs at run time, it is also possible to perform operations on them (e.g., invoke methods, set fields, etc.), and perhaps most powerfully, to generate code from scratch at run time (System.Reflection.Emit).  In fact, the runtime libraries use this capability to create specialized code for matching strings (System.Text.RegularExpressions), and to generate code for \"serializing\" objects to store in a file or send across the network.  Capabilities like this were simply infeasible before (you would have to write a compiler!) but thanks to the runtime, are well within reach of many more programming problems.</p> <p>While reflection capabilities are indeed powerful, that power should be used with care.  Reflection is usually significantly slower than its statically compiled counterparts.  More importantly, self-referential systems are inherently harder to understand.  This means that powerful features such as Reflection or Reflection.Emit should only be used when the value is clear and substantial.</p>"},{"location":"intro-to-clr/#other-features","title":"Other Features","text":"<p>The last grouping of runtime features are those that are not related to the fundamental architecture of the CLR (GC, type safety, high-level specification), but nevertheless fill important needs of any complete runtime system.</p>"},{"location":"intro-to-clr/#interoperation-with-unmanaged-code","title":"Interoperation with Unmanaged Code","text":"<p>Managed code needs to be able to use functionality implemented in unmanaged code.  There are two main \"flavors\" of interoperation.  First is the ability simply to call unmanaged functions (this is called Platform Invoke or PINVOKE).  Unmanaged code also has an object-oriented model of interoperation called COM (component object model) which has more structure than ad hoc method calls.  Since both COM and the CLR have models for objects and other conventions (how errors are handled, lifetime of objects, etc.), the CLR can do a better job interoperating with COM code if it has special support.</p>"},{"location":"intro-to-clr/#ahead-of-time-compilation","title":"Ahead of time Compilation","text":"<p>In the CLR model, managed code is distributed as CIL, not native code.  Translation to native code occurs at run time.  As an optimization, the native code that is generated from the CIL can be saved in a file using a tool called crossgen (similar to .NET Framework NGEN tool).  This avoids large amounts of compilation time at run time and is very important because the class library is so large.</p>"},{"location":"intro-to-clr/#threading","title":"Threading","text":"<p>The CLR fully anticipated the need to support multi-threaded programs in managed code.  From the start, the CLR libraries contained the System.Threading.Thread class which is a 1-to-1 wrapper over the operating system notion of a thread of execution.  However, because it is just a wrapper over the operating system thread, creating a System.Threading.Thread is relatively expensive (it takes milliseconds to start).  While this is fine for many operations, one style of programming creates very small work items (taking only tens of milliseconds).  This is very common in server code (e.g., each task is serving just one web page) or in code that tries to take advantage of multi-processors (e.g., a multi-core sort algorithm).  To support this, the CLR has the notion of a ThreadPool which allows WorkItems to be queued.  In this scheme, the CLR is responsible for creating the necessary threads to do the work.  While the CLR does expose the ThreadPool directly as the System.Threading.Threadpool class, the preferred mechanism is to use the Task Parallel Library, which adds additional support for very common forms of concurrency control.</p> <p>From an implementation perspective, the important innovation of the ThreadPool is that it is responsible for ensuring that the optimal number of threads are used to dispatch the work.  The CLR does this using a feedback system where it monitors the throughput rate and the number of threads and adjusts the number of threads to maximize the throughput.  This is very nice because now programmers can think mostly in terms of \"exposing parallelism\" (that is, creating work items), rather than the more subtle question of determining the right amount of parallelism (which depends on the workload and the hardware on which the program is run).</p>"},{"location":"intro-to-clr/#summary-and-resources","title":"Summary and Resources","text":"<p>Phew!  The runtime does a lot! It has taken many pages just to describe some of the features of the runtime, without even starting to talk about internal details.  The hope is, however, that this introduction will provide a useful framework for a deeper understanding of those internal details.  The basic outline of this framework is:</p> <ul> <li>The Runtime is a complete framework for supporting programming languages</li> <li>The Runtime's goal is to make programming easy.</li> <li>The Fundamental features of the runtime are:</li> <li>Garbage Collection</li> <li>Memory and Type Safety</li> <li>Support for High-Level Language Features</li> </ul>"},{"location":"intro-to-clr/#useful-links","title":"Useful Links","text":"<ul> <li>MSDN Entry for the CLR</li> <li>Wikipedia Entry for the CLR</li> <li>ECMA Standard for the Common Language Infrastructure (CLI)</li> <li>.NET Framework Design Guidelines</li> <li>CoreCLR Repo Documentation</li> </ul>"},{"location":"logging/","title":"Runtime logging for developers","text":"<p>Date: Feb. 2024</p> <p>The .NET runtime codebase is massive, spread across thousands of files in multiple languages. There are thousands of log messages spread throughout that codebase, and most of them are disabled by default. So how do you, a runtime developer beset by all manner of mysterious test failures, get those messages out of the runtime and into your console or log files? And how should you go about adding log messages appropriately to new code you're writing?</p> <p>NOTE: This document will repeatedly reference many environment variables. For a detailed list of environment variables supported by the runtime and explanations of what they do, consult <code>clrconfigvalues.h</code>, <code>gcconfig.h</code>, and <code>jitconfigvalues.h</code>.</p>"},{"location":"logging/#types-of-logging","title":"Types of logging","text":""},{"location":"logging/#eventpipe","title":"EventPipe","text":"<p>EventPipe is a cross-platform tracing system similar to ETW that is used widely in the .NET runtime. For a detailed overview of EventPipe, see the EventPipe documentation pages on .NET Learn. It offers excellent performance along with lots of flexibility, so it is one of the main ways to get information from an active runtime that we advertise. Most events that we support in debug builds also are exposed in release builds.</p> <p>For basic scenarios, you can use the <code>dotnet-counters</code> tool to monitor events and performance counters the runtime reports through EventPipe. For example, to collect the default counters from a project while running it, you could use <code>dotnet-counters collect -- dotnet exec myapp.dll</code>, where the part after <code>--</code> specifies a command to trace.</p> <p>You can also use the <code>dotnet-trace</code> tool to capture EventPipe events from the runtime on the fly. It provides command line options you can use to filter events by severity level or based on keywords. For example, to capture informational GC events from a project while running it, you could use <code>dotnet-trace collect --clreventlevel informational --clrevents gc -- dotnet exec myapp.dll</code>, where the part after <code>--</code> specifies a command to trace. The <code>collect</code> command also supports a set of convenient default profiles, accessible via the <code>--profile</code> switch, i.e. <code>--profile gc-collect</code>: <pre><code>cpu-sampling     - Useful for tracking CPU usage and general .NET runtime information. This is the default option if no profile or providers are specified.\ngc-verbose       - Tracks GC collections and samples object allocations.\ngc-collect       - Tracks GC collections only at very low overhead.\ndatabase         - Captures ADO.NET and Entity Framework database commands\n</code></pre></p> <p>The <code>dotnet-trace collect</code> command will produce a <code>.nettrace</code> file, which you can analyze via <code>dotnet-trace report topN</code> (for CPU sampling information) or <code>dotnet-trace convert</code>. You can also open this <code>.nettrace</code> file directly in Visual Studio to inspect it, for example by double-clicking it in solution explorer or dragging it into the Visual Studio window.</p> <p>If your application fails to run under <code>dotnet-counters</code> or <code>dotnet-trace</code>'s monitoring, try passing the <code>--show-child-io</code> tool argument to make the child process's output visible so you can spot any error messages. Make sure you're running it from the right working directory with any necessary environment variables.</p> <p>Please see the linked documentation pages for more information on how to use each of the above tools. They have an extensive set of options and some helpful features like time limits, attaching to existing processes, and configurable output formatting.</p> <p>To emit your very own EventPipe events from within the C++ part of the runtime, you can use the <code>FireEtwXXX</code> APIs, or their convenience wrappers within the <code>ETW::</code> C++ namespace. You likely will want to create new type(s) of events instead of using existing ones. The events are generated by the <code>genEventing.py</code> script based on the definitions in <code>ClrEtwAll.man</code>.</p> <p>To emit EventPipe events from C# you can use the <code>System.Diagnostics.Tracing.EventSource</code> class, by defining your own event types derived from it. See its documentation for examples and more details.</p>"},{"location":"logging/#alternative-event-consumers","title":"Alternative event consumers","text":"<p>For advanced scenarios, EventSource and the C++ event wrappers both support standard event tracing systems on their respective platforms (ETW on Windows, LTTNG on Linux). EventPipe itself does not integrate with these systems, but the runtime's layers on top of EventPipe will also use them if they are enabled.</p> <p>On Windows you can use standard ETW tools like WPR with its '.NET Activity' scenario. A convenient all-purpose tool for collecting and analyzing ETW traces on Windows is PerfView. To use PerfView you'll want to consult its extensive documentation or go through the tutorial, but a basic starting point is to launch your application via the Collect-&gt;Run menu option, and then double-click the resulting recording in the explorer pane to the left in order to open it.</p> <p>On Linux you can use LTTNG, and the perfcollect script is a useful tool you can use for a more ETW-like experience.</p>"},{"location":"logging/#stresslog","title":"StressLog","text":"<p>The StressLog is a circular buffer inside the runtime process that usually does not escape, and most StressLog messages are available in retail builds, which makes it useful for troubleshooting issues with GC or other subsystems in production scenarios. To enable it, you can set the <code>DOTNET_StressLog</code> environment variable to <code>1</code>, and you can configure it using environment variables, as demonstrated below:</p> <pre><code>echo Enable StressLog\nset DOTNET_StressLog=1\n\necho Show JIT log messages\nset DOTNET_LogFacility=0x00000008\n\necho Only show warnings or errors\nset DOTNET_LogLevel=3\n\necho If the buffer is filling too quickly, it can be helpful to set large buffer sizes.\necho But if you have many threads, you may want to set a low limit to avoid exhausting memory.\necho Set a per-thread size limit for StressLog of 20MB. Config settings default to hex.\nset DOTNET_StressLogSize=1400000\n\necho Set a process-wide size limit for all StressLogs (combined) of 400MB. Config settings default to hex.\nset DOTNET_TotalStressLogSize=18000000\n\necho Write the StressLog's to a file instead of memory for cases where you can't attach a debugger or get a dump file.\nset DOTNET_StressLogFilename=mystresslog.log\n</code></pre> <p>To write your own messages to the StressLog from C++, you can use the <code>STRESS_LOGN(facility, level, msg, ...)</code> macros, i.e. <code>STRESS_LOG1(LF_GC, LL_ERROR, \"A significant but non-fatal error occurred in the garbage collector! Here's my favorite number: %d\\n\", 42)</code> where the first argument is one or more logging facilities (you can combine them using <code>|</code> i.e. <code>LF_GC | LF_GCROOTS</code>), the second argument is a severity level, and the third argument is the log message format string. See log facilities and levels, below, for more information.</p> <p>You can't write your own messages to the StressLog from C#. If for some reason you need to while testing, perhaps you could expose it via a custom qcall.</p>"},{"location":"logging/#traditional-net-runtime-logging","title":"Traditional .NET Runtime Logging","text":"<p>\"Traditional\" log messages are only available in debug or checked builds of the runtime. To enable them, you can set the <code>DOTNET_LogEnable</code> environment variable to <code>1</code>, and configure them using environment variables. There are various configuration variables for traditional logging, demonstrated below:</p> <pre><code>echo Enable traditional logging\nset DOTNET_LogEnable=1\n\necho Show JIT log messages\nset DOTNET_LogFacility=0x00000008\n\necho Only show warnings or errors\nset DOTNET_LogLevel=3\n\necho Log messages to the debugger\nset DOTNET_LogToDebugger=0\n\necho Log messages to the console\nset DOTNET_LogToConsole=1\n\necho Log messages to a specific file\nset DOTNET_LogToFile=0\nset DOTNET_LogFile=mylog.log\n\necho Append log messages to the file instead of erasing it at start\nset DOTNET_LogFileAppend=1\n\necho Flush the log file after every write to ensure messages are not lost after a crash\nset DOTNET_LogFlushFile=1\n\necho Attach the process ID to every log message for multi-process scenarios\nset DOTNET_LogWithPid=1\n</code></pre> <p>This classical logging system is not frequently used, so individual log messages you encounter may be partially or completely nonfunctional - for example, 64-bit-only issues in outdated log statements - but the basics should always work.</p> <p>To send your own traditional log messages from C++, you can use the <code>LOG((facility, level, msg, ...))</code> macro, i.e. <code>LOG((LF_GC, LL_INFO10000, \"An insignificant thing happened in the garbage collector.\\n\"))</code>. The arguments are roughly equivalent to those described above for StressLog. See log facilities and levels, below, for more information on facilities and levels.</p> <p>Note that the <code>LOG</code> and <code>STRESS_LOG</code> macros are very similar, so for debugging purposes you can temporarily convert a <code>LOG((...))</code> statement into a <code>STRESS_LOG(...)</code> statement in order to take advantage of StressLog functionality to diagnose an issue.</p> <p>If you need to send messages to the traditional log from C#, similar to StressLog you could expose it via a custom qcall temporarily.</p>"},{"location":"logging/#log-facilities-and-levels","title":"Log Facilities and Levels","text":"<p>StressLog and traditional logging both rely on the <code>DOTNET_LogFacility</code>, <code>DOTNET_LogFacility2</code>, and <code>DOTNET_LogLevel</code> environment variables to control their level of verbosity and filter the information logged.</p> <p><code>DOTNET_LogLevel</code> allows filtering out log messages of lower importance, regardless of category. This variable is specified as an integer, not a named constant. The levels as of this writing are listed below, and come from <code>log.h</code>:</p> <pre><code>LL_EVERYTHING  10\nLL_INFO1000000  9 // can be expected to generate 1,000,000 logs per small but not trivial run\nLL_INFO100000   8 // can be expected to generate 100,000 logs per small but not trivial run\nLL_INFO10000    7 // can be expected to generate 10,000 logs per small but not trivial run\nLL_INFO1000     6 // can be expected to generate 1,000 logs per small but not trivial run\nLL_INFO100      5 // can be expected to generate 100 logs per small but not trivial run\nLL_INFO10       4 // can be expected to generate 10 logs per small but not trivial run\nLL_WARNING      3\nLL_ERROR        2\nLL_FATALERROR   1\nLL_ALWAYS       0 // impossible to turn off (log level never negative)\n</code></pre> <p><code>DOTNET_LogFacility</code> and <code>DOTNET_LogFacility2</code> allow filtering messages to specific categories. These variables are specified as integers, not named constants. For example, the <code>LF_GC</code> facility's value is <code>0x00000001</code> or just <code>1</code>. The list of available options for <code>DOTNET_LogFacility1</code> as of this writing are listed below, and come from <code>loglf.h</code>: <pre><code>LF_GC                0x00000001\nLF_GCINFO            0x00000002\nLF_STUBS             0x00000004\nLF_JIT               0x00000008\nLF_LOADER            0x00000010\nLF_METADATA          0x00000020\nLF_SYNC              0x00000040\nLF_EEMEM             0x00000080\nLF_GCALLOC           0x00000100\nLF_CORDB             0x00000200\nLF_CLASSLOADER       0x00000400\nLF_CORPROF           0x00000800\nLF_DIAGNOSTICS_PORT  0x00001000\nLF_DBGALLOC          0x00002000\nLF_EH                0x00004000\nLF_ENC               0x00008000\nLF_ASSERT            0x00010000\nLF_VERIFIER          0x00020000\nLF_THREADPOOL        0x00040000\nLF_GCROOTS           0x00080000\nLF_INTEROP           0x00100000\nLF_MARSHALER         0x00200000\nLF_TIEREDCOMPILATION 0x00400000 // This used to be IJW, but now repurposed for tiered compilation\nLF_ZAP               0x00800000\nLF_STARTUP           0x01000000 // Log startup and shutdown failures\nLF_APPDOMAIN         0x02000000\nLF_CODESHARING       0x04000000\nLF_STORE             0x08000000\nLF_SECURITY          0x10000000\nLF_LOCKS             0x20000000\nLF_BCL               0x40000000\n</code></pre></p> <p><code>DOTNET_LogFacility2</code> is newer, and at present only has one facility available: <code>LF2_MULTICOREJIT</code>, with a value of <code>0x00000001</code>.</p> <p>If you're trying to capture a specific message in the log and it's not showing up, make sure you've checked its log level/facility and set your environment variables accordingly!</p>"},{"location":"logging/#mono-logging","title":"Mono logging","text":"<p>Builds of .NET using the Mono runtime have their own Mono-specific log configuration environment variables that control what diagnostic information is logged by the runtime.</p> <p>You can use <code>MONO_LOG_LEVEL</code> to configure the overall verbosity by setting it to one of <code>\"error\"</code>, <code>\"critical\"</code>, <code>\"warning\"</code>, <code>\"message\"</code>, <code>\"info\"</code>, or <code>\"debug\"</code>. You can use <code>MONO_LOG_MASK</code> to filter the log messages down to specific categories like <code>gc</code> or <code>aot</code>. For a full list of mask options, see <code>mono-logger.c</code>'s <code>mono_trace_set_mask_string</code> function.</p> <p>When dealing with Mono's interpreter, the <code>MONO_VERBOSE_METHOD</code> environment variable turns on verbose logging for methods with a specific name. This can be very helpful if you are investigating situations where a method is being compiled or optimized incorrectly, or you're not certain which version of a method is running.</p>"},{"location":"logging/#webassembly-logging","title":"WebAssembly logging","text":"<p>To configure Mono logging in WebAssembly builds of the runtime, you will need to specify your environment variable(s) in the form of a JSON blob inside of a config item inside your csproj, like so: <pre><code>&lt;ItemGroup&gt;\n  &lt;WasmExtraConfig Include=\"environmentVariables\" Value='\n{\n  \"MONO_LOG_LEVEL\": \"warning\",\n  \"MONO_LOG_MASK\": \"all\"\n}' /&gt;\n&lt;/ItemGroup&gt;\n</code></pre> Environment variables set externally at build time or at web browser launch time will not automatically flow through into the WASM runtime.</p> <p>For WebAssembly builds of the runtime there is an additional interop layer written in TypeScript that has its own logging facilities, defined in <code>logging.ts</code>.</p> <p>For debug-severity log messages from the TypeScript layer, they will be suppressed by default unless the <code>diagnosticTracing</code> flag is set. To set it, call <code>.withDiagnosticTracing(true)</code> on the <code>dotnet</code> object during startup, or add an extra config item to your csproj like so: <pre><code>&lt;ItemGroup&gt;\n  &lt;WasmExtraConfig Include=\"diagnosticTracing\" Value=\"true\" /&gt;\n&lt;/ItemGroup&gt;\n</code></pre></p> <p>All other log severities are enabled by default, and they will be written to the developer tools console when running in a web browser. When running automated tests from the command line or running an application in an environment like node.js or the v8 shell, they will be written to standard output and/or standard error.</p> <p>To send messages from within TypeScript, use the appropriate API - <code>mono_log_error</code> for severe errors, <code>mono_log_info</code> for important information, and <code>mono_log_debug</code> for things that ordinary users will never need to see.</p> <p>If for some reason you need to access this logging facility directly from C/C++, you can use the <code>mono_wasm_trace_logger</code> function. Note that fatal errors sent through this function will trigger an immediate process exit after they are written.</p> <p>By default the jiterpreter will only log error messages to the console, but if <code>MONO_VERBOSE_METHOD</code> is used, it will also log verbose information on traces within verbose methods. More advanced logging in the jiterpreter requires editing the configuration variables in <code>jiterpreter.ts</code> and compiling the runtime from source.</p>"},{"location":"managed-type-system/","title":"Managed Type System Overview","text":"<p>Author: Michal Strehovsky (@MichalStrehovsky) - 2016</p>"},{"location":"managed-type-system/#introduction","title":"Introduction","text":"<p>The managed type system is a major component of new generation of .NET tools for AOT and IL verification. It represents the modules, types, methods, and fields within a program and provides higher level services to the type system users that lets them get answers to various interesting questions.</p> <p>The managed type system is equivalent of CoreCLR type system rewritten in C#. We've always wanted to implement runtime functionality in C#. The managed type system is the infrastructure that allows us to do that.</p> <p>Some of the high level services the type system provides are:</p> <ul> <li>Loading new types from the metadata</li> <li>Computing set of interfaces implemented by a specific type</li> <li>Computing static and instance field layout (assigning offsets to individual fields)</li> <li>Computing static and instance GC layout of types (identifying GC pointers within object/class data)</li> <li>Computing VTable layout (assigning slots to virtual methods) and resolving virtual methods to slots</li> <li>Deciding whether a type can be stored to a location of another type</li> </ul> <p>Three major themes drive the design of the type system:</p> <ol> <li>Low overhead and high performance</li> <li>Concurrency</li> <li>Extensibility and reusability</li> </ol> <p>Low overhead is achieved by lazy loading - instead of eagerly populating the types with fields, various attributes, names, etc. these are read on demand from the underlying data source (metadata). Caching is used conservatively.</p> <p>Where necessary, partial classes, extension methods, and pluggable algorithms are used to achieve goal 3 instead of polymorphism and object hierarchies. The reusability of the type system is at the source level (source-including different sets of files to get different features). This allows extensibility without making sacrifices that would take us away from goal 1.</p> <p>The type system in its purest form (i.e. without any partial class extensions) tries to avoid introducing concepts that are not defined in the ECMA-335 specification. The specification is a suggested prerequisite reading to this document and provides definitions to various terms used in this document.</p>"},{"location":"managed-type-system/#relationship-with-metadata","title":"Relationship with metadata","text":"<p>While metadata (such as the file formats described in the ECMA-335 specification) has a close relationship with the type system, there is a clear distinction between these two: the metadata describes physical shape of the type (e.g. what is the base class of the type; or what fields does it have), but the type system builds higher level concepts on top of the shape (e.g. how many bytes are required to store an instance of the type at runtime; what interfaces does the type implement, including the inherited ones).</p> <p>The type system provides access to most of the underlying metadata, but abstracts the way it was obtained. This allows types and members that are backed by metadata in other formats, or in no physical format at all (such as methods on array types), to be representable within the same type system context.</p>"},{"location":"managed-type-system/#type-system-class-hierarchy","title":"Type system class hierarchy","text":"<p>The classes that represent types within the type system are:</p> <p></p> <p>Most of the classes in this hierarchy are not supposed to be derived by the type system user and many of them are sealed to prevent that.</p> <p>The classes that are extensible (and are actually abstract classes) are shown with dark background above. The concrete class should provide implementation of the abstract and virtual methods based on some logic, such as reading metadata from an ECMA-335 module file (the type system already provides such implementation of <code>MetadataType</code> in its <code>EcmaType</code>, for example). Ideally, the type system consumers should operate on the abstract classes and use the concrete class only when creating a new instance. Casting to the concrete implementation type such as <code>EcmaType</code> is discouraged.</p>"},{"location":"managed-type-system/#type-system-classes","title":"Type system classes","text":"<p>Following section goes briefly over the classes representing types within the type system.</p>"},{"location":"managed-type-system/#typedesc","title":"TypeDesc","text":"<p><code>TypeDesc</code> is the base class of all types within the type system. It defines a list of operations all classes must support. Not all operations might make sense for all the children of <code>TypeDesc</code> (for example, it doesn't make sense to request a list of methods on a pointer type), but care is taken to provide an implementation that makes sense for each particular child (i.e. the list of methods on a pointer type is empty).</p>"},{"location":"managed-type-system/#parameterizedtype-arraytype-byreftype-pointertype","title":"ParameterizedType (ArrayType, ByRefType, PointerType)","text":"<p>These are constructed types with a single parameter:</p> <ul> <li>an array (either multi-dimensional, or a vector - a single dimensional array with an implicit zero lower bound),</li> <li>a managed reference, or</li> <li>an unmanaged pointer type.</li> </ul> <p>Note the distinction between multidimensional arrays of rank 1 and vectors is a crucial one, and a source of potential bugs for the type system users. Type system users should take special care.</p>"},{"location":"managed-type-system/#deftype-nometadatatype-metadatatype","title":"DefType (NoMetadataType, MetadataType)","text":"<p><code>DefType</code> represents a value type, interface, or a class. While most instances of <code>DefType</code> will be of children of <code>MetadataType</code> (a type that is based off of some concrete metadata describing the type in full), there will be scenarios where full metadata is no longer available. In those cases, only restricted information (such as the number of bytes occupied by the instance of the type on the GC heap, or whether the type is a value type) is available. It is important that the type system is able to operate on such types. E.g. it should be possible for a type with restricted metadata to be a base type for a type with full metadata and the field layout algorithm should be able to compute the field layout of such a type.</p>"},{"location":"managed-type-system/#genericparameter","title":"GenericParameter","text":"<p>Represents a generic parameter, along with its constraints. Generic definitions are represented as instantiations over generic parameters.</p> <p>Note for readers familiar with the .NET reflection type system: while the .NET reflection type system doesn't distinguish between a generic definition (e.g. <code>List&lt;T&gt;</code>) and an open instantiation of a generic type (e.g. <code>List&lt;!0&gt;</code>), the managed type system draws a distinction between those two. This distinction is important when representing member references from within IL method bodies - e.g. an IL reference using an LDTOKEN instruction to <code>List&lt;T&gt;.Add</code> should always refer to the uninstantiated definition, while a reference to <code>List&lt;!0&gt;.Add</code> will refer to a concrete method after substituting the signature variable.</p>"},{"location":"managed-type-system/#signaturevariable-signaturetypevariable-signaturemethodvariable","title":"SignatureVariable (SignatureTypeVariable, SignatureMethodVariable)","text":"<p>Signature variables represent variables that can be substituted by other types within the system. They differ from generic parameters (because e.g. they don't have constraints or variance). They are simply placeholders to be replaced by other types as part of a process called instantiation. Signature variables have an index that refers to a position within the instantiation context.</p>"},{"location":"managed-type-system/#other-type-system-classes","title":"Other type system classes","text":"<p>Each use of a type system starts with creating a type system context. A type system context represents a type universe across which all types share reference identity (two <code>TypeDesc</code> objects represent identical types if and only if they are the same object instance). Type system context is used to resolve all modules and constructed types within the universe. It's not legal to create new instances of constructed types outside of the type system context.</p> <p>Other important classes within the type system are a <code>MethodDesc</code> (represents a method within the type system) and <code>FieldDesc</code> (represents a field within the type system). A <code>ModuleDesc</code> describes a single module which can optionally implement <code>IAssemblyDesc</code> interface if the module is an assembly. <code>ModuleDesc</code> is typically the owner of the type/method/field definitions within the module. It's the responsibility of the <code>ModuleDesc</code> to maintain the reference identity of those.</p>"},{"location":"managed-type-system/#pluggable-algorithms","title":"Pluggable algorithms","text":"<p>Most algorithms (e.g. the field layout algorithm) provided by the type system are pluggable. The type system context can influence the choice of the algorithm by providing different implementations of it.</p> <p>The algorithms are used as an extensibility mechanism in places where partial classes and source inclusion wouldn't be sufficient. The choice of the particular algorithm might depend on multiple factors and the type system user might want to use multiple algorithms depending on a certain set of conditions determined at runtime (e.g. computing the list of runtime interfaces of regular <code>DefTypes</code> vs. the runtime interfaces of array types).</p>"},{"location":"managed-type-system/#hash-codes-within-the-type-system","title":"Hash codes within the type system","text":"<p>An interesting property of the type system lays in its ability to compute hash codes that can be reliably computed for any type or method represented within the system at compile time and at runtime. Having the same hash code available at both compile time and runtime is leveraged to build high performance lookup tables in AOT compiled code. The hash code is computed from type names and gets preserved as part of the runtime data structures so that it's available in situations when the type name has been optimized away by the compiler.</p>"},{"location":"managed-type-system/#throwing-exceptions-from-the-type-system","title":"Throwing exceptions from the type system","text":"<p>Throwing an exception from within the type system is a bit more involved than a simple <code>throw</code> statement. This is because the type system is designed to be usable in many places and each could have a different requirement about how exceptions are thrown. For example, when the type system is included from the runtime, a <code>System.TypeLoadException</code> should be thrown when type loading fails. On the other hand, if a type loading error occurs in a compiler or IL verifier, a <code>System.TypeLoadException</code> would be indistinguishable from an actual problem with the managed assemblies that comprise the compiler. Therefore a different exception should be thrown.</p> <p>Exception throwing within the type system is wrapped in a <code>ThrowHelper</code> class. The consumer of the type system provides a definition of this class and its methods. The methods control what exception type will be thrown.</p> <p>The type system provides a default implementation of the <code>ThrowHelper</code> class that throws exceptions deriving from a <code>TypeSystemException</code> exception base class. This default implementation is suitable for use in non-runtime scenarios.</p> <p>The exception messages are assigned string IDs and get consumed by the throw helper as well. We require this indirection to support the compiler scenarios: when a type loading exception occurs during an AOT compilation, the AOT compiler has two tasks - emit a warning to warn the user that this occurred, and potentially generate a method body that will throw this exception at runtime when the problematic type is accessed. The localization of the compiler might not match the localization of the class library the compiler output is linking against. Indirecting the actual exception message through the string ID lets us wrap this. The consumer of the type system may reuse the throw helper in places outside the type system where this functionality is needed.</p>"},{"location":"managed-type-system/#physical-architecture","title":"Physical architecture","text":"<p>The type system implementation is found in: * <code>src/coreclr/tools/Common/TypeSystem/Common</code>: most of the common type system is here * <code>src/coreclr/tools/Common/TypeSystem/Ecma</code>: concrete implementations of <code>MetadataType</code>, <code>MethodDesc</code>, <code>FieldDesc</code> etc. that read metadata from ECMA-335 module files is here * <code>src/coreclr/tools/aot/ILCompiler.TypeSystem.ReadyToRun.Tests</code>: unit tests that shed some light into the operation and features of the type system. This is a good starting point to learn about the code.</p>"},{"location":"managed-type-system/#notable-differences-from-coreclr-type-system","title":"Notable differences from CoreCLR type system","text":"<ul> <li><code>MethodDesc</code> has exact generic instantiations where possible in managed type system. The code sharing policy in managed type system is one of the pluggable algorithms and it does not affect <code>MethodDesc</code> identity. The code sharing policy in the CoreCLR type system is coupled with <code>MethodDesc</code> identity. See https://github.com/dotnet/runtime/pull/45744 for an example how this difference manifests itself.</li> </ul>"},{"location":"method-descriptor/","title":"Method Descriptor","text":"<p>Author: Jan Kotas (@jkotas) - 2006</p>"},{"location":"method-descriptor/#introduction","title":"Introduction","text":"<p>MethodDesc (method descriptor) is the internal representation of a managed method. It serves several purposes:</p> <ul> <li>Provides a unique method handle, usable throughout the runtime. For normal methods, the MethodDesc is a unique handle for a  triplet. <li>Caches frequently used information that is expensive to compute from metadata (e.g. whether the method is static).</li> <li>Captures the runtime state of the method (e.g. whether the code has been generated for the method already).</li> <li>Owns the entry point of the method.</li>"},{"location":"method-descriptor/#design-goals-and-non-goals","title":"Design Goals and Non-goals","text":""},{"location":"method-descriptor/#goals","title":"Goals","text":"<p>Performance: The design of MethodDesc is heavily optimized for size, since there is one of them for every method. For example, the MethodDesc for a normal non-generic method is 8 bytes in the current design.</p>"},{"location":"method-descriptor/#non-goals","title":"Non-goals","text":"<p>Richness: The MethodDesc does not cache all information about the method. It is expected that the underlying metadata has to be accessed for less frequently used information (e.g. method signature).</p>"},{"location":"method-descriptor/#design-of-methoddesc","title":"Design of MethodDesc","text":""},{"location":"method-descriptor/#kinds-of-methoddescs","title":"Kinds of MethodDescs","text":"<p>There are multiple kinds of MethodDescs:</p> <p>IL</p> <p>Used for regular IL methods.</p> <p>Instantiated</p> <p>Used for less common IL methods that have generic instantiation or that do not have preallocated slot in method table.</p> <p>FCall</p> <p>Internal methods implemented in unmanaged code. These are methods marked with MethodImplAttribute(MethodImplOptions.InternalCall) attribute, delegate constructors and tlbimp constructors.</p> <p>PInvoke</p> <p>P/Invoke methods. These are methods marked with DllImport attribute.</p> <p>EEImpl</p> <p>Delegate methods whose implementation is provided by the runtime (Invoke, BeginInvoke, EndInvoke). See ECMA 335 Partition II - Delegates.</p> <p>Array</p> <p>Array methods whose implementation is provided by the runtime (Get, Set, Address). See ECMA Partition II \u2013 Arrays.</p> <p>ComInterop</p> <p>COM interface methods. Since the non-generic interfaces can be used for COM interop by default, this kind is usually used for all interface methods.</p> <p>Dynamic</p> <p>Dynamically created methods without underlying metadata. Produced by Stub-as-IL and LKG (light-weight code generation).</p>"},{"location":"method-descriptor/#alternative-implementations","title":"Alternative Implementations","text":"<p>Virtual methods and inheritance would be the natural way to implement various kinds of MethodDesc in C++. The virtual methods would add vtable pointer to each MethodDesc, wasting a lot of precious space. The vtable pointer occupies 4 bytes on x86. Instead, the virtualization is implemented by switching based on the MethodDesc kind, which fits into 3 bits. For example:</p> <pre><code>DWORD MethodDesc::GetAttrs()\n{\n    if (IsArray())\n        return ((ArrayMethodDesc*)this)-&gt;GetAttrs();\n\n    if (IsDynamic())\n        return ((DynamicMethodDesc*)this)-&gt;GetAttrs();\n\n    return GetMDImport()-&gt;GetMethodDefProps(GetMemberDef());\n}\n</code></pre>"},{"location":"method-descriptor/#method-slots","title":"Method Slots","text":"<p>Each MethodDesc has a slot, which contains the current entry point of the method. The slot must exist for all methods, even the ones that never run like abstract methods. There are multiple places in the runtime that depend on mapping between entry points and MethodDescs.</p> <p>Each MethodDesc logically has an entry point, but we do not allocate these eagerly at MethodDesc creation time. The invariant is that once the method is identified as a method to run, or is used in virtual overriding, we will allocate the entrypoint.</p> <p>The slot is either in MethodTable or in MethodDesc itself. The location of the slot is determined by <code>mdcHasNonVtableSlot</code> bit on MethodDesc.</p> <p>The slot is stored in MethodTable for methods that require efficient lookup via slot index, e.g. virtual methods or methods on generic types. The MethodDesc contains the slot index to allow fast lookup of the entry point in this case.</p> <p>Otherwise, the slot is part of the MethodDesc itself. This arrangement improves data locality and saves working set. Also, it is not even always possible to preallocate a slot in a MethodTable upfront for dynamically created MethodDescs, such as for methods added by Edit &amp; Continue, instantiations of generic methods or dynamic methods.</p>"},{"location":"method-descriptor/#methoddesc-chunks","title":"MethodDesc Chunks","text":"<p>The MethodDescs are allocated in chunks to save space. Multiple MethodDesc tend to have identical MethodTable and upper bits of metadata token. MethodDescChunk is formed by hoisting the common information in front of an array of multiple MethodDescs. The MethodDesc contains just the index of itself in the array.</p> <p></p> <p>Figure 1 MethodDescChunk and MethodTable</p>"},{"location":"method-descriptor/#debugging","title":"Debugging","text":"<p>The following SOS commands are useful for debugging MethodDesc:</p> <ul> <li> <p>DumpMD \u2013 dump the MethodDesc content:</p> <pre><code>!DumpMD 00912fd8\nMethod Name: My.Main()\nClass: 009111ec\nMethodTable: 00912fe8md\nToken: 06000001\nModule: 00912c14\nIsJitted: yes\nCodeAddr: 00ca0070\n</code></pre> </li> <li> <p>IP2MD \u2013 find MethodDesc for given code address:</p> <pre><code>!ip2md 00ca007c\nMethodDesc: 00912fd8\nMethod Name: My.Main()\nClass: 009111ec\nMethodTable: 00912fe8md\nToken: 06000001\nModule: 00912c14\nIsJitted: yes\nCodeAddr: 00ca0070\n</code></pre> </li> <li> <p>Name2EE \u2013 find MethodDesc for given method name:</p> <pre><code>!name2ee hello.exe My.Main\nModule: 00912c14 (hello.exe)\nToken: 0x06000001\nMethodDesc: 00912fd8\nName: My.Main()\nJITTED Code Address: 00ca0070\n</code></pre> </li> <li> <p>Token2EE \u2013 find MethodDesc for given token (useful for finding MethodDesc for methods with weird names):</p> <pre><code>!token2ee hello.exe 0x06000001\nModule: 00912c14 (hello.exe)\nToken: 0x06000001\nMethodDesc: 00912fd\n8Name: My.Main()\nJITTED Code Address: 00ca0070\n</code></pre> </li> <li> <p>DumpMT \u2013 MD \u2013 dump all MethodDescs in the given MethodTable:</p> <pre><code>!DumpMT -MD 0x00912fe8\n...\nMethodDesc Table\n   Entry MethodDesc      JIT Name\n79354bec   7913bd48   PreJIT System.Object.ToString()\n793539c0   7913bd50   PreJIT System.Object.Equals(System.Object)\n793539b0   7913bd68   PreJIT System.Object.GetHashCode()\n7934a4c0   7913bd70   PreJIT System.Object.Finalize()\n00ca0070   00912fd8      JIT My.Main()\n0091303c   00912fe0     NONE My..ctor()\n</code></pre> </li> </ul> <p>A MethodDesc has fields with the name and signature of the method on debug builds. This is useful for debugging when the runtime state is severely corrupted and the SOS extension does not work.</p>"},{"location":"method-descriptor/#precode","title":"Precode","text":"<p>The precode is a small fragment of code used to implement temporary entry points and an efficient wrapper for stubs. Precode is a niche code-generator for these two cases, generating the most efficient code possible. In an ideal world, all native code dynamically generated by the runtime would be produced by the JIT. That's not feasible in this case, given the specific requirements of these two scenarios. The basic precode on x86 may look like this:</p> <pre><code>mov eax,pMethodDesc // Load MethodDesc into scratch register\njmp target          // Jump to a target\n</code></pre> <p>Efficient Stub wrappers: The implementation of certain methods (e.g. P/Invoke, delegate invocation, multidimensional array setters and getters) is provided by the runtime, typically as hand-written assembly stubs. Precode provides a space-efficient wrapper over stubs, to multiplex them for multiple callers.</p> <p>The worker code of the stub is wrapped by a precode fragment that can be mapped to the MethodDesc and that jumps to the worker code of the stub. The worker code of the stub can be shared between multiple methods this way. It is an important optimization used to implement P/Invoke marshalling stubs. It also creates a 1:1 mapping between MethodDescs and entry points, which establishes a simple and efficient low-level system.</p> <p>Temporary entry points: Methods must provide entry points before they are jitted so that jitted code has an address to call them. These temporary entry points are provided by precode. They are a specific form of stub wrappers.</p> <p>This technique is a lazy approach to jitting, which provides a performance optimization in both space and time. Otherwise, the transitive closure of a method would need to be jitted before it was executed. This would be a waste, since only the dependencies of taken code branches (e.g. if statement) require jitting.</p> <p>Each temporary entry point is much smaller than a typical method body. They need to be small since there are a lot of them, even at the cost of performance. The temporary entry points are executed just once before the actual code for the method is generated.</p> <p>The target of the temporary entry point is a PreStub, which is a special kind of stub that triggers jitting of a method. It atomically replaces the temporary entry point with a stable entry point. The stable entry point has to remain constant for the method lifetime. This invariant is required to guarantee thread safety since the method slot is always accessed without any locks taken.</p> <p>The stable entry point is either the native code or the precode. The native code is either jitted code or code saved in NGen image. It is common to talk about jitted code when we actually mean native code.</p> <p></p> <p>Figure 2 Entry Point State Diagram</p> <p>A method can have both native code and precode if there is a need to do work before the actual method body is executed. This situation typically happens for NGen image fixups. Native code is an optional MethodDesc slot in this case. This is necessary to lookup the native code of the method in a cheap uniform way.</p> <p></p> <p>Figure 3 The most complex case of Precode, Stub and Native Code</p>"},{"location":"method-descriptor/#single-callable-vs-multi-callable-entry-points","title":"Single Callable vs. Multi Callable entry points","text":"<p>Entry point is needed to call the method. The MethodDesc exposes methods that encapsulate logic to get the most efficient entry point for the given situation. The key difference is whether the entry point will be used to call the method just once or whether it will be used to call the method multiple times.</p> <p>For example, it may be a bad idea to use the temporary entry point to call the method multiple times since it would go through the PreStub each time. On the other hand, using temporary entry point to call the method just once should be fine.</p> <p>The methods to get callable entry points from MethodDesc are:</p> <ul> <li><code>MethodDesc::GetSingleCallableAddrOfCode</code></li> <li><code>MethodDesc::GetMultiCallableAddrOfCode</code></li> <li><code>MethodDesc::TryGetMultiCallableAddrOfCode</code></li> <li><code>MethodDesc::GetSingleCallableAddrOfVirtualizedCode</code></li> <li><code>MethodDesc::GetMultiCallableAddrOfVirtualizedCode</code></li> </ul>"},{"location":"method-descriptor/#types-of-precode","title":"Types of precode","text":"<p>There are multiple specialized types of precodes.</p> <p>The type of precode has to be cheaply computable from the instruction sequence. On x86 and x64, the type of precode is computed by fetching a byte at a constant offset. Of course, this imposes limits on the instruction sequences used to implement the various precode types.</p> <p>StubPrecode</p> <p>StubPrecode is the basic precode type. It loads MethodDesc into a scratch register<sup>2</sup> and then jumps. It must be implemented for precodes to work. It is used as fallback when no other specialized precode type is available.</p> <p>All other precodes types are optional optimizations that the platform specific files turn on via HAS_XXX_PRECODE defines.</p> <p>StubPrecode looks like this on x86:</p> <pre><code>mov eax,pMethodDesc\nmov ebp,ebp // dummy instruction that marks the type of the precode\njmp target\n</code></pre> <p>\"target\" points to prestub initially. It is patched to point to the final target. The final target (stub or native code) may or may not use MethodDesc in eax. Stubs often use it, native code does not use it.</p> <p>FixupPrecode</p> <p>FixupPrecode is used when the final target does not require MethodDesc in scratch register<sup>2</sup>. The FixupPrecode saves a few cycles by avoiding loading MethodDesc into the scratch register.</p> <p>Most stubs used are the more efficient form, we currently can use this form for everything but interop methods when a specialized form of Precode is not required.</p> <p>The initial state of the FixupPrecode on x86:</p> <pre><code>call PrecodeFixupThunk // This call never returns. It pops the return address\n                       // and uses it to fetch the pMethodDesc below to find\n                       // what the method that needs to be jitted\npop esi // dummy instruction that marks the type of the precode\ndword pMethodDesc\n</code></pre> <p>Once it has been patched to point to final target:</p> <pre><code>jmp target\npop edi\ndword pMethodDesc\n</code></pre> <p><sup>2</sup> Passing MethodDesc in scratch register is sometimes referred to as MethodDesc Calling Convention.</p> <p>ThisPtrRetBufPrecode</p> <p>ThisPtrRetBufPrecode is used to switch a return buffer and the this pointer for open instance delegates returning valuetypes. It is used to convert the calling convention of MyValueType Bar(Foo x) to the calling convention of MyValueType Foo::Bar().</p> <p>This precode is always allocated on demand as a wrapper of the actual method entry point and stored in a table (FuncPtrStubs).</p> <p>ThisPtrRetBufPrecode looks like this:</p> <pre><code>mov eax,ecx\nmov ecx,edx\nmov edx,eax\nnop\njmp entrypoint\ndw pMethodDesc\n</code></pre> <p>PInvokeImportPrecode</p> <p>PInvokeImportPrecode is used for lazy binding of unmanaged P/Invoke targets. This precode is for convenience and to reduce amount of platform specific plumbing.</p> <p>Each PInvokeMethodDesc has PInvokeImportPrecode in addition to the regular precode.</p> <p>PInvokeImportPrecode looks like this on x86:</p> <pre><code>mov eax,pMethodDesc\nmov eax,eax // dummy instruction that marks the type of the precode\njmp PInvokeImportThunk // loads P/Invoke target for pMethodDesc lazily\n</code></pre>"},{"location":"mixed-mode/","title":"Mixed Mode Assemblies","text":""},{"location":"mixed-mode/#introduction","title":"Introduction","text":"<p>Most interoperability between managed and native code uses P/Invokes, COM, or WinRT. Since P/Invokes are bound to native code at runtime, they're susceptible to mistakes ranging from incorrect naming to subtle mistakes in signatures that cause stack corruption. COM can also be used to call from native to managed code, but it often requires registration and can add performance overhead. WinRT avoids those problems but isn't available in all cases.</p> <p>C++/CLI provides a different, compiler-verified approach to interoperability called mixed-mode assemblies (also sometimes referred to as It-Just-Works or IJW). Rather than requiring that developers do special declarations like P/Invokes, the C++ compiler automatically generates everything needed to transition to and from managed and native code. Additionally, the C++ compiler decides whether a given C++ method should be managed or native, so even within an assembly, transitions happen regularly and without developer intervention required.</p>"},{"location":"mixed-mode/#calling-native-code","title":"Calling Native Code","text":"<p>C++/CLI code may call into either native code in the same assembly or a different library. Calls to a different library generates P/Invokes similar to those that might be written by hand in C# (but because the C++ compiler is reading that library's headers, the P/Invoke isn't subject to developer error). However, calls to the same assembly work differently. While P/Invokes to different libraries specify the name of the library and the name of an export to call, P/Invokes to the same library have a null entry point and set an RVA \u2013 an address within the library to call. In metadata, that looks like: <pre><code>MethodName: delete (060000EE)\nFlags     : [Assem] [Static] [ReuseSlot] [PinvokeImpl] [HasSecurity]  (00006013)\nRVA       : 0x0001332a\nPinvoke Map Data:\nEntry point:\n</code></pre> Calling these P/Invokes works the same as those that use named entry points with the exception of manually computing an address based on the module address and RVA instead of looking for an export.</p>"},{"location":"mixed-mode/#calling-managed-code","title":"Calling Managed Code","text":"<p>While native-&gt;native calls and managed-&gt;native calls can be based on the address of native functions, native-&gt;managed calls cannot since the managed code is non-executable IL. To solve that, the compiler generates a lookup table that appears in the CIL metadata header as the <code>.vtfixup</code> table. <code>Vtfixups</code> in the library on disk map from an RVA to a managed method token. When the assembly is loaded, the CLR generates a native-callable marshaling stub for each method in the <code>.vtfixup</code> table that calls the corresponding managed method. It then replaces the tokens with the addresses of the stub methods. When native code goes to call a managed method, it calls indirectly via the new address in the <code>.vtfixup</code> table.</p> <p>For example, if a native method in IjwLib.dll wants to call the managed Bar method with token 06000002, it emits: <pre><code>call    IjwLib!Bar (1000112b)\n</code></pre> At that address, it places a jump indirection: <pre><code>jmp     dword ptr [IjwLib!_mep?Bar$$FYAXXZ (10010008)]\n</code></pre> Where 10010008 matches a <code>.vtfixup</code> entry that looks like: <pre><code>.vtfixup [1] int32 retainappdomain at D_00010008 // 06000002 (Bar's token)\n</code></pre> According to ECMA 335, <code>vtfixups</code> can contain multiple entries. However, the Microsoft Visual C++ compiler (MSVC) does not appear to generate those. Vtfixups also contain flags for whether a call should go to the current thread's appdomain and whether the caller is unmanaged code. MSVC appears to always set those.</p>"},{"location":"mixed-mode/#starting-the-runtime","title":"Starting the Runtime","text":"<p>While a mixed mode assembly may be loaded into an already-running CLR, that isn't always the case. It's also possible for a mixed mode executable to start a process or for a running native process to load a mixed mode library and call into it. On .NET Framework (the only implementation that currently has this functionality), the native code's <code>Main</code> or <code>DllMain</code> calls into mscoree.dll's <code>_CorDllMain</code> function (which is resolved from a well-known location). When that happens, <code>_CorDllMain</code> is responsible for both starting the runtime and filling in vtfixups as described above.</p>"},{"location":"porting-ryujit/","title":"RyuJIT: Porting to different platforms","text":"<p>First, understand the JIT architecture by reading RyuJIT overview.</p>"},{"location":"porting-ryujit/#what-is-a-platform","title":"What is a Platform?","text":"<ul> <li>Target instruction set</li> <li>Target pointer size</li> <li>Target operating system</li> <li>Target calling convention and ABI (Application Binary Interface)</li> <li>Runtime data structures (not really covered here)</li> <li>GC encoding</li> <li>All targets use the same GC encoding scheme and APIs, except for Windows x86, which uses JIT32_GCENCODER.</li> <li>Debug information (mostly the same for all targets)</li> <li>EH (exception handling) information (not really covered here)</li> </ul> <p>One advantage of the CLR is that the VM (mostly) hides the (non-ABI) OS differences.</p>"},{"location":"porting-ryujit/#the-very-high-level-view","title":"The Very High Level View","text":"<p>The following components need to be updated, or target-specific versions created, for a new platform.</p> <ul> <li>The basics</li> <li>target.h</li> <li>Instruction set architecture:</li> <li>registerXXX.h - defines all registers used by the architecture and any aliases for them</li> <li>emitXXX.h - defines signatures for public instruction emission methods (e.g. \"emit an instruction which takes a single integer argument\") and private architecture-specific helpers</li> <li>emitXXX.cpp - implementation for emitXXX.h</li> <li>emitfmtXXX.h - optionally defines validity rules for how instructions should be formatted (e.g. RISC-V has no rules defined)</li> <li>instrsXXX.h - defines per-architecture instructions in assembly</li> <li>targetXXX.h - defines architectural constraints used elsewhere, such as \"bitmask for all integer registers where callee is saved\" or \"size in bytes of a floating point register\"</li> <li>targetXXX.cpp - implements ABI classifier for this architecture</li> <li>lowerXXX.cpp - implements Lowering for this architecture</li> <li>lsraXXX.cpp - implements register requirement setting based on GenTree Nodes</li> <li>codegenXXX.cpp - implements main codegen for this architecture (i.e. generating per-architecture instructions based on GenTree Nodes)</li> <li>hwintrinsic*XXX.* and simdashwintrinsic*XXX.h - defines and implements hardware intrinsic features, e.g. vector instructions</li> <li>unwindXXX.cpp - implements public unwinding API and unwind info dumping for debug use</li> <li>Calling Convention and ABI: all over the place</li> <li>32 vs. 64 bits</li> <li>Also all over the place. Some pointer size-specific data is centralized in target.h, but probably not 100%.</li> </ul>"},{"location":"porting-ryujit/#porting-stages-and-steps","title":"Porting stages and steps","text":"<p>There are several steps to follow to port the JIT (some of which can be be done in parallel), described below.</p>"},{"location":"porting-ryujit/#initial-bring-up","title":"Initial bring-up","text":"<ul> <li>Create the new platform-specific files</li> <li>Create the platform-specific build instructions (in CMakeLists.txt). This probably will require   new platform-specific build instructions at the root level, as well as the JIT level of the source tree.</li> <li>Focus on MinOpts; disable the optimization phases, or always test with <code>DOTNET_JITMinOpts=1</code>.</li> <li>Disable optional features, such as:</li> <li><code>FEATURE_EH</code> -- if 0, all exception handling blocks are removed. Of course, tests with exception handling     that depend on exceptions being thrown and caught won't run correctly.</li> <li><code>FEATURE_STRUCTPROMOTE</code></li> <li><code>FEATURE_FASTTAILCALL</code></li> <li><code>FEATURE_TAILCALL_OPT</code></li> <li><code>FEATURE_SIMD</code></li> <li>Build the new JIT as an altjit. In this mode, a \"base\" JIT is invoked to compile all functions except   the one(s) specified by the <code>DOTNET_AltJit</code> variable. For example, setting <code>DOTNET_AltJit=Add</code> and running   a test will use the \"base\" JIT (say, the Windows x64 targeting JIT) to compile all functions except <code>Add</code>, which will be first compiled by the new altjit, and if it fails, fall back to the \"base\" JIT. In this   way, only very limited JIT functionality need to work, as the \"base\" JIT takes care of most functions.</li> <li>Implement the basic instruction encodings. Test them using a method like <code>CodeGen::genArm64EmitterUnitTests()</code>.</li> <li>Implement the bare minimum to get the compiler building and generating code for very simple operations, like addition.</li> <li>Focus on the CodeGenBringUpTests (src\\tests\\JIT\\CodeGenBringUpTests), starting with the simple ones.</li> <li>These are designed such that for a test <code>XXX.cs</code>, there is a single interesting function named <code>XXX</code> to compile     (that is, the name of the source file is the same as the name of the interesting function. This was done to make     the scripts to invoke these tests very simple.). Set <code>DOTNET_AltJit=XXX</code> so the new JIT only attempts to     compile that one function.</li> <li>Merged test groups interfere with the simplicity of these tests by removing the entry point from each individual     test and creating a single wrapper that calls all of the tests in a single process. To restore the     old behavior, build the tests with the environment variable <code>BuildAsStandalone</code> set to <code>true</code>.</li> <li>Use <code>DOTNET_JitDisasm</code> to see the generated code for functions, even if the code isn't run.</li> </ul>"},{"location":"porting-ryujit/#expand-test-coverage","title":"Expand test coverage","text":"<ul> <li>Get more and more tests to run successfully:</li> <li>Run more of the <code>JIT</code> directory of tests</li> <li>Run all of the Pri-0 \"innerloop\" tests</li> <li>Run all of the Pri-1 \"outerloop\" tests</li> <li>It is helpful to collect data on asserts generated by the JIT across the entire test base, and fix the asserts in   order of frequency. That is, fix the most frequently occurring asserts first.</li> <li>Track the number of asserts, and number of tests with/without asserts, to help determine progress.</li> </ul>"},{"location":"porting-ryujit/#bring-the-optimizer-phases-on-line","title":"Bring the optimizer phases on-line","text":"<ul> <li>Run tests with and without <code>DOTNET_JITMinOpts=1</code>.</li> <li>It probably makes sense to set <code>DOTNET_TieredCompilation=0</code> (or disable it for the platform entirely) until much later.</li> </ul>"},{"location":"porting-ryujit/#improve-quality","title":"Improve quality","text":"<ul> <li>When the tests pass with the basic modes, start running with <code>JitStress</code> and <code>JitStressRegs</code> stress modes.</li> <li>Bring <code>GCStress</code> on-line. This also requires VM work.</li> <li>Work on <code>DOTNET_GCStress=4</code> quality. When crossgen/ngen is brought on-line, test with <code>DOTNET_GCStress=8</code>   and <code>DOTNET_GCStress=C</code> as well.</li> </ul>"},{"location":"porting-ryujit/#work-on-performance","title":"Work on performance","text":"<ul> <li>Determine a strategy for measuring and improving performance, both throughput (compile time) and generated code   quality (CQ).</li> </ul>"},{"location":"porting-ryujit/#work-on-platform-parity","title":"Work on platform parity","text":"<ul> <li>Implement features that were intentionally disabled, or for which implementation was delayed.</li> <li>Implement SIMD (<code>Vector&lt;T&gt;</code>) and hardware intrinsics support.</li> </ul>"},{"location":"porting-ryujit/#front-end-changes","title":"Front-end changes","text":"<ul> <li>Calling Convention</li> <li>Struct args and returns seem to be the most complex differences<ul> <li>Importer and morph are highly aware of these</li> <li>E.g. <code>fgMorphArgs()</code>, <code>fgFixupStructReturn()</code>, <code>fgMorphCall()</code>, <code>fgPromoteStructs()</code> and the various struct assignment morphing methods</li> </ul> </li> <li>HFAs on ARM</li> <li>Tail calls are target-dependent, but probably should be less so</li> <li>Intrinsics: each platform recognizes different methods as intrinsics (e.g. <code>Sin</code> only for x86, <code>Round</code> everywhere BUT amd64)</li> <li>Target-specific morphs such as for mul, mod and div</li> </ul>"},{"location":"porting-ryujit/#backend-changes","title":"Backend Changes","text":"<ul> <li>Lowering: fully expose control flow and register requirements</li> <li>Code Generation: traverse blocks in layout order, generating code (InstrDescs) based on register assignments on nodes</li> <li>Then, generate prolog &amp; epilog, as well as GC, EH and scope tables</li> <li>ABI changes:</li> <li>Calling convention register requirements<ul> <li>Lowering of calls and returns</li> <li>Code sequences for prologs &amp; epilogs</li> </ul> </li> <li>Allocation &amp; layout of frame</li> </ul>"},{"location":"porting-ryujit/#target-isa-configuration","title":"Target ISA \"Configuration\"","text":"<ul> <li>Conditional compilation (set in jit.h, based on incoming define, e.g. #ifdef X86) <pre><code>_TARGET_64_BIT_ (32 bit target is just ! _TARGET_64BIT_)\n_TARGET_XARCH_, _TARGET_ARMARCH_\n_TARGET_AMD64_, _TARGET_X86_, _TARGET_ARM64_, _TARGET_ARM_\n</code></pre></li> <li>Target.h</li> <li>InstrsXXX.h</li> </ul>"},{"location":"porting-ryujit/#instruction-encoding","title":"Instruction Encoding","text":"<ul> <li>The <code>insGroup</code> and <code>instrDesc</code> data structures are used for encoding</li> <li><code>instrDesc</code> is initialized with the opcode bits, and has fields for immediates and register numbers.</li> <li><code>instrDesc</code>s are collected into <code>insGroup</code> groups</li> <li>A label may only occur at the beginning of a group</li> <li>The emitter is called to:</li> <li>Create new instructions (<code>instrDesc</code>s), during CodeGen</li> <li>Emit the bits from the <code>instrDesc</code>s after CodeGen is complete</li> <li>Update Gcinfo (live GC vars &amp; safe points)</li> </ul>"},{"location":"porting-ryujit/#adding-encodings","title":"Adding Encodings","text":"<ul> <li>The instruction encodings are captured in instrsXXX.h. These are the opcode bits for each instruction</li> <li>The structure of each instruction set's encoding is target-dependent</li> <li>An \"instruction\" is just the representation of the opcode</li> <li>An instance of <code>instrDesc</code> represents the instruction to be emitted</li> <li>For each \"type\" of instruction, emit methods need to be implemented. These follow a pattern but a target may have unique ones, e.g. <pre><code>emitter::emitInsMov(instruction ins, emitAttr attr, GenTree* node)\nemitter::emitIns_R_I(instruction ins, emitAttr attr, regNumber reg, ssize_t val)\nemitter::emitInsTernary(instruction ins, emitAttr attr, GenTree* dst, GenTree* src1, GenTree* src2) (currently Arm64 only)\n</code></pre></li> </ul>"},{"location":"porting-ryujit/#lowering","title":"Lowering","text":"<ul> <li>Lowering ensures that all register requirements are exposed for the register allocator</li> <li>Use count, def count, \"internal\" reg count, and any special register requirements</li> <li>Does half the work of code generation, since all computation is made explicit<ul> <li>But it is NOT necessarily a 1:1 mapping from lowered tree nodes to target instructions</li> </ul> </li> <li>Its first pass does a tree walk, transforming the instructions. Some of this is target-independent. Notable exceptions:<ul> <li>Calls and arguments</li> <li>Switch lowering</li> <li>LEA transformation</li> </ul> </li> <li>Its second pass walks the nodes in execution order<ul> <li>Sets register requirements</li> <li>sometimes changes the register requirements children (which have already been traversed)</li> <li>Sets the block order and node locations for LSRA</li> <li><code>LinearScan::startBlockSequence()</code> and <code>LinearScan::moveToNextBlock()</code></li> </ul> </li> </ul>"},{"location":"porting-ryujit/#register-allocation","title":"Register Allocation","text":"<ul> <li>Register allocation is largely target-independent</li> <li>The second phase of Lowering does nearly all the target-dependent work</li> <li>Register candidates are determined in the front-end</li> <li>Local variables or temps, or fields of local variables or temps</li> <li>Not address-taken, plus a few other restrictions</li> <li>Sorted by <code>lvaSortByRefCount()</code>, and determined by <code>lvIsRegCandidate()</code></li> </ul>"},{"location":"porting-ryujit/#addressing-modes","title":"Addressing Modes","text":"<ul> <li>The code to find and capture addressing modes is particularly poorly abstracted</li> <li><code>genCreateAddrMode()</code>, in CodeGenCommon.cpp traverses the tree looking for an addressing mode, then captures its constituent elements (base, index, scale &amp; offset) in \"out parameters\"</li> <li>It never generates code, and is only used by <code>gtSetEvalOrder</code>, and by Lowering</li> </ul>"},{"location":"porting-ryujit/#code-generation","title":"Code Generation","text":"<ul> <li>For the most part, the code generation method structure is the same for all architectures</li> <li>Most code generation methods start with \"gen\"</li> <li>Theoretically, CodeGenCommon.cpp contains code \"mostly\" common to all targets (this factoring is imperfect)</li> <li>Method prolog, epilog,</li> <li><code>genCodeForBBList()</code></li> <li>Walks the trees in execution order, calling <code>genCodeForTreeNode()</code>, which needs to handle all nodes that are not \"contained\"</li> <li>Generates control flow code (branches, EH) for the block</li> </ul>"},{"location":"profilability/","title":"Implementing Profilability","text":"<p>This document describes technical details of adding profilability to a CLR feature.  This is targeted toward devs who are modifying the profiling API so their feature can be profilable.</p>"},{"location":"profilability/#philosophy","title":"Philosophy","text":""},{"location":"profilability/#contracts","title":"Contracts","text":"<p>Before delving into the details on which contracts should be used in the profiling API, it's useful to understand the overall philosophy.</p> <p>A philosophy behind the default contracts movement throughout the CLR (outside of the profiling API) is to encourage the majority of the CLR to be prepared to deal with \"aggressive behavior\" like throwing or triggering.  Below you'll see that this goes hand-in-hand with the recommendations for the callback (ICorProfilerCallback) contracts, which generally prefer the more permissive (\"aggressive\") of the contract choices.  This gives the profiler the most flexibility in what it can do during its callback (in terms of which CLR calls it can make via ICorProfilerInfo).</p> <p>However, the Info functions (ICorProfilerInfo) below are just the opposite: they're preferred to be restrictive rather than permissive.  Why?  Because we want these to be safe for the profiler to call from as many places as possible, even from those callbacks that are more restrictive than we might like (e.g., callbacks that for some reason must be GC_NOTRIGGER).</p> <p>Also, the preference for more restrictive contracts in ICorProfilerInfo doesn't contradict the overall CLR default contract philosophy, because it is expected that there will be a small minority of CLR functions that need to be restrictive.  ICorProfilerInfo is the root of call paths that fall into this category.  Since the profiler may be calling into the CLR at delicate times, we want these calls to be as unobtrusive as possible.  These are not considered mainstream functions in the CLR, but are a small minority of special call paths that need to be careful.</p> <p>So the general guidance is to use default contracts throughout the CLR where possible.  But when you need to blaze a path of calls originating from a profiler (i.e., from ICorProfilerInfo), that path will need to have its contracts explicitly specified, and be more restrictive than the default.</p>"},{"location":"profilability/#performance-or-ease-of-use","title":"Performance or ease of use?","text":"<p>Both would be nice.  But if you need to make a trade-off, favor performance.  The profiling API is meant to be a light-weight, thin, in-process layer between the CLR and a profiling DLL.  Profiler writers are few and far between, and are mostly quite sophisticated developers.  Simple validation of inputs by the CLR is expected.  But we only go so far.  For example, consider all the profiler IDs.  They're just casted pointers of C++ EE object instances that are called into directly (AppDomain*, MethodTable*, etc.).  A Profiler provides a bogus ID?  The CLR AVs!  This is expected.  The CLR does not hash IDs, in order to validate a lookup . Profilers are assumed to know what they are doing.</p> <p>That said, I'll repeat: simple validation of inputs by the CLR is expected.  Things like checking for NULL pointers, that classes requested for inspection have been initialized, \"parallel parameters\" are consistent (e.g., an array pointer parameter must be non-null if its size parameter is nonzero), etc.</p>"},{"location":"profilability/#icorprofilercallback","title":"ICorProfilerCallback","text":"<p>This interface comprises the callbacks made by the CLR into the profiler to notify the profiler of interesting events.  Each callback is wrapped in a thin method in the EE that handles locating the profiler's implementation of ICorProfilerCallback(2), and calling its corresponding method.</p> <p>Profilers subscribe to events by specifying the corresponding flag in a call to ICorProfilerInfo::SetEventMask()/ICorProfilerInfo::SetEventMask2().  The profiling API stores these choices and exposes them to the CLR through specialized inline functions (CORProfiler*) that mask against the bit corresponding to the flag.   Then, sprinkled throughout the CLR, you'll see code that calls the ICorProfilerCallback wrapper to notify the profiler of events as they happen, but this call is conditional on the flag being set (determined by calling the specialized inline function):</p> <pre><code>{\n    // check if profiler set flag\n    BEGIN_PROFILER_CALLBACK(CORProfilerTrackModuleLoads());\n\n    // call the ProfControlBlock wrapper around the profiler's callback implementation\n    // which pins the profiler in DoOneProfilerIteration via EvacuationCounterHolder\n    (&amp;g_profControlBlock)-&gt;ModuleLoadStarted((ModuleID) this);\n    // unpins the profiler after completing the callback\n\n    END_PROFILER_CALLBACK();\n}\n</code></pre> <p>To be clear, the code above is what you'll see sprinkled throughout the code base.  The function it calls (in this case ModuleLoadStarted()) is our wrapper around the profiler's callback implementation (in this case ICorProfilerCallback::ModuleLoadStarted()).  All of our wrappers appear in a single file (vm\\EEToProfInterfaceImpl.cpp), and the guidance provided in the sections below relate to those wrappers; not to the above sample code that calls the wrappers.</p> <p>The macro BEGIN_PROFILER_CALLBACK evaluates the expression passed as its argument.  If the expression is TRUE, the code between the BEGIN_PROFILER_CALLBACK and END_PROFILER_CALLBACK macros is executed, and the profiler is pinned into memory (meaning the profiler will not be able to detach from the process) through the ProfControlBlock wrapper.  If the expression is FALSE, all code between the BEGIN_PROFILER_CALLBACK and END_PROFILER_CALLBACK macros is skipped.  For more information about the BEGIN_PROFILER_CALLBACK and END_PROFILER_CALLBACK macros, find their definition in the code base and read the comments there.</p>"},{"location":"profilability/#contracts_1","title":"Contracts","text":"<p>Each and every callback wrapper must have some common gunk at the top.  Here's an example:</p> <pre><code>CONTRACTL\n{\n    // Yay!\n    NOTHROW;\n\n    // Yay!\n    GC_TRIGGERS;\n\n    // Yay!\n    MODE_PREEMPTIVE;\n\n    // Yay!\n    CAN_TAKE_LOCK;\n}\nCONTRACTL_END;\nCLR_TO_PROFILER_ENTRYPOINT((LF_CORPROF,\n                        LL_INFO10,\n                        \"**PROF: useful logging text here.\\n\"));\n</code></pre> <p>Important points:</p> <ul> <li>You must explicitly specify a value for the throws, triggers, mode, take_lock, and ASSERT_NO_EE_LOCKS_HELD() (latter required on callbacks only). This allows us to keep our documentation for profiler-writers accurate.</li> <li>Each contract must have its own comment (see below for specific details on contracts)</li> </ul> <p>There's a \"preferred\" value for each contract type.  If possible, use that and comment it with \"Yay!\" so that others who copy / paste your code elsewhere will know what's best.  If it's not possible to use the preferred value, comment why.</p> <p>Here are the preferred values for callbacks.</p> Preferred Why Details NOTHROW Allows callback to be issued from any CLR context.  Since Infos should be NOTHROW as well, this shouldn't be a hardship for the profiler. Note that you will get throws violations if the profiler calls a THROWS Info function from here, even though the profiler encloses the call in a try/catch (because our contract system can't see the profiler's try/catch).  So you'll need to insert a CONTRACT_VIOLATION(ThrowsViolation) scoped just before the call into the profiler. GC_TRIGGERS Gives profiler the most flexibility in the Infos it can call. If the callback is made at a delicate time where protecting all the object refs would be error-prone or significantly degrade performance, use GC_NOTRIGGER (and comment of course!). MODE_PREEMPTIVE if possible, otherwise MODE_COOPERATIVE MODE_PREEMPTIVE gives profiler the most flexibility in the Infos it can call (except when coop is necessary due to ObjectIDs).  Also, MODE_PREEMPTIVE is a preferred \"default\" contract throughout the EE, and forcing callbacks to be in preemptive encourages use of preemptive elsewhere in the EE. MODE_COOPERATIVE is fair if you're passing ObjectID parameters to the profiler.  Otherwise, specify MODE_PREEMPTIVE.  The caller of the callback should hopefully already be in preemptive mode anyway.  If not, rethink why not and potentially change the caller to be in preemptive.  Otherwise, you will need to use a GCX_PREEMP() macro before calling the callback. CAN_TAKE_LOCK Gives profiler the most flexibility in the Infos it can call Nothing further, your honor. ASSERT_NO_EE_LOCKS_HELD() Gives profiler even more flexibility on Infos it can call, as it ensures no Info could try to retake a lock or take an out-of-order lock (since no lock is taken to \"retake\" or destroy ordering) This isn't actually a contract, though the contract block is a convenient place to put this, so you don't forget.  As with the contracts, if this cannot be specified, comment why. <p>Note: EE_THREAD_NOT_REQUIRED / EE_THREAD_REQUIRED need not be specified for callbacks.  GC callbacks cannot specify \"REQUIRED\" anyway (no EE Thread might be present), and it is only interesting to consider these on the Info functions (profiler \u2192 CLR).</p>"},{"location":"profilability/#entrypoint-macros","title":"Entrypoint macros","text":"<p>As in the example above, after the contracts there should be an entrypoint macro.  This takes care of logging, marking on the EE Thread object that we're in a callback, removing stack guard, and doing some asserts.  There are a few variants of the macro you can use:</p> <pre><code>CLR_TO_PROFILER_ENTRYPOINT\n</code></pre> <p>This is the preferred and typically-used macro.</p> <p>Other macro choices may be used but you must comment why the above (preferred) macro cannot be used.</p> <pre><code>*_FOR_THREAD_*\n</code></pre> <p>These macros are used for ICorProfilerCallback methods that specify a ThreadID parameter whose value may not always be the current ThreadID.  You must specify the ThreadID as the first parameter to these macros.  The macro will then use your ThreadID rather than GetThread(), to assert that the callback is currently allowed for that ThreadID (i.e., that we have not yet issued a ThreadDestroyed() for that ThreadID).</p>"},{"location":"profilability/#icorprofilerinfo","title":"ICorProfilerInfo","text":"<p>This interface comprises the entrypoints used by the profiler to call into the CLR.</p>"},{"location":"profilability/#synchronous-asynchronous","title":"Synchronous / Asynchronous","text":"<p>Each Info call is classified as either synchronous or asynchronous.  Synchronous functions must be called from within a callback, whereas asynchronous functions are safe to be called at any time.</p>"},{"location":"profilability/#synchronous","title":"Synchronous","text":"<p>The vast majority of Info calls are synchronous: They can only be called by a profiler while it is executing inside a Callback.  In other words, an ICorProfilerCallback must be on the stack for it to be legal to call a synchronous Info function.  This is tracked by a bit on the EE Thread object.  When a Callback is made, we set the bit.  When the callback returns, we reset the bit.  When a synchronous Info function is called, we test the bit\u2014if it's not set, disallow the call.</p>"},{"location":"profilability/#threads-without-an-ee-thread","title":"Threads without an EE Thread","text":"<p>Because the above bit is tracked using the EE Thread object, only Info calls made on threads containing an EE Thread object have their \"synchronous-ness\" enforced.  Any Info call made on a non-EE Thread thread is immediately considered legal.  This is generally fine, as it's mainly the EE Thread threads that build up complex contexts that would be problematic to reenter.  Also, it's ultimately the profiler's responsibility to ensure correctness.  As described above, for performance reasons, the profiling API historically keeps its correctness checks down to a bare minimum, so as not to increase the weight.  Typically, Info calls made by a profiler on a non-EE Thread fall into these categories:</p> <ul> <li>An Info call made during a GC callback on a thread doing a server.</li> <li>An Info call made on a thread of the profiler's creation, such as a sampling thread (which therefore would have no CLR code on the stack).</li> </ul>"},{"location":"profilability/#enter-leave-hooks","title":"Enter / leave hooks","text":"<p>If a profiler requests enter / leave hooks and uses the fast path (i.e., direct function calls from the jitted code to the profiler with no intervening profiling API code), then any call to an Info function from within its enter / leave hooks will be considered asynchronous.  Again, this is for pragmatic reasons.  If profiling API code doesn't get a chance to run (for performance), then we have no opportunity to set the EE Thread bit stating that we're executing inside a callback.  This means a profiler is restricted to calling only asynchronous-safe Info functions from within its enter / leave hook.  This is typically acceptable, as a profiler concerned enough with perf that it requires direct function calls for enter / leave will probably not be calling any Info functions from within its enter / leave hooks anyway.</p> <p>The alternative is for the profiler to set a flag specifying that it wants argument or return value information, which forces an intervening profiling API C function to be called to prepare the information for the profiler's Enter / Leave hooks.  When such a flag is set, the profiling API sets the EE Thread bit from inside this C function that prepares the argument / return value information from the profiler.  This enables the profiler to call synchronous Info functions from within its Enter / Leave hook.</p>"},{"location":"profilability/#asynchronous","title":"Asynchronous","text":"<p>Asynchronous Info functions are those that are safe to be called anytime (from a callback or not).  There are relatively few asynchronous Info functions.  They are what a hijacking sampling profiler (e.g., Visual Studio profiler) might want to call from within one of its samples.  It is critical that an Info function labeled as asynchronous be able to execute from any possible call stack.  A thread could be interrupted while holding any number of locks (spin locks, thread store lock, OS heap lock, etc.), and then forced by the profiler to reenter the runtime via an asynchronous Info function.  This can easily cause deadlock or data corruption.  There are two ways an asynchronous Info function can ensure its own safety:</p> <ul> <li>Be very, very simple. Don't take locks, don't trigger a GC, don't access data that could be inconsistent, etc. OR</li> <li>If you need to be more complex than that, have sufficient checks at the top to ensure locks, data structures, etc., are in a safe state before proceeding.<ul> <li>Often, this includes asking whether the current thread is currently inside a forbid suspend thread region, and bailing with an error if it is, though this is not a sufficient check in all cases.</li> <li>DoStackSnapshot is an example of a complex asynchronous function. It uses a combination of checks (including asking whether the current thread is currently inside a forbid suspend thread region) to determine whether to proceed or bail.</li> </ul> </li> </ul>"},{"location":"profilability/#contracts_2","title":"Contracts","text":"<p>Each and every Info function must have some common gunk at the top.  Here's an example:</p> <pre><code>CONTRACTL\n{\n    // Yay!\n    NOTHROW;\n\n    // Yay!\n    GC_NOTRIGGER;\n\n    // Yay!\n    MODE_ANY;\n\n    // Yay!\n    EE_THREAD_NOT_REQUIRED;\n\n    // Yay!\n    CANNOT_TAKE_LOCK;\n}\nCONTRACTL_END;\nPROFILER_TO_CLR_ENTRYPOINT_SYNC((LF_CORPROF,\n                                 LL_INFO1000,\n                                 \"**PROF: EnumModuleFrozenObjects 0x%p.\\n\",\n                                 moduleID));\n</code></pre> <p>Here are the \"preferred\" values for each contract type.  Note these are mostly different from the preferred values for Callbacks!  If that confuses you, reread section 2.</p> Preferred Why Details NOTHROW Makes it easier for profiler to call; profiler doesn't need its own try / catch. If your callees are NOTHROW then use NOTHROW.  Otherwise, it's actually better to mark yourself as THROWS than to set up your own try / catch.  The profiler can probably do this more efficiently by sharing a try block among multiple Info calls. GC_NOTRIGGER Safer for profiler to call from more situations Go out of your way not to trigger.  If an Info function might trigger (e.g., loading a type if it's not already loaded), ensure there's a way, if possible, for the profiler to specify not to take the trigger path (e.g., fAllowLoad parameter that can be set to FALSE), and contract that conditionally. MODE_ANY Safer for profiler to call from more situations MODE_COOPERATIVE is fair if your parameters or returns are ObjectIDs.  Otherwise, MODE_ANY is strongly preferred. CANNOT_TAKE_LOCK Safer for profiler to call from more situations Ensure your callees don't lock.  If they must, comment exactly what locks are taken. Optional:EE_THREAD_NOT_REQUIRED Allows profiler to use this Info fcn from GC callbacks and from profiler-spun threads (e.g., sampling thread). These contracts are not yet enforced, so it's fine to just leave it blank.  If you're pretty sure your Info function doesn't need (or call anyone who needs) a current EE Thread, you can specify EE_THREAD_NOT_REQUIRED as a hint for later when the thread contracts are enforced. <p>Here's an example of commented contracts in a function that's not as \"yay\" as the one above:</p> <pre><code>CONTRACTL\n{\n    // ModuleILHeap::CreateNew throws\n    THROWS;\n\n    // AppDomainIterator::Next calls AppDomain::Release which can destroy AppDomain, and\n    // ~AppDomain triggers, according to its contract.\n    GC_TRIGGERS;\n\n    // Need cooperative mode, otherwise objectId can become invalid\n    if (GetThreadNULLOk() != NULL) { MODE_COOPERATIVE;  }\n\n    // Yay!\n    EE_THREAD_NOT_REQUIRED;\n\n    // Generics::GetExactInstantiationsFromCallInformation eventually\n    // reads metadata which causes us to take a reader lock.\n    CAN_TAKE_LOCK;\n}\nCONTRACTL_END;\n</code></pre>"},{"location":"profilability/#entrypoint-macros_1","title":"Entrypoint macros","text":"<p>After the contracts, there should be an entrypoint macro.  This takes care of logging and, in the case of a synchronous function, consulting callback state flags to enforce it's really called synchronously.  Use one of these, depending on whether the Info function is synchronous, asynchronous, or callable only from within the Initialize callback:</p> <ul> <li>PROFILER_TO_CLR_ENTRYPOINT_SYNC (typical choice)</li> <li>PROFILER_TO_CLR_ENTRYPOINT_ASYNC</li> <li>PROFILER_TO_CLR_ENTRYPOINT_CALLABLE_ON_INIT_ONLY</li> </ul> <p>As described above, asynchronous Info methods are rare and carry a higher burden.  The preferred contracts above are even \"more preferred\" if the method is asynchronous, and these 2 are outright required: GC_NOTRIGGER &amp; MODE_ANY.  CANNOT_TAKE_LOCK, while even more preferred in an async than sync function, is not always possible.  See Asynchronous section above for what to do.</p>"},{"location":"profilability/#files-youll-modify","title":"Files You'll Modify","text":"<p>It's pretty straightforward where to go, to add or modify methods, and code inspection is all you'll need to figure it out.  Here are the places you'll need to go.</p>"},{"location":"profilability/#corprofidl","title":"corprof.idl","text":"<p>All profiling API interfaces and types are defined in src\\inc\\corprof.idl. Go here first to define your types and methods.</p>"},{"location":"profilability/#eetoprofinterfaceimpl","title":"EEToProfInterfaceImpl.*","text":"<p>Wrapper around the profiler's implementation of ICorProfilerCallback is located at src\\vm\\EEToProfInterfaceImpl.*.</p>"},{"location":"profilability/#proftoeeinterfaceimpl","title":"ProfToEEInterfaceImpl.*","text":"<p>Implementation of ICorProfilerInfo is located at src\\vm\\ProfToEEInterfaceImpl.*.</p>"},{"location":"profiling/","title":"Profiling","text":"<p>Profiling, in this document, means monitoring the execution of a program which is executing on the Common Language Runtime (CLR).  This document details the interfaces, provided by the Runtime, to access such information.</p> <p>Although it is called the Profiling API, the functionality provided by it is suitable for use by more than just traditional profiling tools. Traditional profiling tools focus on measuring the execution of the program\u2014time spent in each function, or memory usage of the program over time. However, the profiling API is really targeted at a broader class of diagnostic tools, such as code-coverage utilities or even advanced debugging aids.</p> <p>The common thread among all of these uses is that they are all diagnostic in nature \u2014 the tool is written to monitor the execution of a program. The Profiling API should never be used by the program itself, and the correctness of the program's execution should not depend on (or be affected by) having a profiler active against it.</p> <p>Profiling a CLR program requires more support than profiling conventionally compiled machine code.  This is because the CLR has concepts such as application domains, garbage collection, managed exception handling and JIT compilation of code (converting Intermediate Language into native machine code), that the existing conventional profiling mechanisms are unable to identify and provide useful information. The Profiling API provides this missing information in an efficient way that causes minimal impact on the performance of the CLR and the profiled program.</p> <p>Note that JIT-compiling routines at runtime provide good opportunities, as the API allows a profiler to change the in-memory IL code stream for a routine, and then request that it be JIT-compiled anew.  In this way, the profiler can dynamically add instrumentation code to particular routines that need deeper investigation.  Although this approach is possible in conventional scenarios, it's much easier to do this for the CLR.</p>"},{"location":"profiling/#goals-for-the-profiling-api","title":"Goals for the Profiling API","text":"<ul> <li> <p>Expose information that existing profilers will require for a user to determine and analyze performance of a program run on the CLR. Specifically:</p> <ul> <li>Common Language Runtime startup and shutdown events</li> <li>Application domain creation and shutdown events</li> <li>Assembly loading and unloading events</li> <li>Module load/unload events</li> <li>Com VTable creation and destruction events</li> <li>JIT-compiles, and code pitching events</li> <li>Class load/unload events</li> <li>Thread birth/death/synchronization</li> <li>Function entry/exit events</li> <li>Exceptions</li> <li>Transitions between managed and unmanaged execution</li> <li>Transitions between different Runtime contexts</li> <li>Information about Runtime suspensions</li> <li>Information about the Runtime memory heap and garbage collection activity</li> </ul> </li> <li> <p>Callable from any (non-managed) COM-compatible language</p> </li> <li>Efficient, in terms of CPU and memory consumption - the act of profiling should not cause such a big change upon the program being profiled that the results are misleading</li> <li>Useful to both sampling and non-sampling profilers.  [A _sampling _profiler inspects the profilee at regular clock ticks - maybe 5 milliseconds apart, say.  A _non-sampling _profiler is informed of events, synchronously with the thread that causes them]</li> </ul>"},{"location":"profiling/#non-goals-for-the-profiling-api","title":"Non-goals for the Profiling API","text":"<ul> <li>The Profiling API does not support profiling unmanaged code. Existing mechanisms must instead be used to profile unmanaged code. The CLR profiling API works only for managed code. However, profiler provides managed/unmanaged transition events to determine the boundaries between managed and unmanaged code.</li> <li>The Profiling API does not support writing applications that will modify their own code, for purposes such as aspect-oriented programming.</li> <li>The Profiling API does not provide information needed to check bounds. The CLR provides intrinsic support for bounds checking of all managed code.</li> </ul> <p>The CLR code profiler interfaces do not support remote profiling due to the following reasons:</p> <ul> <li>It is necessary to minimize execution time using these interfaces so that profiling results will not be unduly affected. This is especially true where execution performance is being monitored. However, it is not a limitation when the interfaces are used to monitor memory usage or to obtain Runtime information on stack frames, objects, etc.</li> <li>The code profiler needs to register one or more callback interfaces with the Runtime on the local machine on which the application being profiled runs. This limits the ability to create a remote code profiler.</li> </ul>"},{"location":"profiling/#profiling-api-overview","title":"Profiling API \u2013 Overview","text":"<p>The profiling API within CLR allows the user to monitor the execution and memory usage of a running application.  Typically, this API will be used to write a code profiler package.  In the sections that follow, we will talk about a profiler as a package built to monitor execution of any managed application.</p> <p>The profiling API is used by a profiler DLL, loaded into the same process as the program being profiled. The profiler DLL implements a callback interface (ICorProfilerCallback2). The runtime calls methods on that interface to notify the profiler of events in the profiled process. The profiler can call back into the runtime with methods on ICorProfilerInfo to get information about the state of the profiled application.</p> <p>Note that only the data-gathering part of the profiler solution should be running in-process with the profiled application\u2014UI and data analysis should be done in a separate process.</p> <p></p> <p>The ICorProfilerCallback and ICorProfilerCallback2 interfaces consists of methods with names like ClassLoadStarted, ClassLoadFinished, JITCompilationStarted. Each time the CLR loads/unloads a class, compiles a function, etc., it calls the corresponding method in the profiler's ICorProfilerCallback/ICorProfilerCallback2 interface.  (And similarly for all of the other notifications; see later for details)</p> <p>So, for example, a profiler could measure code performance via the two notifications FunctionEnter and FunctionLeave.  It simply timestamps each notification, accumulates results, then outputs a list indicating which functions consumed the most cpu time, or most wall-clock time, during execution of the application.</p> <p>The ICorProfilerCallback/ICorProfilerCallback2 interface can be considered to be the \"notifications API\".</p> <p>The other interface involved for profiling is ICorProfilerInfo.  The profiler calls this, as required, to obtain more information to help its analysis.  For example, whenever the CLR calls FunctionEnter it supplies a value for the FunctionId.  The profiler can discover more information about that FunctionId by calling the ICorProfilerInfo::GetFunctionInfo to discover the function's parent class, its name, etc, etc.</p> <p>The picture so far describes what happens once the application and profiler are running.  But how are the two connected together when an application is started?  The CLR makes the connection during its initialization in each process.  It decides whether to connect to a profiler, and which profiler that should be, depending upon the value for two environment variables, checked one after the other:</p> <ul> <li>CORECLR_ENABLE_PROFILING - only connect with a profiler if this environment variable exists and is set to a non-zero value.</li> <li>CORECLR_PROFILER - connect with the profiler with this CLSID or ProgID (which must have been stored previously in the Registry). The CORECLR_PROFILER environment variable is defined as a string:<ul> <li>set CORECLR_PROFILER={32E2F4DA-1BEA-47ea-88F9-C5DAF691C94A}, or</li> <li>set CORECLR_PROFILER=\"MyProfiler\"</li> </ul> </li> <li>The profiler class is the one that implements ICorProfilerCallback/ICorProfilerCallback2. It is required that a profiler implement ICorProfilerCallback2; if it does not, it will not be loaded.</li> </ul> <p>When both checks above pass, the CLR creates an instance of the profiler in a similar fashion to CoCreateInstance.  The profiler is not loaded through a direct call to CoCreateInstance so that a call to CoInitialize may be avoided, which requires setting the threading model.  It then calls the ICorProfilerCallback::Initialize method in the profiler.  The signature of this method is:</p> <pre><code>HRESULT Initialize(IUnknown *pICorProfilerInfoUnk)\n</code></pre> <p>The profiler must QueryInterface pICorProfilerInfoUnk for an ICorProfilerInfo interface pointer and save it so that it can call for more info during later profiling.  It then calls ICorProfilerInfo::SetEventMask to say which categories of notifications it is interested in.  For example:</p> <pre><code>ICorProfilerInfo* pInfo;\n\npICorProfilerInfoUnk-&gt;QueryInterface(IID_ICorProfilerInfo, (void**)&amp;pInfo);\n\npInfo-&gt;SetEventMask(COR_PRF_MONITOR_ENTERLEAVE | COR_PRF_MONITOR_GC)\n</code></pre> <p>This mask would be used for a profiler interested only in function enter/leave notifications and garbage collection notifications.  The profiler then simply returns, and is off and running!</p> <p>By setting the notifications mask in this way, the profiler can limit which notifications it receives.  This obviously helps the user build a simpler, or special-purpose profiler; it also reduces wasted cpu time in sending notifications that the profiler would simply 'drop on the floor'  (see later for details).</p> <p>TODO: This text is a bit confusing. It seems to be conflating the fact that you need to create a different 'environment' (as in environment variables) to specify a different profiler and the fact that only one profiler can attach to a process at once. It may also be conflating launch vs. attach scenarios. Is that right??</p> <p>Note that only one profiler can be profiling a process at one time in a given environment. In different environments it is possible to have two different profilers registered in each environment, each profiling separate processes.</p> <p>Certain profiler events are IMMUTABLE which means that once they are set in the ICorProfilerCallback::Initialize callback they cannot be turned off using ICorProfilerInfo::SetEventMask(). Trying to change an immutable event will result in SetEventMask returning a failed HRESULT.</p> <p>The profiler must be implemented as an inproc COM server \u2013 a DLL, which is mapped into the same address space as the process being profiled.  Any other type of COM server is not supported; if a profiler, for example, wants to monitor applications from a remote computer, it must implement 'collector agents' on each machine, which batch results and communicate them to the central data collection machine.</p>"},{"location":"profiling/#profiling-api-recurring-concepts","title":"Profiling API \u2013 Recurring Concepts","text":"<p>This brief section explains a few concepts that apply throughout the profiling API, rather than repeat them with the description of each method.</p>"},{"location":"profiling/#ids","title":"IDs","text":"<p>Runtime notifications supply an ID for reported classes, threads, AppDomains, etc.  These IDs can be used to query the Runtime for more info.  These IDs are simply the address of a block in memory that describes the item; however, they should be treated as opaque handles by any profiler. If an invalid ID is used in a call to any Profiling API function then the results are undefined.  Most likely, the result will be an access violation. The user has to ensure that the ID's used are perfectly valid. The profiling API does not perform any type of validation since that would create overhead and it would slow down the execution considerably.</p>"},{"location":"profiling/#uniqueness","title":"Uniqueness","text":"<p>A ProcessID is unique system-wide for the lifetime of the process. All other IDs are unique process-wide for the lifetime of the ID.</p>"},{"location":"profiling/#hierarchy-containment","title":"Hierarchy &amp; Containment","text":"<p>ID's are arranged in a hierarchy, mirroring the hierarchy in the process. Processes contain AppDomains contain Assemblies contain Modules contain Classes contain Functions. Threads are contained within Processes, and may move from AppDomain to AppDomain. Objects are mostly contained within AppDomains (a very few objects may be members of more than one AppDomain at a time). Contexts are contained within Processes.</p>"},{"location":"profiling/#lifetime-stability","title":"Lifetime &amp; Stability","text":"<p>When a given ID dies, all IDs contained within it die.</p> <p>ProcessID \u2013 Alive and stable from the call to Initialize until the return from Shutdown.</p> <p>AppDomainID \u2013 Alive and stable from the call to AppDomainCreationFinished until the return from AppDomainShutdownStarted.</p> <p>AssemblyID, ModuleID, ClassID \u2013 Alive and stable from the call to LoadFinished for the ID until the return from UnloadStarted for the ID.</p> <p>FunctionID \u2013 Alive and stable from the call to JITCompilationFinished or JITCachedFunctionSearchFinished until the death of the containing ClassID.</p> <p>ThreadID \u2013 Alive and stable from the call to ThreadCreated until the return from ThreadDestroyed.</p> <p>ObjectID \u2013 Alive beginning with the call to ObjectAllocated. Eligible to change or die with each garbage collection.</p> <p>GCHandleID \u2013 Alive from the call to HandleCreated until the return from HandleDestroyed.</p> <p>In addition, any ID returned from a profiling API function will be alive at the time it is returned.</p>"},{"location":"profiling/#app-domain-affinity","title":"App-Domain Affinity","text":"<p>There is an AppDomainID for each user-created app-domain in the process, plus the \"default\" domain, plus a special pseudo-domain used for holding domain-neutral assemblies.</p> <p>Assembly, Module, Class, Function, and GCHandleIDs have app-domain affinity, meaning that if an assembly is loaded into multiple app domains, it (and all of the modules, classes, and functions contained within it) will have a different ID in each, and operations upon each ID will take effect only in the associated app domain. Domain-neutral assemblies will appear in the special pseudo-domain mentioned above.</p>"},{"location":"profiling/#special-notes","title":"Special Notes","text":"<p>All IDs except ObjectID should be treated as opaque values. Most IDs are fairly self-explanatory. A few are worth explaining in more detail:</p> <p>ClassIDs represent classes. In the case of generic classes, they represent fully-instantiated types. List&lt;int&gt;, List&lt;char&gt;, List&lt;object&gt;, and List&lt;string&gt; each have their own ClassID. List&lt;T&gt; is an uninstantiated type, and has no ClassID. Dictionary&lt;string,V&gt; is a partially-instantiated type, and has no ClassID.</p> <p>FunctionIDs represent native code for a function. In the case of generic functions (or functions on generic classes), there may be multiple native code instantiations for a given function, and thus multiple FunctionIDs. Native code instantiations may be shared between different types \u2014 for example List&lt;string&gt; and List&lt;object&gt; share all code\u2014so a FunctionID may \"belong\" to more than one ClassID.</p> <p>ObjectIDs represent garbage-collected objects. An ObjectID is the current address of the object at the time the ObjectID is received by the profiler, and may change with each garbage collection. Thus, an ObjectID value is only valid between the time it is received and when the next garbage collection begins.  The CLR also supplies notifications that allow a profiler to update its internal maps that track objects, so that a profiler may maintain a valid ObjectID across garbage collections.</p> <p>GCHandleIDs represent entries in the GC's handle table. GCHandleIDs, unlike ObjectIDs, are opaque values. GC handles are created by the runtime itself in some situations, or can be created by user code using the System.Runtime.InteropServices.GCHandle structure. (Note that the GCHandle structure merely represents the handle; the handle does not \"live\" within the GCHandle struct.)</p> <p>ThreadIDs represent managed threads. If a host supports execution in fiber mode, a managed thread may exist on different OS threads, depending on when it is examined. ( NOTE: Profiling of fiber-mode applications is not supported.)</p>"},{"location":"profiling/#callback-return-values","title":"Callback Return Values","text":"<p>A profiler returns a status, as an HRESULT, for each notification triggered by the CLR. That status may have the value S_OK or E_FAIL.  Currently the Runtime ignores this status value in every callback except ObjectReferences.</p>"},{"location":"profiling/#caller-allocated-buffers","title":"Caller-Allocated Buffers","text":"<p>ICorProfilerInfo functions that take caller-allocated buffers typically conform to the following signature:</p> <pre><code>HRESULT GetBuffer( [in] /\\* Some query information \\*/,\n   [in] ULONG32 cBuffer,\n   [out] ULONG32 \\*pcBuffer,\n   [out, size\\_is(cBuffer), length\\_is(\\*pcMap)] /\\* TYPE \\*/ buffer[] );\n</code></pre> <p>These functions will always behave as follows:</p> <ul> <li>cBuffer is the number of elements allocated in the buffer.</li> <li>*pcBuffer will be set to the total number of elements available.</li> <li>buffer will be filled with as many elements as possible</li> </ul> <p>If any elements are returned, the return value will be S_OK. It is the caller's responsibility to check if the buffer was large enough.</p> <p>If buffer is NULL, cBuffer must be 0. The function will return S_OK and set *pcBuffer to the total number of elements available.</p>"},{"location":"profiling/#optional-out-parameters","title":"Optional Out Parameters","text":"<p>All [out] parameters on the API are optional, unless a function has only one [out] parameter. A profiler simply passes NULL for any [out] parameters it is not interested in. The profiler must also pass consistent values for any associated [in] parameters\u2014e.g., if the NULL [out] parameter is a buffer to be filled with data, the [in] parameter specifying its size must be 0.</p>"},{"location":"profiling/#notification-thread","title":"Notification Thread","text":"<p>In most cases, the notifications are executed by the same thread as generated the event.  Such notifications (for example, FunctionEnter and FunctionLeave_)_ don't need to supply the explicit ThreadID.  Also, the profiler might choose to use thread-local storage to store and update its analysis blocks, as compared with indexing into global storage, based off the ThreadID of the affected thread.</p> <p>Each notification documents which thread does the call \u2013 either the thread which generated the event or some utility thread (e.g. garbage collector) within the Runtime.  For any callback that might be invoked by a different thread, a user can call the ICorProfilerInfo::GetCurrentThreadID to discover the thread that generated the event.</p> <p>Note that these callbacks are not serialized. The profiler developer must write defensive code, by creating thread safe data structures and by locking the profiler code where necessary to prevent parallel access from multiple threads. Therefore, in certain cases it is possible to receive an unusual sequence of callbacks. For example assume a managed application is spawning two threads, which are executing identical code. In this case it is possible to receive a JITCompilationStarted event for some function from one thread and before receiving the respective JITCompilationFinished callback, the other thread has already sent a FunctionEnter callback. Therefore the user will receive a FunctionEnter callback for a function that it seems not fully JIT compiled yet!</p>"},{"location":"profiling/#gc-safe-callouts","title":"GC-Safe Callouts","text":"<p>When the CLR calls certain functions in the ICorProfilerCallback, the Runtime cannot perform a garbage collection until the Profiler returns control from that call.  This is because profiling services cannot always construct the stack into a state that is safe for a garbage collection; instead garbage collection is disabled around that callback.  For these cases, the Profiler should take care to return control as soon as possible.  The callbacks where this applies are:</p> <ul> <li>FunctionEnter, FunctionLeave, FunctionTailCall</li> <li>ExceptionOSHandlerEnter, ExceptionOSHandlerLeave</li> <li>ExceptionUnwindFunctionEnter, ExceptionUnwindFunctionLeave</li> <li>ExceptionUnwindFinallyEnter, ExceptionUnwindFinallyLeave</li> <li>ExceptionCatcherEnter, ExceptionCatcherLeave</li> <li>ExceptionCLRCatcherFound, ExceptionCLRCatcherExecute</li> <li>COMClassicVTableCreated, COMClassicVTableDestroyed</li> </ul> <p>In addition, the following callbacks may or may not allow the Profiler to block.  This is indicated, call-by-call, via the fIsSafeToBlockargument.  This set includes:</p> <ul> <li>JITCompilationStarted, JITCompilationFinished</li> </ul> <p>Note that if the Profiler does block, it will delay garbage collection.  This is harmless, as long as the Profiler code itself does not attempt to allocate space in the managed heap, which could induce deadlock.</p>"},{"location":"profiling/#using-com","title":"Using COM","text":"<p>Though the profiling API interfaces are defined as COM interfaces, the runtime does not actually initialize COM in order to use them. This is in order to avoid having to set the threading model via CoInitialize before the managed application has had a chance to specify its desired threading model. Similarly, the profiler itself should not call CoInitialize, since it may pick a threading model that is incompatible with the application being profiled and therefore break the app.</p>"},{"location":"profiling/#callbacks-and-stack-depth","title":"Callbacks and Stack Depth","text":"<p>Profiler callbacks may be issued in extremely stack-constrained circumstances, and a stack overflow within a profiler callback will lead to immediate process exit. A profiler should be careful to use as little stack as possible in response to callbacks. If the profiler is intended for use against processes that are robust against stack overflow, the profiler itself should also avoid triggering stack overflow.</p>"},{"location":"profiling/#how-to-profile-a-nt-service","title":"How to profile a NT Service","text":"<p>Profiling is enabled through environment variables, and since NT Services are started when the Operating System boots, those environment variables must be present and set to the required value at that time.  Thus, to profile an NT Service, the appropriate environment variables must be set in advance, system-wide, via:</p> <p>MyComputer -&gt; Properties -&gt; Advanced -&gt; EnvironmentVariables -&gt; System Variables</p> <p>Both CORECLR_ENABLE_PROFILING and CORECLR_PROFILER have to be set , and the user must ensure that the Profiler DLL is registered.  Then, the target machine should be re-booted so that the NT Services pick up those changes.  Note that this will enable profiling on a system-wide basis.  So, to prevent every managed application that is run subsequently from being profiled, the user should delete those system environment variables after the re-boot.</p>"},{"location":"profiling/#profiling-api-high-level-description","title":"Profiling API \u2013 High-Level Description","text":""},{"location":"profiling/#loader-callbacks","title":"Loader Callbacks","text":"<p>The loader callbacks are those issued for app domain, assembly, module, and class loading.</p> <p>One might expect that the CLR would notify an assembly load, followed by one or more module loads for that assembly.  However, what actually happens depends on any number of factors within the implementation of the loader. The profiler may depend on the following:</p> <ul> <li>A Started callback will be delivered before the Finished callback for the same ID.</li> <li>Started and Finished callbacks will be delivered on the same thread.</li> </ul> <p>Though the loader callbacks are arranged in Started/Finished pairs, they cannot be used to accurately attribute time to operations within the loader.</p>"},{"location":"profiling/#call-stacks","title":"Call stacks","text":"<p>The profiling API provides two ways of obtaining call stacks\u2014a snapshot method, suitable for sparse gathering of callstacks, and a shadow-stack method, suitable for tracking the callstack at every instant.</p>"},{"location":"profiling/#stack-snapshot","title":"Stack Snapshot","text":"<p>A stack snapshot is a trace of the stack of a thread at an instant in time. The profiling API provides support for tracing the managed functions on the stack, but leaves the tracing of unmanaged functions to the profiler's own stack walker.</p>"},{"location":"profiling/#shadow-stack","title":"Shadow Stack","text":"<p>Using the above snapshot method too frequently can quickly become a performance issue. When stack traces need to be taken often, profilers should instead build a \"shadow stack\" using the FunctionEnter, FunctionLeave, FunctionTailCall, and Exception* callbacks. The shadow stack is always current and can be quickly copied to storage whenever a stack snapshot is needed.</p> <p>A shadow stack may obtain function arguments, return values, and information about generic instantiations. This information is only available through the shadow stack, because it's readily available at function-enter time but may have been optimized away later in the run of the function.</p>"},{"location":"profiling/#garbage-collection","title":"Garbage Collection","text":"<p>When the profiler specifies the COR_PRF_MONITOR_GC flag, all the GC events will be triggered in the profiler except the ICorProfilerCallback::ObjectAllocated events. They are explicitly controlled by another flag (see next section), for performance reasons. Note that when the COR_PRF_MONITOR_GC is enabled, the Concurrent Garbage Collection is turned off.</p> <p>A profiler may use the GarbageCollectionStarted/Finished callbacks to identify that a GC is taking place, and which generations are covered.</p>"},{"location":"profiling/#tracking-moved-objects","title":"Tracking Moved Objects","text":"<p>Garbage collection reclaims the memory occupied by 'dead' objects and compacts that freed space.  As a result, live objects are moved within the heap.  The effect is that ObjectIDs handed out by previous notifications change their value (the internal state of the object itself does not change (other than its references to other objects), just its location in memory, and therefore its ObjectID).  The MovedReferences notification lets a profiler update its internal tables that are tracking info by ObjectID. Its name is somewhat misleading, as it is issued even for objects that were not moved.</p> <p>The number of objects in the heap can number thousands or millions.  With such large numbers, it's impractical to notify their movement by providing a before-and-after ID for each object.  However, the garbage collector tends to move contiguous runs of live objects as a 'bunch' \u2013 so they end up at new locations in the heap, but they are still contiguous.  This notification reports the \"before\" and \"after\" ObjectID of these contiguous runs of objects.  (see example below)</p> <p>In other words, if an ObjectID value lies within the range:</p> <pre><code>_oldObjectIDRangeStart[i] &lt;= ObjectID &lt; oldObjectIDRangeStart[i] + cObjectIDRangeLength[i]_\n\nfor _0 &lt;= i &lt; cMovedObjectIDRanges_, then the _ObjectID_ value has changed to\n\n_ObjectID - oldObjectIDRangeStart[i] + newObjectIDRangeStart[i]_\n</code></pre> <p>All of these callbacks are made while the Runtime is suspended, so none of the ObjectID values can change until the Runtime resumes and another GC occurs.</p> <p>Example: The diagram below shows 10 objects, before garbage collection.  They lie at start addresses (equivalent to ObjectIDs) of 08, 09, 10, 12, 13, 15, 16, 17, 18 and 19.  ObjectIDs 09, 13 and 19 are dead (shown shaded); their space will be reclaimed during garbage collection.</p> <p></p> <p>The \"After\" picture shows how the space occupied by dead objects has been reclaimed to hold live objects.  The live objects have been moved in the heap to the new locations shown.  As a result, their ObjectIDs all change.  The simplistic way to describe these changes is with a table of before-and-after ObjectIDs, like this:</p> oldObjectIDRangeStart[] newObjectIDRangeStart[] 0 08 07 1 09 2 10 08 3 12 10 3 13 4 15 11 5 16 12 6 17 13 7 18 14 8 19 <p>This works, but clearly, we can compact the information by specifying starts and sizes of contiguous runs, like this:</p> oldObjectIDRangeStart[] newObjectIDRangeStart[] cObjectIDRangeLength[] 0 08 07 1 1 10 08 3 2 15 11 4 <p>This corresponds to exactly how MovedReferences reports the information. Note that MovedReferencesCallback is reporting the new layout of the object BEFORE they actually get relocated in the heap. So the old ObjectIDs are still valid for calls to the ICorProfilerInfo interface (and the new ObjectIDs are not).</p>"},{"location":"profiling/#detecting-all-deleted-objects","title":"Detecting All Deleted Objects","text":"<p>MovedReferences will report all objects that survive a compacting GC, regardless of whether they move; anything not reported did not survive. However not all GC's are compacting.</p> <p>The profiler may call ICorProfilerInfo2::GetGenerationBounds to get the boundaries of the GC heap segments. The rangeLength field in the resulting COR_PRF_GC_GENERATION_RANGE structs can be used to figure out the extent of live objects in a compacted generation.</p> <p>The GarbageCollectionStarted callback indicates which generations are being collected by the current GC. All objects that are in a generation that is not being collected will survive the GC.</p> <p>For a non-compacting GC (a GC in which no objects get moved at all), the SurvivingReferences callback is delivered to indicate which objects survived the GC.</p> <p>Note that a single GC may be compacting for one generation and non-compacting for another. Any given generation will receive either SurvivingReferences callbacks or MovedReferences callbacks for a given GC, but not both.</p>"},{"location":"profiling/#remarks","title":"Remarks","text":"<p>The application is halted following a garbage collection until the Runtime is done passing information about the heap to the code profiler. The method ICorProfilerInfo::GetClassFromObject can be used to obtain the ClassID of the class of which the object is an instance. The method ICorProfilerInfo::GetTokenFromClass can be used to obtain metadata information about the class.</p> <p>RootReferences2 allows the profiler to identify objects held via special handles. The generation bounds information supplied by GetGenerationBounds combined with the collected-generation information supplied by GarbageCollectionStarted enable the profiler to identify objects that live in generations that were not collected.</p>"},{"location":"profiling/#object-inspection","title":"Object Inspection","text":"<p>The FunctionEnter2/Leave2 callbacks provide information about the arguments and return value of a function, as regions of memory. The arguments are stored left-to-right in the given memory regions. A profiler can use the metadata signature of the function to interpret the arguments, as follows:</p> ELEMENT_TYPE Representation Primitives (ELEMENT_TYPE &lt;= R8, I, U) Primitive values Value types (VALUETYPE) Depends on type Reference types (CLASS, STRING, OBJECT, ARRAY, GENERICINST, SZARRAY) ObjectID (pointer into GC heap) BYREF Managed pointer (NOT an ObjectID, but may be pointing to stack or GC heap) PTR Unmanaged pointer (not movable by GC) FNPTR Pointer-sized opaque value TYPEDBYREF Managed pointer, followed by a pointer-sized opaque value <p>The differences between an ObjectID and a managed pointer are:</p> <ul> <li>ObjectID's only point into the GC heap or frozen object heap. Managed pointers may point to the stack as well.</li> <li>ObjectID's always point to the beginning of an object. Managed pointers may point to one of its fields.</li> <li>Managed pointers cannot be passed to functions that expect an ObjectID</li> </ul>"},{"location":"profiling/#inspecting-complex-types","title":"Inspecting Complex Types","text":"<p>Inspecting reference types or non-primitive value types requires some advanced techniques.</p> <p>For value types and reference types other than strings or arrays, GetClassLayout provides the offset for each field. The profiler can then use the metadata to determine the type of the field and recursively evaluate it. (Note that GetClassLayout returns only the fields defined by the class itself; fields defined by the parent class are not included.)</p> <p>For boxed value types, GetBoxClassLayout provides the offset of the value type within the box. The layout of the value type itself does not change, so once the profiler has found the value type within the box, it can use GetClassLayout to understand its layout.</p> <p>For strings, GetStringClassLayout provides the offsets of interesting pieces of data in the string object.</p> <p>Arrays are somewhat special, in that to understand arrays a function must be called for every array object, rather than just for the type. (This is because there are too many formats of arrays to describe using offsets.) GetArrayObjectInfo is provided to do the interpretation.</p> <p>@TODO: Callbacks from which inspection is safe</p> <p>@TODO: Functions that are legal to call when threads are hard-suspended</p>"},{"location":"profiling/#inspecting-static-fields","title":"Inspecting Static Fields","text":"<p>GetThreadStaticAddress, GetAppDomainStaticAddress, GetContextStaticAddress, and GetRVAStaticAddress provide information about the location of static fields. Looking at the memory at that location, you interpret it as follows:</p> <ul> <li>Reference types: ObjectID</li> <li>Value types: ObjectID of box containing the actual value</li> <li>Primitive types: Primitive value</li> </ul> <p>There are four types of statics. The following table describes what they are and how to identify them.</p> Static Type Definition Identifying in Metadata AppDomain Your basic static field\u2014has a different value in each app domain. Static field with no attached custom attributes Thread Managed TLS\u2014a static field with a unique value for each thread and each app domain. Static field with System.ThreadStaticAttribute RVA Process-scoped static field with a home in the module's data section Static field with hasRVA flag Context Static field with a different value in each COM+ Context Static field with System.ContextStaticAttribute"},{"location":"profiling/#exceptions","title":"Exceptions","text":"<p>Notifications of exceptions are the most difficult of all notifications to describe and to understand.  This is because of the inherent complexity in exception processing.  The set of exception notifications described below was designed to provide all the information required for a sophisticated profiler \u2013 so that, at every instant, it can keep track of which pass (first or second), which frame, which filter and which finally block is being executed, for every thread in the profilee process. Note that the Exception notifications do not provide any threadID's but a profiler can always call ICorProfilerInfo::GetCurrentThreadID to discover which managed thread throws the exception.</p> <p></p> <p>The figure above displays how the code profiler receives the various callbacks, when monitoring exception events. Each thread starts out in \"Normal Execution.\" When the thread is in a state within the big gray box, the exception system has control of the thread\u2014any non-exception-related callbacks (e.g. ObjectAllocated) that occur while the thread is in one of these states may be attributed to the exception system itself. When the thread is in a state outside of the big gray box, it is running arbitrary managed code.</p>"},{"location":"profiling/#nested-exceptions","title":"Nested Exceptions","text":"<p>Threads that have transitioned into managed code in the midst of processing an exception could throw another exception, which would result in a whole new pass of exception handling (the \"New EH Pass\" boxes above). If such a \"nested\" exception escapes the filter/finally/catch from the original exception, it can affect the original exception:</p> <ul> <li>If the nested exception occurred within a filter, and escapes the filter, the filter will be considered to return \"false\" and the first pass will continue.</li> <li>If the nested exception occurred within a finally, and escapes the finally, the original exception's processing will never resume.</li> <li>If the nested exception occurred within a catch, and escapes the catch, the original exception's processing will never resume.</li> </ul>"},{"location":"profiling/#unmanaged-handlers","title":"Unmanaged Handlers","text":"<p>An exception might be handled in unmanaged code. In this case, the profiler will see the unwind phase, but no notification of any catch handlers. Execution will simply resume normally in the unmanaged code. An unmanaged-aware profiler will be able to detect this, but a managed-only profiler may see any number of things, including but not limited to:</p> <ul> <li>An UnmanagedToManagedTransition callback as the unmanaged code calls or returns to managed code.</li> <li>Thread termination (if the unmanaged code was at the root of the thread).</li> <li>App termination (if the unmanaged code terminates the app).</li> </ul>"},{"location":"profiling/#clr-handlers","title":"CLR Handlers","text":"<p>An exception might be handled by the CLR itself. In this case, the profiler will see the unwind phase, but no notification of any catch handlers. It may see execution resume normally in managed or unmanaged code.</p>"},{"location":"profiling/#unhandled-exceptions","title":"Unhandled Exceptions","text":"<p>By default, an unhandled exception will lead to process termination. If an application has locked back to the legacy exception policy, an unhandled exception on certain kinds of threads may only lead to thread termination.</p>"},{"location":"profiling/#code-generation","title":"Code Generation","text":""},{"location":"profiling/#getting-from-il-to-native-code","title":"Getting from IL to Native Code","text":"<p>The IL in a .NET assembly may get compiled to native code in one of two ways: it may get JIT-compiled at run time, or it may be compiled into a \"native image\" by a tool called NGEN.exe (or CrossGen.exe for CoreCLR). Both the JIT-compiler and NGEN have a number of flags that control code generation.</p> <p>At the time an assembly is loaded, the CLR first looks for a native image for the assembly. If no native image is found with the right set of code-generation flags, the CLR will JIT-compile the functions in the assembly as they are needed during the run. Even when a native image is found and loaded, the CLR may end up JIT-compiling some of the functions in the assembly.</p>"},{"location":"profiling/#profiler-control-over-code-generation","title":"Profiler Control over Code-Generation","text":"<p>The profiler has control over code generation, as described below:</p> Flag Effect COR_PRF_USE_PROFILE_IMAGES Causes the native image search to look for profiler-enhanced images (ngen /profile).Has no effect on JITted code. COR_PRF_DISABLE_INLINING Has no effect on the native image search.If JITting, disables inlining. All other optimizations remain in effect. COR_PRF_DISABLE_OPTIMIZATIONS Has no effect on the native image search.If JITting, disables all optimizations, including inlining. COR_PRF_MONITOR_ENTERLEAVE Causes the native image search to look for profiler-enhanced images (ngen /profile).If JITting, inserts enter/leave hooks into the generated code. COR_PRF_MONITOR_CODE_TRANSITIONS Causes the native image search to look for profiler-enhanced images (ngen /profile).If JITting, inserts hooks at managed/unmanaged transition points."},{"location":"profiling/#profilers-and-native-images","title":"Profilers and Native Images","text":"<p>When NGEN.exe creates a native image, it does much of the work that the CLR would have done at run-time\u2014for example, class loading and method compilation. As a result, in cases where work was done at NGEN time, certain profiler callbacks will not be received at run-time:</p> <ul> <li>JITCompilation*</li> <li>ClassLoad*, ClassUnload*</li> </ul> <p>To deal with this situation, profilers that do not wish to perturb the process by requesting profiler-enhanced native images should be prepared to lazily gather any data required about FunctionIDs or ClassIDs as they are encountered.</p>"},{"location":"profiling/#profiler-enhanced-native-images","title":"Profiler-Enhanced Native Images","text":"<p>Creating a native image with NGEN /profile turns on a set of code-generation flags that make the image easier to profile:</p> <ul> <li>Enter/leave hooks are inserted into the code.</li> <li>Managed/unmanaged transition hooks are inserted into the code.</li> <li>JITCachedFunctionSearch notifications are given as each function in the native image is invoked for the first time.</li> <li>ClassLoad notifications are given as each class in the native image is used for the first time.</li> </ul> <p>Because profiler-enhanced native images differ significantly from regular ones, profilers should only use them when the extra perturbation is acceptable.</p> <p>TODO: Instrumentation</p> <p>TODO: Remoting</p>"},{"location":"profiling/#security-issues-in-profiling","title":"Security Issues in Profiling","text":"<p>A profiler DLL is an unmanaged DLL that is effectively running as part of the CLR's execution engine itself. As a result, the code in the profiler DLL is not subject to the restrictions of managed code-access security, and the only limitations on it are those imposed by the OS on the user running the profiled application.</p>"},{"location":"profiling/#combining-managed-and-unmanaged-code-in-a-code-profiler","title":"Combining Managed and Unmanaged Code in a Code Profiler","text":"<p>A close review of the CLR Profiling API creates the impression that you could write a profiler that has managed and unmanaged components that call to each other through COM Interop or PInvoke calls.</p> <p>Although this is possible from a design perspective, the CLR Profiling API does not support it. A CLR profiler is supposed to be purely unmanaged. Attempts to combine managed and unmanaged code from a CLR profiler can cause crashes, hangs and deadlocks. The danger is clear since the managed parts of the profiler will \"fire\" events back to its unmanaged component, which subsequently would call into the managed part of the profiler etc. The danger at this point is clear.</p> <p>The only location that a CLR profiler could invoke managed code safely would be through replacement of the MSIL body of a method. The profiler before the JIT-compilation of a function is completed inserts managed calls in the MSIL body of a method and then lets the JIT compile it. This technique can successfully be used for selective instrumentation of managed code, or it can be used to gather statistics and times about the JIT.</p> <p>Alternatively a code profiler could insert native \"hooks\" in the MSIL body of every managed function that call into unmanaged code. That technique could be used for instrumentation and coverage. For example a code profiler could be inserting instrumentation hooks after every MSIL block to ensure that the block has been executed. The modification of the MSIL body of a method is very delicate operation and there are many factors that should be taken into consideration.</p>"},{"location":"profiling/#profiling-unmanaged-code","title":"Profiling Unmanaged Code","text":"<p>There is minimal support in the Runtime profiling interfaces for profiling unmanaged code. The following functionality is provided:</p> <ul> <li>Enumeration of stack chains. This allows a code profiler to determine the boundary between managed code and unmanaged code.</li> <li>Determine if a stack chain corresponds to managed or native code.</li> </ul> <p>These methods are available through the in-process subset of the CLR debugging API.  These are defined in the CorDebug.IDL and explained in DebugRef.doc, please refer to both for more details.</p>"},{"location":"profiling/#sampling-profilers","title":"Sampling Profilers","text":""},{"location":"profiling/#hijacking","title":"Hijacking","text":"<p>Some sampling profilers operate by hijacking the thread at sample time and forcing it to do the work of the sample. This is a very tricky practice that we do not recommend. The rest of this section is mostly to discourage you from going this way.</p>"},{"location":"profiling/#timing-of-hijacks","title":"Timing of Hijacks","text":"<p>A hijacking profiler must track the runtime suspension events (COR_PRF_MONITOR_SUSPENDS). The profiler should assume that when it returns from a RuntimeThreadSuspended callback, the runtime will hijack that thread. The profiler must avoid having its hijack conflict with the runtime's hijack. To do so, the profiler must ensure that:</p> <ol> <li>The profiler does not attempt to hijack a thread between RuntimeThreadSuspended and RuntimeThreadResumed.</li> <li>If the profiler has begun hijacking before the RuntimeThreadSuspended callback was issued, the callback does not return before the hijack completes.</li> </ol> <p>This can be accomplished by some simple synchronization.</p>"},{"location":"profiling/#initializing-the-runtime","title":"Initializing the Runtime","text":"<p>If the profiler has its own thread on which it will be calling ICorProfilerInfo functions, it needs to ensure that it calls one such function before doing any thread suspensions. This is because the runtime has per-thread state that needs to be initialized with all other threads running to avoid possible deadlocks.</p>"},{"location":"r2r-perfmap-format/","title":"Ready to run PerfMap format","text":"<p>Traditionally in .NET symbols have been described using PDBs. These are used to map IL to source lines for code that the JIT will compile. The JIT usually emits the data that can then map from IL to a native address for symbolication purposes.</p> <p>Ready to run, however, avoids this IL to native code translation at runtime. For this reason, tools that emit R2R images often need to emit auxiliary artifacts to facilitate the mapping between source and native addresses. The Ready to Run PerfMap format describes such one map - where any method in the source code is associated with a region within the R2R image. That way any region from such image that gets executed can be linked back to a method at the source level. This facilitates tasks like stack symbolication for performance oriented investigations, although it is not appropriate to aid in tasks such as debugging at the source line level.</p>"},{"location":"r2r-perfmap-format/#version-1","title":"Version 1","text":"<p>R2R PerfMaps of version 1 are usually found in files with the extension <code>.ni.r2rmap</code>. It's a plain text UTF-8 format where each entry is on a separate line. Each entry is composed of a triplet: an offset relative to the beginning of the image, a length, and a name. The file is laid out in the following as follows.</p>"},{"location":"r2r-perfmap-format/#header","title":"Header","text":"<p>The header leads the file and is composed by special entries. Each entry contains a 4 byte integer token in place of an RVA signifying the type of information in the entry, a length that is always 0, and the entry data. The entries are emitted in the following order.</p> Token Description 0xFFFFFFFF A 16 byte sequence representing a signature to correlate the perfmap with the r2r image. 0xFFFFFFFE The version of the perfmap being emitted as a unsigned 4 byte integer. 0xFFFFFFFD An unsigned 4 byte unsigned integer representing the OS the image targets. See enumerables section 0xFFFFFFFC An unsigned 4 byte unsigned integer representing the architecture the image targets. See enumerables section 0xFFFFFFFB An unsigned 4 byte unsigned integer representing the ABI of the image. See enumerables section <p>These entries contain information about the compilation that can be useful to tools and identifiers that can be used to correlate a perfmap with an image as described in \"Ready to Run format - debug directory entries\".</p>"},{"location":"r2r-perfmap-format/#content","title":"Content","text":"<p>Each entry is a triplet - the relative address of a method with respect to the image start as an unsigned 4 byte integer, the number of bytes used by the native code represented by an unsigned 2 byte integer, and the name of the method. There's one entry per line after the header, and a method can appear more than once since if may have gone through cold/hot path splitting.</p>"},{"location":"r2r-perfmap-format/#enumerables-used-in-headers","title":"Enumerables used in headers.","text":"<pre><code>PerfMapArchitectureToken\n    Unknown = 0,\n    ARM     = 1,\n    ARM64   = 2,\n    X64     = 3,\n    X86     = 4,\n</code></pre> <pre><code>PerfMapOSToken\n    Unknown     = 0,\n    Windows     = 1,\n    Linux       = 2,\n    OSX         = 3,\n    FreeBSD     = 4,\n    NetBSD      = 5,\n    SunOS       = 6,\n</code></pre> <pre><code>PerfMapAbiToken\n    Unknown = 0,\n    Default = 1,\n    Armel = 2,\n</code></pre>"},{"location":"readytorun-format/","title":"ReadyToRun File Format","text":"<p>Revisions: * 1.1 - Jan Kotas - 2015 * 3.1 - Tomas Rylek - 2019 * 4.1 - Tomas Rylek - 2020 * 5.3 - Tomas Rylek - 2021 * 5.4 - David Wrighton - 2021 * 6.3 - David Wrighton - 2022</p>"},{"location":"readytorun-format/#introduction","title":"Introduction","text":"<p>This document describes ReadyToRun format 3.1 implemented in CoreCLR as of June 2019 and not yet implemented proposed extensions 4.1 for the support of composite R2R file format. Composite R2R file format has basically the same structure as the traditional R2R file format defined in earlier revisions except that the output file represents a larger number of input MSIL assemblies compiled together as a logical unit.</p>"},{"location":"readytorun-format/#pe-headers-and-cli-headers","title":"PE Headers and CLI Headers","text":"<p>Single-file ReadyToRun images conform to CLI file format as described in ECMA-335 with the following customizations:</p> <ul> <li>The PE file is always platform specific</li> <li>CLI Header Flags field has set <code>COMIMAGE_FLAGS_IL_LIBRARY</code> (0x00000004) bit set</li> <li>CLI Header <code>ManagedNativeHeader</code> points to READYTORUN_HEADER</li> </ul> <p>The COR header and ECMA 335 metadata pointed to by the COM descriptor data directory item in the COFF header represent a full copy of the input IL and MSIL metadata it was generated from.</p> <p>Composite R2R files currently conform to Windows PE executable file format as the native envelope. Moving forward we plan to gradually add support for platform-native executable formats (ELF on Linux, MachO on OSX) as the native envelopes. There is a global CLI / COR header in the file, but it only exists to facilitate pdb generation, and does not participate in any usages by the CoreCLR runtime. The ReadyToRun header structure is pointed to by the well-known export symbol <code>RTR_HEADER</code> and has the <code>READYTORUN_FLAG_COMPOSITE</code> flag set.</p> <p>Input MSIL metadata and IL streams can be either embedded in the composite R2R file or left as separate files on disk. In case of embedded MSIL, the \"actual\" metadata for the individual component assemblies is accessed via the R2R section <code>ComponentAssemblies</code>.</p> <p>Standalone MSIL files used as the source of IL and metadata for composite R2R executables without MSIL embedding are copied to the output folder next to the composite R2R executable and are rewritten by the compiler to include a formal ReadyToRun header with forwarding information pointing to the owner composite R2R executable (section <code>OwnerCompositeExecutable</code>).</p>"},{"location":"readytorun-format/#additions-to-the-debug-directory","title":"Additions to the debug directory","text":"<p>Currently shipping PE envelopes - both single-file and composite - can contain records for additional debug information in the debug directory. One such entry specific to R2R images is the one for R2R PerfMaps. The format of the auxiliary file is described R2R perfmap format and the corresponding debug directory entry is described in PE COFF.</p>"},{"location":"readytorun-format/#future-improvements","title":"Future Improvements","text":"<p>The limitations of the current format are:</p> <ul> <li> <p>Type loading from IL metadata: All types are built from IL metadata at runtime currently.   It is bloating the size - prevents stripping full metadata from the image, and fragile -   assumes fixed field layout algorithm. A new section with compact type layout description   optimized for runtime type loading is needed to address it. (Similar concept as CTL.)</p> </li> <li> <p>Debug info size: The debug information is unnecessarily bloating the image. This solution was   chosen for compatibility with the current desktop/CoreCLR debugging pipeline. Ideally, the   debug information should be stored in separate file.</p> </li> </ul>"},{"location":"readytorun-format/#structures","title":"Structures","text":"<p>The structures and accompanying constants are defined in the readytorun.h header file. Basically the entire R2R executable image is addressed through the READYTORUN_HEADER singleton pointed to by the well-known export RTR_HEADER in the export section of the native executable envelope.</p> <p>For single-file R2R executables, there's just one header representing all image sections. For composite and single exe, the global <code>READYTORUN_HEADER</code> includes a section of the type <code>ComponentAssemblies</code> representing the component assemblies comprising the composite R2R image. This table is parallel to (it used the same indexing as) the table <code>READYTORUN_MANIFEST_METADATA</code>. Each <code>READYTORUN_SECTION_ASSEMBLIES_ENTRY</code> record points to a <code>READYTORUN_CORE_HEADER</code> variable-length structure representing sections specific to the particular assembly.</p>"},{"location":"readytorun-format/#readytorun_header","title":"READYTORUN_HEADER","text":"<pre><code>struct READYTORUN_HEADER\n{\n    DWORD                   Signature;      // READYTORUN_SIGNATURE\n    USHORT                  MajorVersion;   // READYTORUN_VERSION_XXX\n    USHORT                  MinorVersion;\n\n    READYTORUN_CORE_HEADER  CoreHeader;\n}\n</code></pre>"},{"location":"readytorun-format/#readytorun_headersignature","title":"READYTORUN_HEADER::Signature","text":"<p>Always set to 0x00525452 (ASCII encoding for RTR). The signature can be used to distinguish ReadyToRun images from other CLI images with ManagedNativeHeader (e.g. NGen images).</p>"},{"location":"readytorun-format/#readytorun_headermajorversionminorversion","title":"READYTORUN_HEADER::MajorVersion/MinorVersion","text":"<p>The current format version is 3.1. MajorVersion increments are meant for file format breaking changes. MinorVersion increments are meant to compatible file format changes.</p> <p>Example: Assume the highest version supported by the runtime is 2.3. The runtime should be able to successfully execute native code from images of version 2.9. The runtime should refuse to execute native code from image of version 3.0.</p>"},{"location":"readytorun-format/#readytorun_core_header","title":"READYTORUN_CORE_HEADER","text":"<pre><code>struct READYTORUN_CORE_HEADER\n{\n    DWORD                   Flags;          // READYTORUN_FLAG_XXX\n\n    DWORD                   NumberOfSections;\n\n    // Array of sections follows. The array entries are sorted by Type\n    // READYTORUN_SECTION   Sections[];\n};\n</code></pre>"},{"location":"readytorun-format/#readytorun_core_headerflags","title":"READYTORUN_CORE_HEADER::Flags","text":"Flag Value Description READYTORUN_FLAG_PLATFORM_NEUTRAL_SOURCE 0x00000001 Set if the original IL image was platform neutral. The platform neutrality is part of assembly name. This flag can be used to reconstruct the full original assembly name. READYTORUN_FLAG_COMPOSITE 0x00000002 The image represents a composite R2R file resulting from a combined compilation of a larger number of input MSIL assemblies. READYTORUN_FLAG_PARTIAL 0x00000004 READYTORUN_FLAG_NONSHARED_PINVOKE_STUBS 0x00000008 PInvoke stubs compiled into image are non-shareable (no secret parameter) READYTORUN_FLAG_EMBEDDED_MSIL 0x00000010 Input MSIL is embedded in the R2R image. READYTORUN_FLAG_COMPONENT 0x00000020 This is a component assembly of a composite R2R image READYTORUN_FLAG_MULTIMODULE_VERSION_BUBBLE 0x00000040 This R2R module has multiple modules within its version bubble (For versions before version 6.3, all modules are assumed to possibly have this characteristic) READYTORUN_FLAG_UNRELATED_R2R_CODE 0x00000080 This R2R module has code in it that would not be naturally encoded into this module"},{"location":"readytorun-format/#readytorun_section","title":"READYTORUN_SECTION","text":"<pre><code>struct READYTORUN_SECTION\n{\n    DWORD                   Type;           // READYTORUN_SECTION_XXX\n    IMAGE_DATA_DIRECTORY    Section;\n};\n</code></pre> <p>The <code>READYTORUN_CORE_HEADER</code> structure is immediately followed by an array of <code>READYTORUN_SECTION</code> records representing the individual R2R sections. Number of elements in the array is <code>READYTORUN_HEADER::NumberOfSections</code>. Each record contains section type and its location within the binary. The array is sorted by section type to allow binary searching.</p> <p>This setup allows adding new or optional section types, and obsoleting existing section types, without file format breaking changes. The runtime is not required to understand all section types in order to load and execute the ready to run file.</p> <p>The following section types are defined and described later in this document:</p> ReadyToRunSectionType Value Scope (component assembly / entire image) CompilerIdentifier 100 Image ImportSections 101 Image RuntimeFunctions 102 Image MethodDefEntryPoints 103 Assembly ExceptionInfo 104 Assembly DebugInfo 105 Assembly DelayLoadMethodCallThunks 106 Assembly ~~AvailableTypes~~ 107 (obsolete - used by an older format) AvailableTypes 108 Assembly InstanceMethodEntryPoints 109 Image InliningInfo 110 Assembly (added in V2.1) ProfileDataInfo 111 Image (added in V2.2) ManifestMetadata 112 Image (added in V2.3) AttributePresence 113 Assembly (added in V3.1) InliningInfo2 114 Image (added in V4.1) ComponentAssemblies 115 Image (added in V4.1) OwnerCompositeExecutable 116 Image (added in V4.1) PgoInstrumentationData 117 Image (added in V5.2) ManifestAssemblyMvids 118 Image (added in V5.3) CrossModuleInlineInfo 119 Image (added in V6.3) HotColdMap 120 Image (added in V8.0) MethodIsGenericMap 121 Assembly (Added in V9.0) EnclosingTypeMap 122 Assembly (Added in V9.0) TypeGenericInfoMap 123 Assembly (Added in V9.0)"},{"location":"readytorun-format/#readytorunsectiontypecompileridentifier","title":"ReadyToRunSectionType.CompilerIdentifier","text":"<p>This section contains zero terminated ASCII string that identifies the compiler used to produce the image.</p> <p>Example: <code>CoreCLR 4.6.22727.0 PROJECTK</code></p>"},{"location":"readytorun-format/#readytorunsectiontypeimportsections","title":"ReadyToRunSectionType.ImportSections","text":"<p>This section contains array of READYTORUN_IMPORT_SECTION structures. Each entry describes range of slots that had to be filled with the value from outside the module (typically lazily). The initial values of slots in each range are either zero or pointers to lazy initialization helper.</p> <pre><code>struct READYTORUN_IMPORT_SECTION\n{\n    IMAGE_DATA_DIRECTORY    Section;            // Section containing values to be fixed up\n    USHORT                  Flags;              // One or more of ReadyToRunImportSectionFlags\n    BYTE                    Type;               // One of ReadyToRunImportSectionType\n    BYTE                    EntrySize;\n    DWORD                   Signatures;         // RVA of optional signature descriptors\n    DWORD                   AuxiliaryData;      // RVA of optional auxiliary data (typically GC info)\n};\n</code></pre>"},{"location":"readytorun-format/#readytorun_import_sectionsflags","title":"READYTORUN_IMPORT_SECTIONS::Flags","text":"ReadyToRunImportSectionFlags Value Description ReadyToRunImportSectionFlags::None 0x0000 None ReadyToRunImportSectionFlags::Eager 0x0001 Set if the slots in the section have to be initialized at image load time. It is used to avoid lazy initialization when it cannot be done or when it would have undesirable reliability or performance effects (unexpected failure or GC trigger points, overhead of lazy initialization). ReadyToRunImportSectionFlags::PCode 0x0004 Section contains pointers to code"},{"location":"readytorun-format/#readytorun_import_sectionstype","title":"READYTORUN_IMPORT_SECTIONS::Type","text":"ReadyToRunImportSectionType Value Description ReadyToRunImportSectionType::Unknown 0 The type of slots in this section is unspecified. ReadyToRunImportSectionType::StubDispatch 2 The type of slots in this section rely on stubs for dispatch. ReadyToRunImportSectionType::StringHandle 3 The type of slots in this section hold strings ReadyToRunImportSectionType::ILBodyFixups 7 The type of slots in this section represent cross module IL bodies <p>Future: The section type can be used to group slots of the same type together. For example, all virtual stub dispatch slots may be grouped together to simplify resetting of virtual stub dispatch cells into their initial state.</p>"},{"location":"readytorun-format/#readytorun_import_sectionssignatures","title":"READYTORUN_IMPORT_SECTIONS::Signatures","text":"<p>This field points to array of RVAs that is parallel with the array of slots. Each RVA points to fixup signature that contains the information required to fill the corresponding slot. The signature encoding builds upon the encoding used for signatures in ECMA-335. The first element of the signature describes the fixup kind, the rest of the signature varies based on the fixup kind.</p> ReadyToRunFixupKind Value Description READYTORUN_FIXUP_ThisObjDictionaryLookup 0x07 Generic lookup using <code>this</code>; followed by the type signature and by the method signature READYTORUN_FIXUP_TypeDictionaryLookup 0x08 Type-based generic lookup for methods on instantiated types; followed by the typespec signature READYTORUN_FIXUP_MethodDictionaryLookup 0x09 Generic method lookup; followed by the method spec signature READYTORUN_FIXUP_TypeHandle 0x10 Pointer uniquely identifying the type to the runtime, followed by typespec signature (see ECMA-335) READYTORUN_FIXUP_MethodHandle 0x11 Pointer uniquely identifying the method to the runtime, followed by method signature (see below) READYTORUN_FIXUP_FieldHandle 0x12 Pointer uniquely identifying the field to the runtime, followed by field signature (see below) READYTORUN_FIXUP_MethodEntry 0x13 Method entrypoint or call, followed by method signature READYTORUN_FIXUP_MethodEntry_DefToken 0x14 Method entrypoint or call, followed by methoddef token (shortcut) READYTORUN_FIXUP_MethodEntry_RefToken 0x15 Method entrypoint or call, followed by methodref token (shortcut) READYTORUN_FIXUP_VirtualEntry 0x16 Virtual method entrypoint or call, followed by method signature READYTORUN_FIXUP_VirtualEntry_DefToken 0x17 Virtual method entrypoint or call, followed by methoddef token (shortcut) READYTORUN_FIXUP_VirtualEntry_RefToken 0x18 Virtual method entrypoint or call, followed by methodref token (shortcut) READYTORUN_FIXUP_VirtualEntry_Slot 0x19 Virtual method entrypoint or call, followed by typespec signature and slot READYTORUN_FIXUP_Helper 0x1A Helper call, followed by helper call id (see chapter 4 Helper calls) READYTORUN_FIXUP_StringHandle 0x1B String handle, followed by metadata string token READYTORUN_FIXUP_NewObject 0x1C New object helper, followed by typespec  signature READYTORUN_FIXUP_NewArray 0x1D New array helper, followed by typespec signature READYTORUN_FIXUP_IsInstanceOf 0x1E isinst helper, followed by typespec signature READYTORUN_FIXUP_ChkCast 0x1F chkcast helper, followed by typespec signature READYTORUN_FIXUP_FieldAddress 0x20 Field address, followed by field signature READYTORUN_FIXUP_CctorTrigger 0x21 Static constructor trigger, followed by typespec signature READYTORUN_FIXUP_StaticBaseNonGC 0x22 Non-GC static base, followed by typespec signature READYTORUN_FIXUP_StaticBaseGC 0x23 GC static base, followed by typespec signature READYTORUN_FIXUP_ThreadStaticBaseNonGC 0x24 Non-GC thread-local static base, followed by typespec signature READYTORUN_FIXUP_ThreadStaticBaseGC 0x25 GC thread-local static base, followed by typespec signature READYTORUN_FIXUP_FieldBaseOffset 0x26 Starting offset of fields for given type, followed by typespec signature. Used to address base class fragility. READYTORUN_FIXUP_FieldOffset 0x27 Field offset, followed by field signature READYTORUN_FIXUP_TypeDictionary 0x28 Hidden dictionary argument for generic code, followed by typespec signature READYTORUN_FIXUP_MethodDictionary 0x29 Hidden dictionary argument for generic code, followed by method signature READYTORUN_FIXUP_Check_TypeLayout 0x2A Verification of type layout, followed by typespec and expected type layout descriptor READYTORUN_FIXUP_Check_FieldOffset 0x2B Verification of field offset, followed by field signature and expected field layout descriptor READYTORUN_FIXUP_DelegateCtor 0x2C Delegate constructor, followed by method signature READYTORUN_FIXUP_DeclaringTypeHandle 0x2D Dictionary lookup for method declaring type. Followed by the type signature. READYTORUN_FIXUP_IndirectPInvokeTarget 0x2E Target (indirect) of an inlined PInvoke. Followed by method signature. READYTORUN_FIXUP_PInvokeTarget 0x2F Target of an inlined PInvoke. Followed by method signature. READYTORUN_FIXUP_Check_InstructionSetSupport 0x30 Specify the instruction sets that must be supported/unsupported to use the R2R code associated with the fixup. READYTORUN_FIXUP_Verify_FieldOffset 0x31 Generate a runtime check to ensure that the field offset matches between compile and runtime. Unlike CheckFieldOffset, this will generate a runtime exception on failure instead of silently dropping the method READYTORUN_FIXUP_Verify_TypeLayout 0x32 Generate a runtime check to ensure that the field offset matches between compile and runtime. Unlike CheckFieldOffset, this will generate a runtime exception on failure instead of silently dropping the method READYTORUN_FIXUP_Check_VirtualFunctionOverride 0x33 Generate a runtime check to ensure that virtual function resolution has equivalent behavior at runtime as at compile time. If not equivalent, code will not be used. See Virtual override signatures for details of the signature used. READYTORUN_FIXUP_Verify_VirtualFunctionOverride 0x34 Generate a runtime check to ensure that virtual function resolution has equivalent behavior at runtime as at compile time. If not equivalent, generate runtime failure. See Virtual override signatures for details of the signature used. READYTORUN_FIXUP_Check_IL_Body 0x35 Check to see if an IL method is defined the same at runtime as at compile time. A failed match will cause code not to be used. SeeIL Body signatures for details. READYTORUN_FIXUP_Verify_IL_Body 0x36 Verify an IL body is defined the same at compile time and runtime. A failed match will cause a hard runtime failure. SeeIL Body signatures for details. READYTORUN_FIXUP_ModuleOverride 0x80 When or-ed to the fixup ID, the fixup byte in the signature is followed by an encoded uint with assemblyref index, either within the MSIL metadata of the master context module for the signature or within the manifest metadata R2R header table (used in cases inlining brings in references to assemblies not seen in the input MSIL)."},{"location":"readytorun-format/#method-signatures","title":"Method Signatures","text":"<p>MethodSpec signatures defined by ECMA-335 are not rich enough to describe method flavors referenced by native code. The first element of the method signature are flags. It is followed by method token, and additional data determined by the flags.</p> ReadyToRunMethodSigFlags Value Description READYTORUN_METHOD_SIG_UnboxingStub 0x01 Unboxing entrypoint of the method. READYTORUN_METHOD_SIG_InstantiatingStub 0x02 Instantiating entrypoint of the method does not take hidden dictionary generic argument. READYTORUN_METHOD_SIG_MethodInstantiation 0x04 Method instantitation. Number of instantiation arguments followed by typespec for each of them appended as additional data. READYTORUN_METHOD_SIG_SlotInsteadOfToken 0x08 If set, the token is slot number. Used for multidimensional array methods that do not have metadata token, and also as an optimization for stable interface methods. Cannot be combined with <code>MemberRefToken</code>. READYTORUN_METHOD_SIG_MemberRefToken 0x10 If set, the token is memberref token. If not set, the token is methoddef token. READYTORUN_METHOD_SIG_Constrained 0x20 Constrained type for method resolution. Typespec appended as additional data. READYTORUN_METHOD_SIG_OwnerType 0x40 Method type. Typespec appended as additional data. READYTORUN_METHOD_SIG_UpdateContext 0x80 If set, update the module which is used to parse tokens before performing any token processing. A uint index into the modules table immediately follows the flags"},{"location":"readytorun-format/#field-signatures","title":"Field Signatures","text":"<p>ECMA-335 does not define field signatures that are rich enough to describe method flavors referenced by native code. The first element of the field signature are flags. It is followed by field token, and additional data determined by the flags.</p> ReadyToRunFieldSigFlags Value Description READYTORUN_FIELD_SIG_IndexInsteadOfToken 0x08 Used as an optimization for stable fields. Cannot be combined with <code>MemberRefToken</code>. READYTORUN_FIELD_SIG_MemberRefToken 0x10 If set, the token is memberref token. If not set, the token is fielddef token. READYTORUN_FIELD_SIG_OwnerType 0x40 Field type. Typespec appended as additional data."},{"location":"readytorun-format/#virtual-override-signatures","title":"Virtual override signatures","text":"<p>ECMA 335 does not have a natural encoding for describing an overridden method. These signatures are encoded as a ReadyToRunVirtualFunctionOverrideFlags byte, followed by a method signature representing the declaration method, a type signature representing the type which is being devirtualized, and (optionally) a method signature indicating the implementation method.</p> ReadyToRunVirtualFunctionOverrideFlags Value Description READYTORUN_VIRTUAL_OVERRIDE_None 0x00 No flags are set READYTORUN_VIRTUAL_OVERRIDE_VirtualFunctionOverridden 0x01 If set, then the virtual function has an implementation, which is encoded in the optional method implementation signature."},{"location":"readytorun-format/#il-body-signatures","title":"IL Body signatures","text":"<p>ECMA 335 does not define a format that can represent the exact implementation of a method by itself. This signature holds all of the IL of the method, the EH table, the locals table, and each token (other than type references) in those tables is replaced with an index into a local stream of signatures. Those signatures are simply verbatim copies of the needed metadata to describe MemberRefs, TypeSpecs, MethodSpecs, StandaloneSignatures and strings. All of that is bundled into a large byte array. In addition, a series of TypeSignatures follows which allow the type references to be resolved, as well as a methodreference to the uninstantiated method. Assuming all of this matches with the data that is present at runtime, the fixup is considered to be satisfied. See ReadyToRunStandaloneMetadata.cs for the exact details of the format.</p>"},{"location":"readytorun-format/#readytorun_import_sectionsauxiliarydata","title":"READYTORUN_IMPORT_SECTIONS::AuxiliaryData","text":"<p>For slots resolved lazily via <code>READYTORUN_HELPER_DelayLoad_MethodCall</code> helper, auxiliary data are compressed argument maps that allow precise GC stack scanning while the helper is running. The CoreCLR runtime class <code>GCRefMapDecoder</code> is used to parse this information. This data would not be required for runtimes that allow conservative stack scanning.</p> <p>The auxiliary data table contains the exact same number of GC ref map records as there are method entries in the import section. To accelerate GC ref map lookup, the auxiliary data section starts with a lookup table holding the offset of every 1024-th method in the runtime function table within the linearized GC ref map.</p> Offset in auxiliary data Size Content 0 4 Offset to GC ref map info for method #0 relative to this byte i.e. 4 * (MethodCount / 1024 + 1) 4 4 Offset to GC ref map info for method #1024 8 4 Offset to GC ref map info for method #2048 ... 4 * (MethodCount / 1024 + 1) ... Serialized GC ref map info <p>The GCRef map is used to encode GC type of arguments for callsites. Logically, it is a sequence <code>&lt;pos, token&gt;</code> where <code>pos</code> is position of the reference in the stack frame and <code>token</code> is type of GC reference (one of <code>GCREFMAP_XXX</code> values):</p> CORCOMPILE_GCREFMAP_TOKENS Value Stack frame entry interpretation GCREFMAP_SKIP 0 Not a GC-relevant entry GCREFMAP_REF 1 GC reference GCREFMAP_INTERIOR 2 Pointer to a GC reference GCREFMAP_METHOD_PARAM 3 Hidden method instantiation argument to generic method GCREFMAP_TYPE_PARAM 4 Hidden type instantiation argument to generic method GCREFMAP_VASIG_COOKIE 5 VARARG signature cookie <p>The position values are calculated in <code>size_t</code> aka <code>IntPtr</code> units (4 bytes for 32-bit architectures vs. 8 bytes for 64-bit architectures) starting at the first position in the transition frame that may contain GC references. For all architectures except for arm64 this is the beginning of the array of spilled argument registers. On arm64 it is the offset of the <code>X8</code> register used to pass the location to be filled in with the return value by the called method.</p> <ul> <li> <p>The encoding always starts at the byte boundary. The high order bit of each byte is used to signal end of the encoding stream. The last byte has the high order bit zero. It means that there are 7 useful bits in each byte.</p> </li> <li> <p>\"pos\" is always encoded as delta from previous pos.</p> </li> <li> <p>The basic encoding unit is two bits. Values 0, 1 and 2 are the common constructs (skip single slot, GC reference, interior pointer). Value 3 means that extended encoding follows.</p> </li> <li> <p>The extended information is integer encoded in one or more four bit blocks. The high order bit of the four bit block is used to signal the end.</p> </li> <li> <p>For x86, the encoding starts with size of the callee popped stack. The size is encoded using the same mechanism as above (two bit basic encoding, with extended encoding for large values).</p> </li> </ul>"},{"location":"readytorun-format/#readytorunsectiontyperuntimefunctions","title":"ReadyToRunSectionType.RuntimeFunctions","text":"<p>This section contains sorted array of <code>RUNTIME_FUNCTION</code> entries that describe all code blocks in the image with pointers to their unwind info. Despite the name, these code block might represent a method body, or it could be just a part of it (e.g. a funclet) that requires its own unwind data. The standard Windows xdata/pdata format is used. ARM format is used for x86 to compensate for the lack of x86 unwind info standard. The unwind info blob is immediately followed by the GC info blob. The encoding slightly differs for amd64 which encodes an extra 4-byte representing the end RVA of the unwind info blob.</p>"},{"location":"readytorun-format/#runtime_function-x86-arm-arm64-size-8-bytes","title":"RUNTIME_FUNCTION (x86, arm, arm64, size = 8 bytes)","text":"Offset Size Value 0 4 Unwind info start RVA 4 4 GC info start RVA"},{"location":"readytorun-format/#runtime_function-amd64-size-12-bytes","title":"RUNTIME_FUNCTION (amd64, size = 12 bytes)","text":"Offset Size Value 0 4 Unwind info start RVA 4 4 Unwind info end RVA (1 plus RVA of last byte) 8 4 GC info start RVA"},{"location":"readytorun-format/#readytorunsectiontypemethoddefentrypoints","title":"ReadyToRunSectionType.MethodDefEntryPoints","text":"<p>This section contains a native format sparse array (see 4 Native Format) that maps methoddef rows to method entrypoints. Methoddef is used as index into the array. The element of the array is index of the method in <code>RuntimeFunctions</code>, followed by list of slots that need to be filled before the method can start executing.</p> <p>The index of the method is left-shifted by 1 bit with the low bit indicating whether a list of slots to fix up follows. The list of slots is encoded as follows (same encoding as used by NGen):</p> <pre><code>READYTORUN_IMPORT_SECTIONS absolute index\n    absolute slot index\n    slot index delta\n    _\n    slot index delta\n    0\nREADYTORUN_IMPORT_SECTIONS index delta\n    absolute slot index\n    slot index delta\n    _\n    slot delta\n    0\nREADYTORUN_IMPORT_SECTIONS index delta\n    absolute slot index\n    slot index delta\n    _\n    slot delta\n    0\n0\n</code></pre> <p>The fixup list is a stream of integers encoded as nibbles (1 nibble = 4 bits). 3 bits of a nibble are used to store 3 bits of the value, and the top bit indicates if the following nibble contains rest of the value. If the top bit in the nibble is set, then the value continues in the next nibble.</p> <p>The section and slot indices are delta-encoded offsets from that initial absolute index.  Delta-encoded means that the i-th value is the sum of values [1..i].</p> <p>The list is terminated by a 0 (0 is not meaningful as valid delta).</p> <p>Note: This is a per-assembly section. In single-file R2R files, it is pointed to directly by the main R2R header; in composite R2R files, each component module has its own entrypoint section pointed to by the <code>READYTORUN_SECTION_ASSEMBLIES_ENTRY</code> core header structure.</p>"},{"location":"readytorun-format/#readytorunsectiontypeexceptioninfo","title":"ReadyToRunSectionType.ExceptionInfo","text":"<p>Exception handling information. This section contains array of <code>READYTORUN_EXCEPTION_LOOKUP_TABLE_ENTRY</code> sorted by <code>MethodStart</code> RVA. <code>ExceptionInfo</code> is RVA of <code>READYTORUN_EXCEPTION_CLAUSE</code> array that described the exception handling information for given method.</p> <pre><code>struct READYTORUN_EXCEPTION_LOOKUP_TABLE_ENTRY\n{\n    DWORD MethodStart;\n    DWORD ExceptionInfo;\n};\n\nstruct READYTORUN_EXCEPTION_CLAUSE\n{\n    CorExceptionFlag    Flags;\n    DWORD               TryStartPC;\n    DWORD               TryEndPC;\n    DWORD               HandlerStartPC;\n    DWORD               HandlerEndPC;\n    union {\n        mdToken         ClassToken;\n        DWORD           FilterOffset;\n    };\n};\n</code></pre> <p>Same encoding is as used by NGen.</p>"},{"location":"readytorun-format/#readytorunsectiontypedebuginfo","title":"ReadyToRunSectionType.DebugInfo","text":"<p>This section contains information to support debugging: native offset and local variable maps.</p> <p>TODO: Document the debug info encoding. It is the same encoding as used by NGen. It should not be required when debuggers are able to handle debug info stored separately.</p>"},{"location":"readytorun-format/#readytorunsectiontypedelayloadmethodcallthunks","title":"ReadyToRunSectionType.DelayLoadMethodCallThunks","text":"<p>This section marks region that contains thunks for <code>READYTORUN_HELPER_DelayLoad_MethodCall</code> helper. It is used by debugger for step-in into lazily resolved calls. It should not be required when debuggers are able to handle debug info stored separately.</p>"},{"location":"readytorun-format/#readytorunsectiontypeavailabletypes","title":"ReadyToRunSectionType.AvailableTypes","text":"<p>This section contains a native hashtable of all defined &amp; export types within the compilation module. The key is the full type name, the value is the exported type or defined type token row ID left-shifted by one and or-ed with bit 0 defining the token type:</p> Bit value Token type 0 defined type 1 exported type <p>The version-resilient hashing algorithm used for hashing the type names is implemented in vm/versionresilienthashcode.cpp.</p> <p>Note: This is a per-assembly section. In single-file R2R files, it is pointed to directly by the main R2R header; in composite R2R files, each component module has its own available type section pointed to by the <code>READYTORUN_SECTION_ASSEMBLIES_ENTRY</code> core header structure.</p>"},{"location":"readytorun-format/#readytorunsectiontypeinstancemethodentrypoints","title":"ReadyToRunSectionType.InstanceMethodEntryPoints","text":"<p>This section contains a native hashtable of all generic method instantiations compiled into the R2R executable. The key is the method instance signature; the appropriate version-resilient hash code calculation is implemented in vm/versionresilienthashcode.cpp; the value, represented by the <code>EntryPointWithBlobVertex</code> class, stores the method index in the runtime function table, the fixups blob and a blob encoding the method signature.</p> <p>Note: In contrast to non-generic method entrypoints, this section is image-wide for composite R2R images. It represents all generics needed by all assemblies within the composite executable. As mentioned elsewhere in this document, CoreCLR runtime requires changes to properly look up methods stored in this section in the composite R2R case.</p> <p>Note: Generic methods and non-generic methods on generic types are encoded into this table and the runtime is expected to lookup into this table in potentially multiple modules. First the runtime is expected to lookup into this table for the module which defines the method, then it is expected to use the \"alternate\" generics location which is defined as the module which is NOT the defining module which is the defining module of one of the generic arguments to the method. This alternate lookup is not currently a deeply nested algorithm. If that lookup fails, then lookup will proceed to every module which specified <code>READYTORUN_FLAG_UNRELATED_R2R_CODE</code> as a flag.</p>"},{"location":"readytorun-format/#readytorunsectiontypeinlininginfo-v21","title":"ReadyToRunSectionType.InliningInfo (v2.1+)","text":"<p>TODO: document inlining info encoding</p>"},{"location":"readytorun-format/#readytorunsectiontypeprofiledatainfo-v22","title":"ReadyToRunSectionType.ProfileDataInfo (v2.2+)","text":"<p>TODO: document profile data encoding</p>"},{"location":"readytorun-format/#readytorunsectiontypemanifestmetadata-v23-with-changes-for-v63","title":"ReadyToRunSectionType.ManifestMetadata (v2.3+ with changes for v6.3+)","text":"<p>Manifest metadata is an [ECMA-335] metadata blob containing extra reference assemblies within the version bubble introduced by inlining on top of assembly references stored in the input MSIL. As of R2R version 3.1, the metadata is only used for the AssemblyRef table. This is used to translate module override indices in signatures to the actual reference modules (using either the <code>READYTORUN_FIXUP_ModuleOverride</code> bit flag on the signature fixup byte or the <code>ELEMENT_TYPE_MODULE_ZAPSIG</code> COR element type).</p> <p>Note: It doesn't make sense to use references to assemblies external to the version bubble in the manifest metadata via the <code>READYTORUN_FIXUP_ModuleOverride</code> or <code>ELEMENT_TYPE_MODULE_ZAPSIG</code> concept as there's no guarantee that their metadata token values remain constant; thus we cannot encode signatures relative to them. However, as of R2R version 6.3, the native manifest metadata may contain tokens to be further resolved to actual implementation assemblies.</p> <p>The module override index translation algorithm is as follows (ILAR = the number of <code>AssemblyRef</code> rows in the input MSIL):</p> <p>For R2R version 6.2 and below</p> Module override index (i) Reference assembly i = 0 Global context - assembly containing the signature 1 &lt;= i &lt;= ILAR i is the index into the MSIL <code>AssemblyRef</code> table i &gt; ILAR i - ILAR - 1 is the zero-based index into the <code>AssemblyRef</code> table in the manifest metadata <p>Note: This means that the entry corresponding to i = ILAR + 1 is actually undefined as it corresponds to the <code>NULL</code> entry (ROWID #0) in the manifest metadata AssemblyRef table. The first meaningful index into the manifest metadata, i = ILAR + 2, corresponding to ROWID #1, is historically filled in by Crossgen with the input assembly info but this shouldn't be depended upon, in fact the input assembly is useless in the manifest metadata as the module override to it can be encoded by using the special index 0.</p> <p>For R2R version 6.3 and above | Module override index (i) | Reference assembly |:----------------------------|:------------------ | i = 0                     | Global context - assembly containing the signature | 1 &lt;= i &lt;= ILAR        | i is the index into the MSIL <code>AssemblyRef</code> table | i = ILAR + 1          | i is the index which refers to the Manifest metadata itself | i &gt; ILAR + 1          | i - ILAR - 2 is the zero-based index into the <code>AssemblyRef</code> table in the manifest metadata</p> <p>In addition, a ModuleRef within the module which refers to <code>System.Private.CoreLib</code> may be used to serve as the ResolutionContext of a TypeRef within the manifest metadata. This will always refer to the module which contains the <code>System.Object</code> type.</p>"},{"location":"readytorun-format/#readytorunsectiontypeattributepresence-v31","title":"ReadyToRunSectionType.AttributePresence (v3.1+)","text":"<p>TODO: document attribute presence encoding</p> <p>Note: This is a per-assembly section. In single-file R2R files, it is pointed to directly by the main R2R header; in composite R2R files, each component module has its own attribute presence section pointed to by the <code>READYTORUN_SECTION_ASSEMBLIES_ENTRY</code> core header structure.</p>"},{"location":"readytorun-format/#readytorunsectiontypeinlininginfo2-v41","title":"ReadyToRunSectionType.InliningInfo2 (v4.1+)","text":"<p>The inlining information section captures what methods got inlined into other methods. It consists of a single Native Format Hashtable (described below).</p> <p>The entries in the hashtable are lists of inliners for each inlinee. One entry in the hashtable corresponds to one inlinee. The hashtable is hashed by hashcode of the module name XORed with inlinee RID.</p> <p>The entry of the hashtable is a counted sequence of compressed unsigned integers:</p> <ul> <li>RID of the inlinee shifted left by one bit. If the lowest bit is set, this is an inlinee from a foreign module. The module override index (as defined above) follows as another compressed unsigned integer in that case.</li> <li>RIDs of the inliners follow. They are encoded similarly to the way the inlinee is encoded (shifted left with the lowest bit indicating foreign RID). Instead of encoding the RID directly, RID delta (the difference between the previous RID and the current RID) is encoded. This allows better integer compression.</li> </ul> <p>Foreign RIDs are only present if a fragile inlining was allowed at compile time.</p> <p>TODO: It remains to be seen whether <code>DelayLoadMethodCallThunks</code> and / or <code>InliningInfo</code> also require changes specific to the composite R2R file format.</p>"},{"location":"readytorun-format/#readytorunsectiontypecomponentassemblies-v41","title":"ReadyToRunSectionType.ComponentAssemblies (v4.1+)","text":"<p>This image-wide section is only present in the main R2R header of composite R2R files. It is an array of the entries <code>READYTORUN_SECTION_ASSEMBLIES_ENTRY</code> parallel to the indices in the manifest metadata AssemblyRef table in the sense that it's a linear table where the row indices correspond to the equivalent AssemblyRef indices. Just like in the AssemblyRef ECMA 335 table, the indexing is 1-based (the first entry in the table corresponds to index 1).</p> <pre><code>struct READYTORUN_SECTION_ASSEMBLIES_ENTRY\n{\n    IMAGE_DATA_DIRECTORY CorHeader;        // Input MSIL metadata COR header (for composite R2R images with embedded MSIL metadata)\n    IMAGE_DATA_DIRECTORY ReadyToRunHeader; // READYTORUN_CORE_HEADER of the assembly in question\n};\n</code></pre>"},{"location":"readytorun-format/#readytorunsectiontypeownercompositeexecutable-v41","title":"ReadyToRunSectionType.OwnerCompositeExecutable (v4.1+)","text":"<p>For composite R2R executables with standalone MSIL, the MSIL files are rewritten during compilation by receiving a formal ReadyToRun header with the appropriate signature and major / minor version pair; in <code>Flags</code>, it has the <code>READYTORUN_FLAG_COMPONENT</code> bit set and its section list only contains the <code>OwnerCompositeExecutable</code> section that contains a UTF-8 string encoding the file name of the composite R2R executable this MSIL belongs to with extension (without path). Runtime uses this information to locate the composite R2R executable with the compiled native code when loading the MSIL.</p>"},{"location":"readytorun-format/#readytorunsectiontypepgoinstrumentationdata-v52","title":"ReadyToRunSectionType.PgoInstrumentationData (v5.2+)","text":"<p>TODO: document PGO instrumentation data</p>"},{"location":"readytorun-format/#readytorunsectiontypemanifestassemblymvids-v53","title":"ReadyToRunSectionType.ManifestAssemblyMvids (v5.3+)","text":"<p>This section is a binary array of 16-byte MVID records, one for each assembly in the manifest metadata. Number of assemblies stored in the manifest metadata is equal to the number of MVID records in the array. MVID records are used at runtime to verify that the assemblies loaded match those referenced by the manifest metadata representing the versioning bubble.</p>"},{"location":"readytorun-format/#readytorunsectiontypecrossmoduleinlineinfo-v63","title":"ReadyToRunSectionType.CrossModuleInlineInfo (v6.3+)","text":"<p>The inlining information section captures what methods got inlined into other methods. It consists of a single Native Format Hashtable (described below).</p> <p>The entries in the hashtable are lists of inliners for each inlinee. One entry in the hashtable corresponds to one inlinee. The hashtable is hashed with the version resilient hashcode of the uninstantiated methoddef inlinee.</p> <p>The entry of the hashtable is a counted sequence of compressed unsigned integers which begins with an InlineeIndex which combines a 30 bit index with 2 bits of flags which how the sequence of inliners shall be parsed and what table is to be indexed into to find the inlinee.</p> <ul> <li>InlineeIndex</li> <li> <p>Index with 2 flags field in lowest 2 bits to define the inlinee</p> <ul> <li>If (flags &amp; 1) == 0 then index is a MethodDef RID, and if the module is a composite image, a module index of the method follows</li> <li>If (flags &amp; 1) == 1, then index is an index into the ILBody import section</li> <li>If (flags &amp; 2) == 0 then inliner list is:</li> <li>Inliner RID deltas - See definition below</li> <li>if (flags &amp; 2) == 2 then what follows is:</li> <li>count of delta encoded indices into the ILBody import section</li> <li>the sequence of delta encoded indices into the first import section with a type of READYTORUN_IMPORT_SECTION_TYPE_ILBODYFIXUPS</li> <li>Inliner RID deltas - See definition below</li> </ul> </li> <li> <p>Inliner RID deltas (for multi-module version bubble images specified by the module having the READYTORUN_FLAG_MULTIMODULE_VERSION_BUBBLE flag set)</p> </li> <li>a sequence of inliner RID deltas with flag in the lowest bit</li> <li>if flag is set, the inliner RID is followed by a module ID</li> <li>otherwise the module is the same as the module of the inlinee method</li> <li>Inliner RID deltas (for single module version bubble images)</li> <li>a sequence of inliner RID deltas</li> </ul> <p>This section may be included in addition to a InliningInfo2 section.</p>"},{"location":"readytorun-format/#readytorunsectiontypehotcoldmap-v80","title":"ReadyToRunSectionType.HotColdMap (v8.0+)","text":"<p>In ReadyToRun 8.0+, the format supports splitting a method into hot and cold parts so that they are not located together. This hot-cold map section captures the information about how methods are split so that the runtime can locate them for various services.</p> <p>For every method that is split, there is a single entry in the section. Each entry has two unsigned 32-bit integers. The first integer is the runtime function index of the cold part and the second integer is the runtime function index of the hot part.</p> <p>The methods in this table are sorted by their hot part runtime function indices, which are also sorted by their cold part runtime function indices because we always emit the cold part in the same order as the hot parts, or by their RVAs because the runtime function table itself is sorted by the RVAs.</p> <p>This section may not exist if no method is split - this happens when the <code>--hot-cold-splitting</code> flag is not specified during compilation, or the compiler decides it should not split any methods.</p>"},{"location":"readytorun-format/#readytorunsectiontypemethodisgenericmap-v90","title":"ReadyToRunSectionType.MethodIsGenericMap (v9.0+)","text":"<p>This optional section holds a bit vector to indicate if the MethodDefs contained within the assembly have generic parameters or not. This allows determining if a method is generic or not by querying a bit vector (which is fast, and efficient) as opposed to examining the GenericParameter table, or the signature of the Method.</p> <p>The section begins with a single 32 bit integer indicating the number of bits in the bit vector. Following that integer is the actual bit vector of all of the data. The data is grouped into 8 bit bytes, where the least significant bit of the byte is the bit which represents the lowest MethodDef.</p> <p>For instance, the first byte in the bit vector represents the MethodDefs 06000001 to 06000008, and the least signficant bit of that first byte is the bit representing the IsGeneric bit for MethodDef 06000001.</p>"},{"location":"readytorun-format/#readytorunsectiontypeenclosingtypemap-v90","title":"ReadyToRunSectionType.EnclosingTypeMap (v9.0+)","text":"<p>This optional section allows for efficient O(1) lookup from the enclosed type to the type which encloses it without requiring the binary search that is necessary if using the ECMA 335 defined NestedClass table (which encodes exactly the same information). This section may only be included in the assembly if the assembly has fewer than 0xFFFE types defined within it.</p> <p>The structure of this section is: A single 16 bit unsigned integer listing the count of entries in the map. This count is followed by a 16 bit unsigned integer for each TypeDef defined in the assembly. This typedef is the RID of the enclosing type, or 0 if the typedef is not enclosed by another type.</p>"},{"location":"readytorun-format/#readytorunsectiontypetypegenericinfomap-v90","title":"ReadyToRunSectionType.TypeGenericInfoMap (v9.0+)","text":"<p>This optional section represents a condensed view of some generic details about types. This can make it more efficient to load types.</p> <p>The structure of this section is: A single 32 bit integer representing the number of entries in the map followed by a series of 4 bit entries, one per type. These 4 bit entries are grouped into bytes, where each byte holds 2 entries, and the entry in the most significant 4 bits of the byte is the entry representing a lower TypeDef RID.</p> <p>TypeGenericInfoMap entries have 4 bits representing 3 different sets of information.</p> <ol> <li>What is the count of generic parameters (0, 1, 2, MoreThanTwo) (This is represented in the least significant 2 bits of the TypeGenericInfoMap entry)</li> <li>Are there any constraints on the generic parameters? (This is the 3rd bit of the entry)</li> <li>Do any of the generic parameters have co or contra variance? (This is the 4th bit of the entry)</li> </ol>"},{"location":"readytorun-format/#native-format","title":"Native Format","text":"<p>Native format is set of encoding patterns that allow persisting type system data in a binary format that is efficient for runtime access - both in working set and CPU cycles. (Originally designed for and extensively used by .NET Native.)</p>"},{"location":"readytorun-format/#integer-encoding","title":"Integer encoding","text":"<p>Native format uses a variable length encoding scheme for signed and unsigned numbers. The low bits of the first byte of the encoding specify the number of following bytes as follows:</p> <ul> <li><code>xxxxxxx0</code> (i.e. the least significant bit is 0): no more bytes follow. Shift the byte one bit right, and   sign or zero extend for signed and unsigned number, respectively.</li> <li><code>xxxxxx01</code>: one more byte follows. Build a 16-bit number from the two bytes read (little-endian   order), shift it right by 2 bits, then sign or zero extend.</li> <li><code>xxxxx011</code>: two more bytes follow. Build a 24-bit number from the three bytes read (little-endian   order), shift it right by 3 bits, then sign or zero extend.</li> <li><code>xxxx0111</code>: three more bytes follow. Build a 32-bit number from the four bytes read, then sign or   zero extend</li> <li><code>xxxx1111</code>: four more bytes follow. Discard the first byte, build the signed or unsigned number   from the following four bytes (again little-endian order).</li> </ul> <p>Examples: * the unsigned number 12 (<code>0x0000000c</code>) would be expressed as the single byte <code>0x18</code>. * The unsigned number 1000 (<code>0x000003e8</code>) would be expressed as the two bytes <code>0xa1, 0x0f</code></p>"},{"location":"readytorun-format/#sparse-array","title":"Sparse Array","text":"<p>The NativeArray provides O(1) indexed access while maintaining compact storage through null element compression (empty blocks share storage) and variable-sized offset encoding (adapts to data size).</p> <p>The array is made up of three parts, the header, block index, and the blocks.</p> <p>The header is a variable encoded value where: - Bits 0-1: Entry index size   - 0 = uint8 offsets   - 1 = uint16 offsets   - 2 = uint32 offsets - Bits 2-31: Number of elements in the array</p> <p>The block index immediately follows the header in memory and consists of one offset entry per block (dynamic size encoded in the header), where each entry points to the location of a data block relative to the start of the block index section. The array uses a maximum block size of 16 elements, the block index effectively maps every group of 16 consecutive array indices to their corresponding data blocks.</p> <p>The following the block index are the actual data blocks. These are made up of two types of nodes. Tree nodes and Data nodes.</p> <p>Tree nodes are made up of a variable length encoded uint where: - Bit 0: If set, the node has a lower index child - Bit 1: If set, the node has a higher index child - Bits 2-31: Shifted relative offset of higher index child</p> <p>Data nodes contain the user defined data.</p> <p>Since each block has at most 16 elements, they have a depth of <code>4</code>.</p>"},{"location":"readytorun-format/#lookup-algorithm-steps","title":"Lookup Algorithm Steps","text":"<p>Step 1: Read the Header - Decode the variable-length encoded header value from the array - Extract the entry index size from bits 0-1 (0=uint8, 1=uint16, 2=uint32 offsets) - Extract the total number of elements from bits 2-31 by right-shifting the header value by 2 bits - Use this information to determine how to interpret the block index entries and validate array bounds</p> <p>Step 2: Calculate Block Offset - Determine the block index <code>blockIndex</code> containing the target element by dividing the index by the block size (16). - Calculate the memory location containing the block offset <code>pBlockOffset = baseOffset + entrySize * blockIndex</code> where <code>baseOffset</code> is the address immediately following the header and <code>entrySize</code> is determined by the low bits of the header. - Read the block offset <code>blockOffset</code> from the block index table using the calculated <code>pBlockOffset</code> and entry size determined by the header. - Add the <code>baseOffset</code> to convert the relative <code>blockOffset</code> to an absolute position.</p> <p>Step 3: Initialize Tree Navigation - Using the <code>blockOffset</code> calculated above, begin traversal at the root of the block's binary tree structure</p> <p>Step 4: Navigate Binary Tree For each level of the tree (iterating through bit positions 8, 4, 2, 1):</p> <p>Step 4a: Read Node Descriptor - Decode the current node's control value, which contains navigation flags and child offset information - Extract flags indicating the presence of left and right child nodes - Extract the relative offset to the right child node (if present)</p> <p>Step 4b: Determine Navigation Direction - Test the current bit position against the target index - If the bit is set in the target index, attempt to navigate to the right child - If the bit is clear in the target index, attempt to navigate to the left child</p> <p>Step 4c: Follow Navigation Path - If the desired child exists (indicated by the appropriate flag), update the current position - For right child navigation, add the encoded offset to the current position - For left child navigation, move to the position immediately following the current node - Continue to the next bit level if navigation was successful</p> <p>Step 5: Return Element Location - Upon successful traversal, return the final offset position which points to the stored data. - If traversal is not successful (child node does not exist), the element can not be found in the array and return a failure status.</p>"},{"location":"readytorun-format/#hashtable","title":"Hashtable","text":"<p>Conceptually, a native hash table is a header that describe the dimensions of the table, a table that maps hash values of the keys to buckets followed with a list of buckets that store the values. These three things are stored consecutively in the format.</p> <p>To make look up fast, the number of buckets is always a power of 2. The table is simply a sequence of <code>(1 + number of buckets)</code> cells, for the first <code>(number of buckets)</code> cells, its stores the offset of the bucket list from the beginning of the whole native hash table. The last cell stores the offset to the end of the buckets. Entries are mapped to buckets using <code>x</code> lowest bits of the hash not in the lowest byte where <code>2^x = (number of buckets)</code>. For example, if <code>x=2</code> the following bits marked with <code>X</code> would be used in a 32-bit hash <code>b00000000_00000000_000000XX_00000000</code>.</p> <p>Physically, the header is a single byte. The most significant six bits is used to store the number of buckets in its base-2 logarithm. The remaining two bits are used for storing the entry size, as explained below:</p> <p>Because the offsets to the bucket lists are often small numbers, the table cells are variable sized. It could be either 1 byte, 2 bytes or 4 bytes. The three cases are described with two bits. <code>00</code> means it is one byte, <code>01</code> means it is two bytes and <code>10</code> means it is four bytes.</p> <p>The remaining data are the entries. The entries has only the least significant byte of the hash code, followed by the offset to the actual object stored in the hash table. The entries are sorted by hash code.</p> <p>To perform a lookup, one starts with reading the header, computing the hash code, using the number of buckets to determine the number of bits to mask away from the hash code, look it up in the table using the right pointer size, find the bucket list, find the next bucket list (or the end of the table) so that we know where to stop, search the entries in that list and then we will find the object if we have a hit, or we have a miss.</p> <p>To enumerate all the values, simply walk from the first entry and go all the way to the end of the hash table.</p> <p>To see this in action, we can take a look at the following example, with these objects placed in the native hash table.</p> Object HashCode P 0x1231 Q 0x1232 R 0x1234 S 0x1338 <p>Suppose we decided to have only two buckets, then only the 9th bit will be used to index the table, the whole hash table will look like this:</p> Part Offset Content Meaning Header 0 0x04 This is the header, the least significant bit is <code>00</code>, therefore the table cell is just one byte. The most significant six bit represents 1, which means the number of buckets is 2^1 = 2. Table 1 0x04 This is the representation of the unsigned integer 4, which correspond to the offset of the bucket correspond to hash code <code>0</code>. Table 2 0x0A This is the representation of the unsigned integer 10, which correspond to the offset of the bucket correspond to hash code <code>1</code>. Table 3 0x0C This is the representation of the unsigned integer 12, which correspond to the offset of the end of the whole hash table. Bucket1 4 0x31 This is the least significant byte of the hash code of P Bucket1 5 P This should be the offset to the object P Bucket1 6 0x32 This is the least significant byte of the hash code of Q Bucket1 7 Q This should be the offset to the object Q Bucket1 8 0x34 This is the least significant byte of the hash code of R Bucket1 9 R This should be the offset to the object R Bucket2 10 0x38 This is the least significant byte of the hash code of S Bucket2 11 S This should be the offset to the object S"},{"location":"readytorun-format/#helper-calls","title":"Helper calls","text":"<p>List of helper calls supported by READYTORUN_FIXUP_Helper:</p> <pre><code>enum ReadyToRunHelper\n{\n    READYTORUN_HELPER_Invalid                   = 0x00,\n\n    // Not a real helper - handle to current module passed to delay load helpers.\n    READYTORUN_HELPER_Module                    = 0x01,\n    READYTORUN_HELPER_GSCookie                  = 0x02,\n\n    //\n    // Delay load helpers\n    //\n\n    // All delay load helpers use custom calling convention:\n    // - scratch register - address of indirection cell. 0 = address is inferred from callsite.\n    // - stack - section index, module handle\n    READYTORUN_HELPER_DelayLoad_MethodCall      = 0x08,\n\n    READYTORUN_HELPER_DelayLoad_Helper          = 0x10,\n    READYTORUN_HELPER_DelayLoad_Helper_Obj      = 0x11,\n    READYTORUN_HELPER_DelayLoad_Helper_ObjObj   = 0x12,\n\n    // JIT helpers\n\n    // Exception handling helpers\n    READYTORUN_HELPER_Throw                     = 0x20,\n    READYTORUN_HELPER_Rethrow                   = 0x21,\n    READYTORUN_HELPER_Overflow                  = 0x22,\n    READYTORUN_HELPER_RngChkFail                = 0x23,\n    READYTORUN_HELPER_FailFast                  = 0x24,\n    READYTORUN_HELPER_ThrowNullRef              = 0x25,\n    READYTORUN_HELPER_ThrowDivZero              = 0x26,\n\n    // Write barriers\n    READYTORUN_HELPER_WriteBarrier              = 0x30,\n    READYTORUN_HELPER_CheckedWriteBarrier       = 0x31,\n    READYTORUN_HELPER_ByRefWriteBarrier         = 0x32,\n\n    // Array helpers\n    READYTORUN_HELPER_Stelem_Ref                = 0x38,\n    READYTORUN_HELPER_Ldelema_Ref               = 0x39,\n\n    READYTORUN_HELPER_MemSet                    = 0x40,\n    READYTORUN_HELPER_MemCpy                    = 0x41,\n\n    // Get string handle lazily\n    READYTORUN_HELPER_GetString                 = 0x50,\n\n    // Used by /Tuning for Profile optimizations\n    READYTORUN_HELPER_LogMethodEnter            = 0x51, // Unused since READYTORUN_MAJOR_VERSION 10.0\n\n    // Reflection helpers\n    READYTORUN_HELPER_GetRuntimeTypeHandle      = 0x54,\n    READYTORUN_HELPER_GetRuntimeMethodHandle    = 0x55,\n    READYTORUN_HELPER_GetRuntimeFieldHandle     = 0x56,\n\n    READYTORUN_HELPER_Box                       = 0x58,\n    READYTORUN_HELPER_Box_Nullable              = 0x59,\n    READYTORUN_HELPER_Unbox                     = 0x5A,\n    READYTORUN_HELPER_Unbox_Nullable            = 0x5B,\n    READYTORUN_HELPER_NewMultiDimArr            = 0x5C,\n\n    // Helpers used with generic handle lookup cases\n    READYTORUN_HELPER_NewObject                 = 0x60,\n    READYTORUN_HELPER_NewArray                  = 0x61,\n    READYTORUN_HELPER_CheckCastAny              = 0x62,\n    READYTORUN_HELPER_CheckInstanceAny          = 0x63,\n    READYTORUN_HELPER_GenericGcStaticBase       = 0x64,\n    READYTORUN_HELPER_GenericNonGcStaticBase    = 0x65,\n    READYTORUN_HELPER_GenericGcTlsBase          = 0x66,\n    READYTORUN_HELPER_GenericNonGcTlsBase       = 0x67,\n    READYTORUN_HELPER_VirtualFuncPtr            = 0x68,\n    READYTORUN_HELPER_IsInstanceOfException     = 0x69,\n    READYTORUN_HELPER_NewMaybeFrozenArray       = 0x6A,\n    READYTORUN_HELPER_NewMaybeFrozenObject      = 0x6B,\n\n    // Long mul/div/shift ops\n    READYTORUN_HELPER_LMul                      = 0xC0,\n    READYTORUN_HELPER_LMulOfv                   = 0xC1,\n    READYTORUN_HELPER_ULMulOvf                  = 0xC2,\n    READYTORUN_HELPER_LDiv                      = 0xC3,\n    READYTORUN_HELPER_LMod                      = 0xC4,\n    READYTORUN_HELPER_ULDiv                     = 0xC5,\n    READYTORUN_HELPER_ULMod                     = 0xC6,\n    READYTORUN_HELPER_LLsh                      = 0xC7,\n    READYTORUN_HELPER_LRsh                      = 0xC8,\n    READYTORUN_HELPER_LRsz                      = 0xC9,\n    READYTORUN_HELPER_Lng2Dbl                   = 0xCA,\n    READYTORUN_HELPER_ULng2Dbl                  = 0xCB,\n\n    // 32-bit division helpers\n    READYTORUN_HELPER_Div                       = 0xCC,\n    READYTORUN_HELPER_Mod                       = 0xCD,\n    READYTORUN_HELPER_UDiv                      = 0xCE,\n    READYTORUN_HELPER_UMod                      = 0xCF,\n\n    // Floating point conversions\n    READYTORUN_HELPER_Dbl2Int                   = 0xD0, // Unused since READYTORUN_MAJOR_VERSION 15.0\n    READYTORUN_HELPER_Dbl2IntOvf                = 0xD1,\n    READYTORUN_HELPER_Dbl2Lng                   = 0xD2,\n    READYTORUN_HELPER_Dbl2LngOvf                = 0xD3,\n    READYTORUN_HELPER_Dbl2UInt                  = 0xD4, // Unused since READYTORUN_MAJOR_VERSION 15.0\n    READYTORUN_HELPER_Dbl2UIntOvf               = 0xD5,\n    READYTORUN_HELPER_Dbl2ULng                  = 0xD6,\n    READYTORUN_HELPER_Dbl2ULngOvf               = 0xD7,\n    READYTORUN_HELPER_Lng2Flt                   = 0xD8,\n    READYTORUN_HELPER_ULng2Flt                  = 0xD9,\n\n    // Floating point ops\n    READYTORUN_HELPER_DblRem                    = 0xE0,\n    READYTORUN_HELPER_FltRem                    = 0xE1,\n    READYTORUN_HELPER_DblRound                  = 0xE2, // Unused since READYTORUN_MAJOR_VERSION 10.0\n    READYTORUN_HELPER_FltRound                  = 0xE3, // Unused since READYTORUN_MAJOR_VERSION 10.0\n\n#ifndef _TARGET_X86_\n    // Personality routines\n    READYTORUN_HELPER_PersonalityRoutine        = 0xF0,\n    READYTORUN_HELPER_PersonalityRoutineFilterFunclet = 0xF1,\n#endif\n\n    //\n    // Deprecated/legacy\n    //\n\n    // JIT32 x86-specific write barriers\n    READYTORUN_HELPER_WriteBarrier_EAX          = 0x100,\n    READYTORUN_HELPER_WriteBarrier_EBX          = 0x101,\n    READYTORUN_HELPER_WriteBarrier_ECX          = 0x102,\n    READYTORUN_HELPER_WriteBarrier_ESI          = 0x103,\n    READYTORUN_HELPER_WriteBarrier_EDI          = 0x104,\n    READYTORUN_HELPER_WriteBarrier_EBP          = 0x105,\n    READYTORUN_HELPER_CheckedWriteBarrier_EAX   = 0x106,\n    READYTORUN_HELPER_CheckedWriteBarrier_EBX   = 0x107,\n    READYTORUN_HELPER_CheckedWriteBarrier_ECX   = 0x108,\n    READYTORUN_HELPER_CheckedWriteBarrier_ESI   = 0x109,\n    READYTORUN_HELPER_CheckedWriteBarrier_EDI   = 0x10A,\n    READYTORUN_HELPER_CheckedWriteBarrier_EBP   = 0x10B,\n\n    // JIT32 x86-specific exception handling\n    READYTORUN_HELPER_EndCatch                  = 0x110,\n};\n</code></pre>"},{"location":"readytorun-format/#references","title":"References","text":"<p>ECMA-335</p>"},{"location":"readytorun-overview/","title":"Managed Executables with Native Code","text":""},{"location":"readytorun-overview/#motivation","title":"Motivation","text":"<p>Since shipping the .NET Runtime over 10 years ago, there has only been one file format which can be used to distribute and deploy managed code components: the CLI file format. This format expresses all execution as machine independent intermediate language (IL) which must either be interpreted or compiled to native code sometime before the code is run. This lack of an efficient, directly executable file format is a very significant difference between unmanaged and managed code, and has become more and more problematic over time. Problems include:</p> <ul> <li>Native code generation takes a relatively long time and consumes power.</li> <li>For security / tamper-resistance, there is a very strong desire to validate any native code that gets run (e.g. code is signed).</li> <li>Existing native codegen strategies produce brittle code such that when the runtime or low level framework is updated, all native code is invalidated, which forces the need for recompilation of all that code.</li> </ul> <p>All of these problems and complexity are things that unmanaged code simply avoids. They are avoided because unmanaged code has a format with the following characteristics:</p> <ul> <li>The executable format can be efficiently executed directly. Very little needs to be updated at runtime (binding some external references) to prepare for execution. What does need to be updated can be done lazily.</li> <li>As long as a set of known versioning rules are followed, version compatible changes in one executable do not affect any other executable (you can update your executables independently of one another).</li> <li>The format is clearly defined, which allows variety of compilers to produce it.</li> </ul> <p>In this proposal we attack this discrepancy between managed and unmanaged code head on: by giving managed code a file format that has the characteristics of unmanaged code listed above. Having such a format brings managed up to at least parity with unmanaged code with respect to deployment characteristics. This is a huge win!</p>"},{"location":"readytorun-overview/#problem-constraints","title":"Problem Constraints","text":"<p>The .NET Runtime has had a native code story (NGEN) for a long time. However what is being proposed here is architecturally different than NGEN. NGEN is fundamentally a cache (it is optional and only affects the performance of the app) and thus the fragility of the images was simply not a concern. If anything changes, the NGEN image is discarded and regenerated. On the other hand:</p> <p>A native file format carries a strong guarantee that the file will continue to run despite updates and improvements to the runtime or framework.</p> <p>Most of this proposal is the details of achieving this guarantee while giving up as little performance as possible.</p> <p>This compatibility guarantee means that, unlike NGEN, anything you place in the file is a liability because you will have to support it in all future runtimes. This drives a desire to be 'minimalist' and only place things into the format that really need to be there. For everything we place into the format we have to believe either:</p> <ol> <li>It is very unlikely to change (in particular we have not changed it over the current life of CLR)</li> <li>We have a scheme in which we can create future runtimes that could support both old and new format efficiently (both in terms of runtime efficiency and engineering complexity).</li> </ol> <p>Each feature of the file format needs to have an answer to the question of how it versions, and we will be trying to be as 'minimalist' as possible.</p>"},{"location":"readytorun-overview/#solution-outline","title":"Solution Outline","text":"<p>As mentioned, while NGEN is a native file format, it is not an appropriate starting point for this proposal because it is too fragile.</p> <p>Looking carefully at the CLI file format shows that it is really 'not that bad' as a starting point. At its heart CLI is a set of database-like tables (one for types, methods, fields, etc.), which have entries that point at variable-length things (e.g. method names, signatures, method bodies). Thus CLI is 'pay for play' and since it is already public and version resilient, there is very little downside to including it in the format. By including it we also get the following useful properties:</p> <ul> <li>Immediate support for all features of the runtime (at least for files that include complete CLI within them)</li> <li>The option to only add the 'most important' data required to support fast, direct execution. Everything else can be left in CLI format and use the CLI code paths. This is quite valuable given our desire to be minimalist in augmenting the format.</li> </ul> <p>Moreover there is an 'obvious' way of extending the CIL file to include the additional data we need. A CLI file has a well-defined header structure, and that header already has a field that can point of to 'additional information'. This is used today in NGEN images. We would use this same technique to allow the existing CLI format to include a new 'Native Header' that would then point at any additional information needed to support fast, direct execution.</p> <p>The most important parts of this extra information include:</p> <ol> <li>Native code for the methods (as well as a way of referencing things outside the module)</li> <li>Garbage Collection (GC) information for each method that allows you to know what values in registers and on the stack are pointers to the GC heap wherever a GC is allowed.</li> <li>Exception handling (EH) tables that allow an exception handler to be found when an exception is thrown.</li> <li>A table that allows the GC and EH to be found given just the current instruction pointer (IP) within the code. (IP map).</li> <li>A table that links the information in the metadata to the corresponding native structure.</li> </ol> <p>That is, we need something to link the world of metadata to the world of native. We can't eliminate meta-data completely because we want to support existing functionality. In particular we need to be able to support having other CLI images refer to types, methods and fields in this image. They will do so by referencing the information in the metadata, but once they find the target in the metadata, we will need to find the actual native code or type information corresponding to that meta-data entry. This is the purpose of the additional table. Effectively, this table is the 'export' mechanism for managed references.</p> <p>Some of this information can be omitted or stored in more efficient form, e.g.:</p> <ul> <li>The garbage collection information can be omitted for environments with conservative garbage collection, such as IL2CPP.</li> <li>The full metadata information is not strictly required for 'private' methods or types so it is possible to strip it from the CLI image.</li> <li>The metadata can be stored in more efficient form, such as the .NET Native metadata format.</li> <li>The platform native executable format (ELF, Mach-O) can be used as envelope instead of PE to take advantage of platform OS loader.</li> </ul>"},{"location":"readytorun-overview/#definition-of-version-compatibility-for-native-code","title":"Definition of Version Compatibility for Native Code","text":"<p>Even for IL or unmanaged native code, there are limits to what compatible changes can be made. For example, deleting a public method is sure to be an incompatible change for any extern code using that method.</p> <p>Since CIL already has a set of compatibility rules, ideally the native format would have the same set of compatibility rules as CIL. Unfortunately, that is difficult to do efficiently in all cases. In those cases we have multiple choices:</p> <ol> <li>Change the compatibility rules to disallow some changes</li> <li>Never generate native structures for the problematic cases (fall back to CIL techniques)</li> <li>Generate native structures for the problematic cases, but use them only if there was no incompatible change made</li> <li>Generate less efficient native code that is resilient</li> </ol> <p>Generally the hardest versioning issues revolve around:</p> <ul> <li>Value types (structs)</li> <li>Generic methods over value types (structs)</li> </ul> <p>These are problematic because value classes are valuable precisely because they have less overhead than classes. They achieve this value by being 'inlined' where they are used. This makes the code generated for value classes very fragile with respect to any changes to the value class's layout, which is bad for resilience. Generics over structs have a similar issue.</p> <p>Thus this proposal does not suggest that we try to solve the problem of having version resilience in the presence of layout changes to value types. Instead we suggest creating a new compatibility rule:</p> <p>It is a breaking change to change the number or type of any (including private) fields of a public value type (struct). However if the struct is non-public (that is internal), and not reachable from any nesting of value type fields in any public value type, then the restriction does not apply.</p> <p>This is a compatibility that is not present for CIL. All other changes allowed by CIL can be allowed by native code without prohibitive penalty. In particular the following changes are allowed:</p> <ol> <li>Adding instance and static fields to reference classes</li> <li>Adding static fields to a value class.</li> <li>Adding virtual, instance or static methods to a reference or value class</li> <li>Changing existing methods (assuming the semantics is compatible).</li> <li>Adding new classes.</li> </ol>"},{"location":"readytorun-overview/#version-bubbles","title":"Version Bubbles","text":"<p>When changes to managed code are made, we have to make sure that all the artifacts in a native code image only depend on information in other modules that cannot change without breaking the compatibility rules. What is interesting about this problem is that the constraints only come into play when you cross module boundaries. As an example, consider the issue of inlining of method bodies. If module A would inline a method from Module B, that would break our desired versioning property because now if that method in module B changes, there is code in Module A that would need to be updated (which we do not wish to do). Thus inlining is illegal across modules. Inlining within a module, however, is still perfectly fine.</p> <p>Thus in general the performance impact of versioning decreases as module size increases because there are fewer cross-module references. We can take advantage of this observation by defining something called a version bubble. A version bubble is a set of DLLs that we are willing to update as a set. From a versioning perspective, this set of DLLs is a single module. Inlining and other cross-module optimizations are allowed within a version bubble.</p> <p>It is worth reiterating the general principle covered in this section</p> <p>Code of methods and types that do NOT span version bubbles does NOT pay a performance penalty.</p> <p>This principle is important because it means that only a fraction (for most apps a small fraction) of all code will pay any performance penalties we discuss in the sections that follow.</p> <p>The extreme case is where the entire application is a single version bubble. This configuration does not need to pay any performance penalty for respecting versioning rules. It still benefits from a clearly defined file format and runtime contract that are the essential part of this proposal.</p>"},{"location":"readytorun-overview/#runtime-versioning","title":"Runtime Versioning","text":"<p>The runtime versioning is solved using different techniques because the runtime is responsible for interpretation of the binary format.</p> <p>To allow changes in the runtime, we simply require that the new runtime handle all old formats as well as the new format. The 'main defense' in the design of the file format is having version numbers on important structures so that the runtime has the option of supporting a new version of that structure as well as the old version unambiguously by checking the version number. Fundamentally, we are forcing the developers of the runtime to be aware of this constraint and code and test accordingly.</p>"},{"location":"readytorun-overview/#restrictions-on-runtime-evolution","title":"Restrictions on Runtime Evolution","text":"<p>As mentioned previously, when designing for version compatibility we have the choice of either simply disallowing a change (by changing the breaking change rules), or ensuring that the format is sufficiently flexible to allow evolution. For example, for managed code we have opted to disallow changes to value type (struct) layout so that codegen for structs can be efficient. In addition, the design also includes a small number of restrictions that affect the flexibility of evolving the runtime itself. They are:</p> <ul> <li>The field layout of <code>System.Object</code> cannot change. (First, there is a pointer sized field for type information and then the other fields.)</li> <li>The field layout of arrays cannot change. (First, there is a pointer sized field for type information, and then a pointer sized field for the length. After these fields is the array data, packed using existing alignment rules.)</li> <li>The field layout of <code>System.String</code> cannot change. (First, there is a pointer sized field for type information, and then a int32 sized field for the length. After these fields is the zero terminated string data in UTF16 encoding.)</li> </ul> <p>These restrictions were made because the likelihood of ever wanting to change these restrictions is low, and the performance cost not having these assumptions is high. If we did not assume the field layout of <code>System.Object</code> never changes, then every field fetch object outside the framework itself would span a version bubble and pay a penalty. Similarly if we don't assume the field layout for arrays or strings, then every access will pay a versioning penalty.</p>"},{"location":"readytorun-overview/#selective-use-of-the-jit","title":"Selective use of the JIT","text":"<p>One final point that is worth making is that selective use of the JIT compiler is another tool that can be used to avoid code quality penalties associated with version resilience, in environments where JITing is permitted. For example, assume that there is a hot user method that calls across a version bubble to a method that would a good candidate for inlining, but is not inlined because of versioning constraints. For such cases, we could have an attribute that indicates that a particular method should be compiled at runtime. Since the JIT compiler is free to generate fragile code, it can perform this inlining and thus the program steady-state performance improves. It is true that a startup time cost has been paid, but if the number of such 'hot' methods is small, the amount of JIT compilation (and thus its penalty) is not great. The point is that application developers can make this determination on a case by case basis. It is very easy for the runtime to support this capability.</p>"},{"location":"readytorun-overview/#version-resilient-native-code-generation","title":"Version Resilient Native Code Generation","text":"<p>Because our new native format starts with the current CLI format, we have the option of falling back to it whenever we wish to. Thus we can choose to add new parts to the format in chunks. In this section we talk about the 'native code' chunk. Here we discuss the parts of the format needed to emit native code for the bodies of 'ordinary' methods. Native images that have this addition information will not need to call the JIT compiler, but will still need to call the type loader to create types.</p> <p>It is useful to break the problem of generating version resilient native code by CIL instruction. Many CIL instructions (e.g. <code>ADD</code>, <code>MUL</code>, <code>LDLOC</code> ... naturally translate to native code in a version resilient ways. However CIL that deals with object model (e.g. <code>NEWOBJ</code>, <code>LDFLD</code>, etc) need special care as explained below. The descriptions below are roughly ordered in the performance priority in typical applications. Typically, each section will describe what code generation looks like when all information is within the version bubble, and then when the information crosses version bubbles. We use x64 as our native instruction set, applying the same strategy to other processor architectures is straightforward. We use the following trivial example to demonstrate the concepts</p> <pre><code>interface Intf\n{\n    void intfMethod();\n}\n\nclass BaseClass\n{\n    static int sField;\n    int iField;\n\n    public void iMethod()\n    {\n    }\n\n    public virtual void vMethod(BaseClass aC)\n    {\n    }\n}\n\nclass SubClass : BaseClass, Intf\n{\n    int subField;\n\n    public override void vMethod(BaseClass aC)\n    {\n    }\n\n    virtual void intfMethod()\n    {\n    }\n}\n</code></pre>"},{"location":"readytorun-overview/#instance-field-access-ldfld-stfld","title":"Instance Field access - LDFLD / STFLD","text":"<p>The CLR stores fields in the 'standard' way, so if RCX holds a BaseClass then</p> <pre><code>MOV RAX, [RCX + iField_Offset]\n</code></pre> <p>will fetch <code>iField</code> from this object. <code>iField_Offset</code> is a constant known at native code generation time. This is known at compile time only because we mandated that the field layout of <code>System.Object</code> is fixed, and thus the entire inheritance chain of <code>BaseClass</code> is in the version bubble. It's also true even when fields in <code>BaseClass</code> contain structs (even from outside the version bubble), because we have made it a breaking change to modify the field layout of any public value type. Thus for types whose inheritance hierarchy does not span a version bubble, field fetch is as it always was.</p> <p>To consider the inter-bubble case, assume that <code>SubClass</code> is defined in a different version bubble than BaseClass and we are fetching <code>subField</code>. The normal layout rules for classes require <code>subField</code> to come after all the fields of <code>BaseClass</code>. However <code>BaseClass</code> could change over time, so we can't wire in a literal constant anymore. Instead we require the following code</p> <pre><code>    MOV TMP, [SIZE_OF_BASECLASS]\n    MOV EAX, [RCX + TMP + subfield_OffsetInSubClass]\n\n .data // In the data section\n SIZE_OF_BASECLASS: UINT32 // One per EXTERN CLASS that is subclassed\n</code></pre> <p>Which simply assumes that a uint32 sized location has been reserved in the module and that it will be filled in with the size of <code>BaseClass</code> before this code is executed. Now a field fetch has one extra instruction, which fetches this size and that dynamic value is used to compute the field. This sequence is a great candidate for CSE (common sub-expression elimination) optimization when multiple fields of the same class are accessed by single method.</p> <p>A special attention needs to be given to alignment requirements of <code>SubClass</code>.</p>"},{"location":"readytorun-overview/#gc-write-barrier","title":"GC Write Barrier","text":"<p>The .NET GC is generational, which means that most GCs do not collect the whole heap, and instead only collect the 'new' part (which is much more likely to contain garbage). To do this it needs to know the set of roots that point into this 'new' part. This is what the GC write barrier does. Every time an object reference that lives in the GC heap is updated, bookkeeping code needs to be called to log that fact. Any fields whose values were updated are used as potential roots on these partial GCs. The important part here is that any field update of a GC reference must do this extra bookkeeping.</p> <p>The write barrier is implemented as a set of helper functions in the runtime. These functions have special calling conventions (they do not trash any registers). Thus these helpers act more like instructions than calls. The write barrier logic does not need to be changed to support versioning (it works fine the way it is).</p>"},{"location":"readytorun-overview/#initializing-the-field-size-information","title":"Initializing the field size information","text":"<p>A key observation is that you only need this overhead for each distinct class that inherits across a version bubble. Thus there is unlikely to be many slots like <code>SIZE_OF_BASECLASS</code>. Because there are likely to be few of them, the compiler can choose to simply initialize them at module load.</p> <p>Note that if you accessed an instance field of a class that was defined in another module, it is not the size that you need but the offset of a particular field. The code generated will be the same (in fact it will be simpler as no displacement is needed in the second instruction). Our coding guidelines strongly discourage public instance fields so this scenario is not particularly likely in practice (it will end up being a property call) but we can handle it in a natural way. Note also that even complex inheritance hierarchies that span multiple version bubbles are not a problem. In the end all you need is the final size of the base type. It might take a bit longer to compute during one time initialization, but that is the extent of the extra cost.</p>"},{"location":"readytorun-overview/#performance-impact","title":"Performance Impact","text":"<p>Clearly we have added an instruction and thus made the code bigger and more expensive to run. However what is also true is that the additional cost is small. The 'worst' case would be if this field fetch was in a tight loop. To measure this we created a linked list element which inherited across a version bubble. The list was long (1K) but small enough to fit in the L1 cache. Even for this extreme example (which by the way is contrived, linked list nodes do not normally inherit in such a way), the extra cost was small (&lt; 1%).</p>"},{"location":"readytorun-overview/#null-checks","title":"Null checks","text":"<p>The managed runtime requires any field access on null instance pointer to generate null reference exception. To avoid inserting explicit null checks, the code generator assumes that memory access at addresses smaller than certain threshold (64k on Windows NT) will generate null reference exception. If we allowed unlimited growth of the base class for cross-version bubble inheritance hierarchies, this optimization would be no longer possible.</p> <p>To make this optimization possible, we will limit growth of the base class size for cross-module inheritance hierarchies. It is a new versioning restriction that does not exist in IL today.</p>"},{"location":"readytorun-overview/#non-virtual-method-calls-call","title":"Non-Virtual Method Calls - CALL","text":""},{"location":"readytorun-overview/#intra-module-call","title":"Intra-module call","text":"<p>If RCX holds a <code>BaseClass</code> and the caller of <code>iMethod</code> is in the same module as BaseClass then a method call is simple machine call instruction</p> <pre><code>    CALL ENTRY_IMETHOD\n</code></pre>"},{"location":"readytorun-overview/#inter-module-call","title":"Inter-module call","text":"<p>However if the caller is outside the module of BaseClass (even if it is in the same version bubble) we need to call it using an indirection</p> <pre><code>    CALL [PTR_IMETHOD]\n\n.data // In the data section\nPTR_IMETHOD: PTR = RUNTIME_ENTRY_FIXUP_METHOD // One per call TARGET.\n</code></pre> <p>Just like the field case, the pointer sized data slot <code>PTR_IMETHOD</code> must be fixed up to point at the entry point of <code>BaseClass.iMethod</code>. However unlike the field case, because we are fixing up a call (and not a MOV), we can have the call fix itself up lazily via standard delay loading mechanism. The delay loading mechanism often uses low-level tricks for maximum efficiency. Any low-level implementation of delay loading can be used as long as the resolution of the call target is left to the runtime.</p>"},{"location":"readytorun-overview/#retained-flexibility-for-runtime-innovation","title":"Retained Flexibility for runtime innovation","text":"<p>Note that it might seem that we have forever removed the possibility of innovating in the way we do SLOT fixup, since we 'burn' these details into the code generation and runtime helpers. However this is not true. What we have done is require that we support the current mechanism for doing such fixup. Thus we must always support a <code>RUNTIME_ENTRY_FIXUP_METHOD</code> helper. However we could devise a completely different scheme. All that would be required is that you use a new helper and keep the old one. Thus you can have a mix of old and new native code in the same process without issue.</p>"},{"location":"readytorun-overview/#calling-convention","title":"Calling Convention","text":"<p>The examples above did not have arguments and the issue of calling convention was not obvious. However it is certainly true that the native code at the call site does depend heavily on the calling convention and that convention must be agreed to between the caller and the callee at least for any particular caller-callee pair.</p> <p>The issue of calling convention is not specific to managed code and thus hardware manufacturers typically define a calling convention that tends to be used by all languages on the system (thus allowing interoperability). In fact for all platforms except x86, CLR attempts to follow the platform calling convention.</p> <p>Our understanding of the most appropriate managed convention evolved over time. Our experience tells us that it is worthwhile for implementation simplicity to always pass managed <code>this</code> pointer in the fixed register, even if the platform standard calling convention says otherwise.</p>"},{"location":"readytorun-overview/#managed-code-specific-conventions","title":"Managed Code Specific Conventions","text":"<p>In addition the normal conventions for passing parameters as well as the normal convention of having a hidden byref parameter for returning value types, CLR has a few managed code specific argument conventions:</p> <ol> <li>Shared generic code has a hidden parameter that represents the type parameters in some cases for methods on generic types and for generic methods.</li> <li>GC interactions with hidden return buffer. The convention for whether the hidden return buffer can be allocated in the GC heap, and thus needs to be written to using write barrier.</li> </ol> <p>These conventions would be codified as well.</p>"},{"location":"readytorun-overview/#performance-impact_1","title":"Performance Impact","text":"<p>Because it was already the case that methods outside the current module had to use an indirect call, versionability does not introduce more overhead for non-virtual method calls if inlining was not done. Thus the main cost of  making the native code version resilient is the requirement that no cross version bubble inlining can happen.</p> <p>The best solution to this problem is to avoid 'chatty' library designs (Unfortunately, <code>IEnumerable</code>, is such a chatty design, where each iteration does a <code>MoveNext</code> and <code>Current</code> property fetch). Another mitigation is the one mentioned previously: to allow clients of the library to selectively JIT compile some methods that make these chatty calls. Finally you can also use new custom <code>NonVersionableAttribute</code> attribute, which effectively changes the versioning contract to indicate that the library supplier has given up their right to change that method's body and thus it would be legal to inline.</p> <p>The proposal is to disallow cross-version bubble inlining by default, and selectively allow inlining for critical methods (by giving up the right to change the method).</p> <p>Experiments with disabled cross-module inlining with the selectively enabled inlining of critical methods showed no visible regression in ASP.NET throughput.</p>"},{"location":"readytorun-overview/#non-virtual-calls-as-the-baseline-solution-to-all-other-versioning-issues","title":"Non-Virtual calls as the baseline solution to all other versioning issues","text":"<p>It is important to observe that once you have a mechanism for doing non-virtual function calls in a version resilient way (by having an indirect CALL through a slot that can be fixed lazily at runtime, all other versioning problems can be solved in that way by calling back to the 'definer' module, and having the operation occur there instead. Issues associated with this technique</p> <ol> <li>You will pay the cost of a true indirection function call and return, as well as any argument setup cost. This cost may be visible in constructs that do not contain a call naturally, like fetching string literals or other constants. You may be able to get better performance from another technique (for example, we did so with instance field access).</li> <li>It introduces a lot of indirect calls. It is not friendly to systems that disallow on the fly code generation. A small helper stub has to be created at runtime in the most straightforward implementation, or there has to be a scheme how to pre-create or recycle the stubs.</li> <li>It requires that the defining assembly 'know' the operations that it is responsible for defining. In general this could be fixed by JIT compiling whatever is needed at runtime (where the needed operations are known), but JIT compiling is the kind of expensive operation that we are trying to avoid at runtime.</li> </ol> <p>So while there are limitations to the technique, it works very well on a broad class of issues, and is conceptually simple. Moreover, it has very nice simplicity on the caller side (a single indirect call). It is hard to get simpler than this. This simplicity means that you have wired very few assumptions into the caller which maximizes the versioning flexibility, which is another very nice attribute. Finally, this technique also allows generation of optimal code once the indirect call was made. This makes for a very flexible technique that we will use again and again.</p> <p>The runtime currently supports two mechanisms for virtual dispatch. One mechanism is called virtual stub dispatch (VSD). It is used when calling interface methods. The other is a variation on traditional vtable-based dispatch and it is used when a non-interface virtual is called. We first discuss the VSD approach.</p> <p>Assume that RCX holds a <code>Intf</code> then the call to <code>intfMethod()</code> would look like</p> <pre><code>     CALL [PTR_CALLSITE]\n.data // in the data section\nPTR_CALLSITE: INT_PTR = RUNTIME_ENTRY_FIXUP_METHOD // One per call SITE.\n</code></pre> <p>This looks same as the cross-module, non-virtual case, but there are important differences. Like the non-virtual case there is an indirect call through a pointer that lives in the module. However unlike the non-virtual case, there is one such slot per call site (not per target). What is in this slot is always guaranteed to get to the target (in this case to <code>Intf.intfMethod()</code>), but it is expected to change over time. It starts out pointing to a 'dumb' stub which simply calls a runtime helper that does the lookup (in likely a slow way). However, it can update the <code>PTR_CALLSITE</code> slot to a stub that efficiently dispatches to the interface for the type that actually occurred (the remaining details of stubbed based interface dispatch are not relevant to versioning).</p> <p>The above description is accurate for the current CLR implementation for interface dispatch. What's more, is that nothing needs to be changed about the code generation to make it version resilient. It 'just works' today. Thus interface dispatch is version resilient with no performance penalty.</p> <p>What's more, we can actually see VSD is really just a modification of the basic 'indirect call through updateable slot' technique that was used for non-virtual method dispatch. The main difference is that because the target depends on values that are not known until runtime (the type of the 'this' pointer), the 'fixup' function can never remove itself completely but must always check this runtime value and react accordingly (which might include fixing up the slot again). To make as likely as possible that the value in the fixup slot stabilizes, we create a fixup slot per call site (rather than per target).</p>"},{"location":"readytorun-overview/#vtable-dispatch","title":"Vtable Dispatch","text":"<p>The CLR current also supports doing virtual dispatch through function tables (vtables). Unfortunately, vtables have the same version resilience problem as fields. This problem can be fixed in a similar way, however unlike fields, the likelihood of having many cross bubble fixups is higher for methods than for instance fields. Further, unlike fields we already have a version resilient mechanism that works (VSD), so it would have to be better than that to be worth investing in. Vtable dispatch is only better than VSD for polymorphic call sites (where VSD needs to resort to a hash lookup). If we find we need to improve dispatch for this case we have some possible mitigations to try:</p> <ol> <li>If the polymorphism is limited, simply trying more cases before falling back to the hash table has been prototyped and seems to be a useful optimization.</li> <li>For high polymorphism case, we can explore the idea of dynamic vtable slots (where over time the virtual method a particular vtable slot holds can change). Before falling back to the hash table a virtual method could claim a vtable slot and now the dispatch of that method for any type will be fast.</li> </ol> <p>In short, because of the flexibility and natural version resilience of VSD, we propose determining if VSD can be 'fixed' before investing in making vtables version resilient and use VSD for all cross version bubble interface dispatch. This does not preclude using vtables within a version bubble, nor adding support for vtable based dispatch in the future if we determine that VSD dispatch can't be fixed.</p>"},{"location":"readytorun-overview/#object-creation-newobj-newarr","title":"Object Creation - NEWOBJ / NEWARR","text":"<p>Object allocation is always done by a helper call that allocates the uninitialized object memory (but does initialize the type information <code>MethodTable</code> pointer), followed by calling the class constructor. There are a number of different helpers depending on the characteristics of the type (does it have a finalizer, is it smaller than a certain size, ...).</p> <p>We will defer the choice of the helper to use to allocate the object to runtime. For example, to create an instance of <code>SubClass</code> the code would be:</p> <pre><code>    CALL [NEWOBJ_SUBCLASS]\n    MOV RCX, RAX  // EAX holds the new object\n    // If the constructor had parameters, set them\n    CALL SUBCLASS_CONSTRUCTOR\n\n.data // In the data section\nNEWOBJ_SUBCLASS: RUNTIME_ENTRY_FIXUP // One per type\n</code></pre> <p>where the <code>NEWOBJ_SUBCLASS</code> would be fixed up using the standard lazy technique.</p> <p>The same technique works for creating new arrays (NEWARR instruction).</p>"},{"location":"readytorun-overview/#type-casting-isinst-castclass","title":"Type Casting - ISINST / CASTCLASS","text":"<p>The proposal is to use the same technique as for object creation. Note that type casting could easily be a case where VSD techniques would be helpful (as any particular call might be monomorphic), and thus caching the result of the last type cast would be a performance win. However this optimization is not necessary for version resilience.</p>"},{"location":"readytorun-overview/#gc-information-for-types","title":"GC Information for Types","text":"<p>To do its job the garbage collector must be able to take an arbitrary object in the GC heap and find all the GC references in that object. It is also necessary for the GC to 'scan' the GC from start to end, which means it needs to know the size of every object. Fast access to two pieces of information is what is needed. From a versioning perspective, the fundamental problem with GC information is that (like field offsets) it incorporates information from the entire inheritance hierarchy in general case. This means that the information is not version resilient.</p> <p>While it is possible to make the GC information resilient and have the GC use this resilient data, GC happens frequently and type loading happens infrequently, so arguably you should trade type loading speed for GC speed if given the choice. Moreover the size of the GC information is typically quite small (e.g. 12-32 bytes) and will only occur for those types that cross version bubbles. Thus forming the GC information on the fly (from a version resilient form) is a reasonable starting point.</p> <p>Another important observation is that <code>MethodTable</code> contains other very frequently accessed data, like flags indicating whether the <code>MethodTable</code> represents an array, or pointer to parent type. This data tends to change a lot with the evolution of the runtime. Thus, generating method tables at runtime will solve a number of other versioning issues in addition to the GC information versioning.</p>"},{"location":"readytorun-overview/#current-state","title":"Current State","text":"<p>The design and implementation is a work in progress under code name ReadyToRun (<code>FEATURE_READYTORUN</code>). RyuJIT is used as the code generator to produce the ReadyToRun images currently.</p>"},{"location":"ryujit-overview/","title":"JIT Compiler Structure","text":""},{"location":"ryujit-overview/#introduction","title":"Introduction","text":"<p>RyuJIT is the code name for the Just-In-Time Compiler (aka \"JIT\") for the .NET runtime. It was evolved from the JIT used for x86 (jit32) on .NET Framework, and ported to support all other architecture and platform targets supported by .NET Core.</p> <p>The primary design considerations for RyuJIT are to:</p> <ul> <li>Maintain a high compatibility bar with previous JITs, especially those for x86 (jit32) and x64 (jit64).</li> <li>Support and enable good runtime performance through code optimizations, register allocation, and code generation.</li> <li>Ensure good throughput via largely linear-order optimizations and transformations, along with limitations on tracked variables for analyses (such as dataflow) that are inherently super-linear.</li> <li>Ensure that the JIT architecture is designed to support a range of targets and scenarios.</li> </ul> <p>The first objective was the primary motivation for evolving the existing code base, rather than starting from scratch or departing more drastically from the existing IR and architecture.</p>"},{"location":"ryujit-overview/#execution-environment-and-external-interface","title":"Execution Environment and External Interface","text":"<p>RyuJIT provides both just-in-time and ahead-of-time compilation service for the .NET runtime. The runtime itself is variously called the EE (execution engine), the VM (virtual machine), or simply the CLR (common language runtime). Depending upon the configuration, the EE and JIT may reside in the same or different executable files. RyuJIT implements the JIT side of the JIT/EE interfaces:</p> <ul> <li><code>ICorJitCompiler</code> \u2013 this is the interface that the JIT compiler implements. This interface is defined in src/inc/corjit.h and its implementation is in src/jit/ee_il_dll.cpp. The following are the key methods on this interface:</li> <li><code>compileMethod</code> is the main entry point for the JIT. The EE passes it a <code>ICorJitInfo</code> object,   and the \"info\" containing the IL, the method header, and various other useful tidbits.   It returns a pointer to the code, its size, and additional GC, EH and (optionally) debug info.</li> <li><code>getVersionIdentifier</code> is the mechanism by which the JIT/EE interface is versioned.   There is a single GUID (manually generated) which the JIT and EE must agree on.</li> <li><code>ICorJitInfo</code> \u2013 this is the interface that the EE implements. It has many methods defined on it that allow the JIT to look up metadata tokens, traverse type signatures, compute field and vtable offsets, find method entry points, construct string literals, etc. This bulk of this interface is inherited from <code>ICorDynamicInfo</code> which is defined in src/inc/corinfo.h. The implementation is defined in src/vm/jitinterface.cpp.</li> </ul>"},{"location":"ryujit-overview/#internal-representation-ir","title":"Internal Representation (IR)","text":""},{"location":"ryujit-overview/#compiler-object","title":"<code>Compiler</code> object","text":"<p>The <code>Compiler</code> object is the primary data structure of the JIT. While it is not part of the JIT's IR per se, it serves as the root from which the data structures that implement the IR are accessible. For example, the <code>Compiler</code> object points to the head of the function's <code>BasicBlock</code> list with the <code>fgFirstBB</code> field, as well as having additional pointers to the end of the list, and other distinguished locations. <code>ICorJitCompiler::compileMethod()</code> is invoked for each method, and creates a new <code>Compiler</code> object. Thus, the JIT need not worry about thread synchronization while accessing <code>Compiler</code> state. The EE has the necessary synchronization to ensure there is a single JIT compiled copy of a method when two or more threads try to trigger JIT compilation of the same method.</p>"},{"location":"ryujit-overview/#overview-of-the-ir","title":"Overview of the IR","text":"<p>RyuJIT represents a function as a doubly-linked list of <code>BasicBlock</code> values. Each <code>BasicBlock</code> has explicit edges to its successors that define the function's non-exceptional control flow. Exceptional control flow is implicit, with protected regions and handlers described in a table of <code>EHblkDsc</code> values. At the beginning of a compilation, each <code>BasicBlock</code> contains nodes in a high-level, statement- and tree-oriented form (HIR: \"high-level intermediate representation\"); this form persists throughout the JIT's front end. During the first phase of the back end--the rationalization phase--the HIR for each block is lowered to a linearly-ordered, node-oriented form (LIR: \"low-level intermediate representation\"). The fundamental distinction between HIR and LIR is in ordering semantics, though there are also some restrictions on the types of nodes that may appear in an HIR or LIR block.</p> <p>Both HIR and LIR blocks are composed of <code>GenTree</code> nodes that define the operations performed by the block. A <code>GenTree</code> node may consume some number of operands and may produce a singly-defined, at-most-singly-used value as a result. These values are referred to interchangeably as SDSU (single def, single use) temps or tree temps. Definitions (aka, defs) of SDSU temps are represented by <code>GenTree</code> nodes themselves, and uses are represented by edges from the using node to the defining node. Furthermore, SDSU temps defined in one block may not be used in a different block. In cases where a value must be multiply-defined, multiply-used, or defined in one block and used in another, the IR provides another class of temporary: the local var (aka, local variable). Local vars are defined by store nodes and used by users of local var nodes.</p> <p>An HIR block is composed of a doubly-linked list of statement nodes (<code>Statement</code>), each of which references a single expression tree (<code>m_rootNode</code>). The <code>GenTree</code> nodes in this tree execute in \"tree order\", which is defined as the order produced by a depth-first, left-to-right traversal of the tree, with one notable exception: * Binary nodes marked with the <code>GTF_REVERSE_OPS</code> flag execute their right operand tree (<code>gtOp2</code>) before their left operand tree (<code>gtOp1</code>)</p> <p>In addition to tree order, HIR also requires that no SDSU temp is defined in one statement and used in another. In situations where the requirements of tree and statement order prove onerous (e.g. when code must execute at a particular point in a function), HIR provides <code>GT_COMMA</code> nodes as an escape valve: these nodes consume and discard the results of their left-hand side while producing a copy of the value produced by their right-hand side. This allows the compiler to insert code in the middle of a statement without requiring that the statement be split apart.</p> <p>An LIR block is composed of a doubly-linked list of <code>GenTree</code> nodes, each of which describes a single operation in the method. These nodes execute in the order given by the list; there is no relationship between the order in which a node's operands appear and the order in which the operators that produced those operands execute. The only exception to this rule occurs after the register allocator, which may introduce <code>GT_COPY</code> and <code>GT_RELOAD</code> nodes that execute in \"spill order\". Spill order is defined as the order in which the register allocator visits a node's operands. For correctness, the code generator must generate code for spills, reloads, and <code>GT_COPY</code>/<code>GT_RELOAD</code> nodes in this order.</p> <p>In addition to HIR and LIR <code>BasicBlock</code>s, a separate representation--<code>insGroup</code> and <code>instrDesc</code>--is used during the actual instruction encoding.</p> <p>(Note that this diagram is slightly out-of-date: GenTreeStmt no longer exists, and is replaced by <code>Statement</code> nodes in the IR.)</p> <p></p>"},{"location":"ryujit-overview/#gentree-nodes","title":"GenTree Nodes","text":"<p>Each operation is represented as a GenTree node, with an opcode (<code>GT_xxx</code>), zero or more child/operand <code>GenTree</code> nodes, and additional fields as needed to represent the semantics of that node. Every node includes its type, value number, assertions, register assignments, etc. when available.</p> <p><code>GenTree</code> nodes are doubly-linked in execution order, but the links are not necessarily valid during all phases of the JIT. In HIR these links are primarily a convenience, as the order produced by a traversal of the links must match the order produced by a \"tree order\" traversal (see above for details). In LIR these links define the execution order of the nodes.</p> <p>HIR statement nodes are represented by the <code>Statement</code> type. * The statement nodes are doubly-linked. The first statement node in a block points to the last node in the block via its <code>m_prev</code> link. Note that the last statement node does not point to the first; that is, the list is not fully circular. * Each statement node contains two <code>GenTree</code> links \u2013 <code>m_rootNode</code> points to the top-level node in the statement (i.e. the root of the tree that represents the statement), while <code>m_treeList</code> points to the first node in execution order (again, this link is not always valid).</p>"},{"location":"ryujit-overview/#local-var-descriptors","title":"Local var descriptors","text":"<p>A <code>LclVarDsc</code> represents a possibly-multiply-defined, possibly-multiply-used temporary. These temporaries may be used to represent user local variables, arguments or JIT-created temps. Each lclVar has a <code>gtLclNum</code> which is the identifier usually associated with the variable in the JIT and its dumps. The <code>LclVarDsc</code> contains the type, use count, weighted use count, frame or register assignment, etc. A local var may be \"tracked\" (<code>lvTracked</code>), in which case it participates in dataflow analysis, and has a secondary name (<code>lvVarIndex</code>) that allows for the use of dense bit vectors.</p>"},{"location":"ryujit-overview/#example-of-post-import-ir","title":"Example of Post-Import IR","text":"<p>For this snippet of code (extracted from src/tests/JIT/CodeGenBringUpTests/DblRoots.cs), with <code>DOTNET_TieredCompilation=0</code> and using the DblRoots_ro.csproj project to compile it:</p> <pre><code>   r1 = (-b + Math.Sqrt(b*b - 4*a*c))/(2*a);\n</code></pre> <p>A stripped-down dump of the <code>GenTree</code> nodes just after they are imported looks like this:</p> <pre><code>STMT00000 (IL 0x000...0x026)\n\u258c  STOREIND  double\n\u251c\u2500\u2500\u258c  LCL_VAR   byref  V03 arg3\n\u2514\u2500\u2500\u258c  DIV       double\n   \u251c\u2500\u2500\u258c  ADD       double\n   \u2502  \u251c\u2500\u2500\u258c  NEG       double\n   \u2502  \u2502  \u2514\u2500\u2500\u258c  LCL_VAR   double V01 arg1\n   \u2502  \u2514\u2500\u2500\u258c  INTRINSIC double sqrt\n   \u2502     \u2514\u2500\u2500\u258c  SUB       double\n   \u2502        \u251c\u2500\u2500\u258c  MUL       double\n   \u2502        \u2502  \u251c\u2500\u2500\u258c  LCL_VAR   double V01 arg1\n   \u2502        \u2502  \u2514\u2500\u2500\u258c  LCL_VAR   double V01 arg1\n   \u2502        \u2514\u2500\u2500\u258c  MUL       double\n   \u2502           \u251c\u2500\u2500\u258c  MUL       double\n   \u2502           \u2502  \u251c\u2500\u2500\u258c  CNS_DBL   double 4.0000000000000000\n   \u2502           \u2502  \u2514\u2500\u2500\u258c  LCL_VAR   double V00 arg0\n   \u2502           \u2514\u2500\u2500\u258c  LCL_VAR   double V02 arg2\n   \u2514\u2500\u2500\u258c  MUL       double\n      \u251c\u2500\u2500\u258c  CNS_DBL   double 2.0000000000000000\n      \u2514\u2500\u2500\u258c  LCL_VAR   double V00 arg0\n</code></pre>"},{"location":"ryujit-overview/#types","title":"Types","text":"<p>The JIT is primarily concerned with \"primitive\" types, i.e. integers, reference types, pointers, floating point and SIMD types. It must also be concerned with the format of user-defined value types (i.e. struct types derived from <code>System.ValueType</code>) \u2013 specifically, their size and the offset of any GC references they contain, so that they can be correctly initialized and copied. The primitive types are represented in the JIT by the <code>var_types</code> enum, and any additional information required for struct types is obtained from the JIT/EE interface by the use of an opaque <code>CORINFO_CLASS_HANDLE</code>, which is converted into a <code>ClassLayout</code> instance that caches the most important information. All <code>TYP_STRUCT</code>-typed nodes can be queried for the layout they produce via <code>GenTree::GetLayout</code>.</p> <p>Some nodes also use \"small\" integer types - <code>TYP_BYTE</code>, <code>TYP_UBYTE</code>, <code>TYP_SHORT</code> and <code>TYP_USHORT</code>, to represent that they produce implicitly sign- or zero-extended <code>TYP_INT</code> values, much like in the IL stack model.</p>"},{"location":"ryujit-overview/#dataflow-information","title":"Dataflow Information","text":"<p>In order to limit throughput impact, the JIT limits the number of lclVars for which liveness information is computed. These are the tracked lclVars (<code>lvTracked</code> is true), and they are the only candidates for register allocation (i.e. only these lclVars may be assigned registers over their entire lifetime). Defs and uses of untracked lclVars are treated as stores and loads to/from the appropriate stack location, and the corresponding nodes act as normal operators during register allocation.</p> <p>The liveness analysis determines the set of defs, as well as the uses that are upward exposed, for each block. It then propagates the liveness information. The result of the analysis is captured in the following:</p> <ul> <li>The live-in and live-out sets are captured in the <code>bbLiveIn</code> and <code>bbLiveOut</code> fields of the <code>BasicBlock</code>.</li> <li>The <code>GTF_VAR_DEF</code> flag is set on a lclVar node (all of which are of type <code>GenTreeLclVarCommon</code>) that is a definition.</li> <li>The <code>GTF_VAR_USEASG</code> flag is set (in addition to the <code>GTF_VAR_DEF</code> flag) on partial definitions of a local variable (i.e. <code>GT_LCL_FLD</code> nodes that do not define the entire variable).</li> </ul>"},{"location":"ryujit-overview/#ssa","title":"SSA","text":"<p>Static single assignment (SSA) form is constructed in a traditional manner [1]. The SSA names are recorded on the lclVar references and point to the <code>LclSsaVarDsc</code> descriptors that contain the defining store node and block in which it occurs.</p>"},{"location":"ryujit-overview/#value-numbering","title":"Value Numbering","text":"<p>Value numbering utilizes SSA for lclVar values, but also performs value numbering of expression trees. It takes advantage of type safety by not invalidating the value number for field references with a heap write, unless the write is to the same field. The IR nodes are annotated with the value numbers, which are indexes into a type-specific value number store. Value numbering traverses the trees, performing symbolic evaluation of many operations.</p>"},{"location":"ryujit-overview/#phases-of-ryujit","title":"Phases of RyuJIT","text":"<p>The top-level function of interest is <code>Compiler::compCompile</code>. It invokes the following phases in order.</p> Phase IR Transformations Pre-import <code>Compiler-&gt;lvaTable</code> created and filled in for each user argument and variable. <code>BasicBlock</code> list initialized. Importation <code>GenTree</code> nodes created and linked in to <code>Statement</code> nodes, and Statements into BasicBlocks. Inlining candidates identified. Inlining The IR for inlined methods is incorporated into the flowgraph. Struct Promotion New lclVars are created for each field of a promoted struct. Mark Address-Exposed Locals lclVars with references occurring in an address-taken context are marked.  This must be kept up-to-date. Early liveness Compute lclVar liveness for use by phases up to and including global morph. Forward Subtitution Eliminate SDSU-like locals by substituting their values directly into uses. Physical promotion Split struct locals into primitives based on access patterns. Morph Blocks Performs localized transformations, including mandatory normalization as well as simple optimizations. Eliminate Qmarks All <code>GT_QMARK</code> nodes are eliminated, other than simple ones that do not require control flow. Flowgraph Analysis Loops are identified and normalized, cloned and/or unrolled. Normalize IR for Optimization lclVar references counts are set, and must be kept valid. Evaluation order of <code>GenTree</code> nodes (<code>gtNext</code>/<code>gtPrev</code>) is determined, and must be kept valid. SSA and Value Numbering Optimizations Computes liveness (<code>bbLiveIn</code> and <code>bbLiveOut</code> on <code>BasicBlock</code>s), and dominators. Builds SSA for tracked lclVars. Computes value numbers. Loop Invariant Code Hoisting Hoists expressions out of loops. Copy Propagation Copy propagation based on value numbers. Common Subexpression Elimination (CSE) Elimination of redundant subexressions based on value numbers. Assertion Propagation Utilizes value numbers to propagate and transform based on properties such as non-nullness. Range analysis Eliminate array index range checks based on value numbers and assertions Induction variable optimization Optimize induction variables used inside natural loops based on scalar evolution analysis VN-based dead store elimination Eliminate stores that do not change the value of a local. If conversion Transform conditional definitions into <code>GT_SELECT</code> operators. Rationalization Flowgraph order changes from <code>FGOrderTree</code> to <code>FGOrderLinear</code>. All <code>GT_COMMA</code> nodes are transformed. Lowering Nodes are tranformed for register allocation; Target-specific optimizations are performed. Register allocation Registers are assigned (<code>gtRegNum</code> and/or <code>gtRsvdRegs</code>), and the number of spill temps calculated. Code Generation Determines frame layout. Generates code for each <code>BasicBlock</code>. Generates prolog &amp; epilog code for the method. Emits EH, GC and Debug info."},{"location":"ryujit-overview/#pre-import","title":"Pre-import","text":"<p>Prior to reading in the IL for the method, the JIT initializes the local variable table, and scans the IL to find branch targets and form BasicBlocks.</p>"},{"location":"ryujit-overview/#importation","title":"Importation","text":"<p>Importation is the phase that creates the IR for the method, reading in one IL instruction at a time, and building up the statements. During this process, it may need to generate IR with multiple, nested expressions. This is the purpose of the non-expression-like IR nodes:</p> <ul> <li>It may need to evaluate part of the expression into a temp, in which case it will use a comma (<code>GT_COMMA</code>) node to ensure that the temp is evaluated in the proper execution order \u2013 i.e. <code>GT_COMMA(GT_STORE_LCL_VAR&lt;temp&gt;(exp), temp)</code> is inserted into the tree where \"exp\" would go.</li> <li>It may need to create conditional expressions, but adding control flow at this point would be quite messy. In this case it generates question mark/colon (?: or <code>GT_QMARK</code>/<code>GT_COLON</code>) trees that may be nested within an expression.</li> </ul> <p>During importation, tail call candidates (either explicitly marked or opportunistically identified) are identified and flagged. They are further validated, and possibly unmarked, during morphing.</p>"},{"location":"ryujit-overview/#morphing","title":"Morphing","text":"<p>The <code>fgMorph</code> phase includes a number of transformations:</p>"},{"location":"ryujit-overview/#inlining","title":"Inlining","text":"<p>The <code>fgInline</code> phase determines whether each call site is a candidate for inlining. The initial determination is made via a state machine that runs over the candidate method's IL. It estimates the native code size corresponding to the inline method, and uses a set of heuristics, including the estimated size of the current method, to determine if inlining would be profitable. If so, a separate <code>Compiler</code> object is created, and the importation phase is called to create the tree for the candidate inline method. Inlining may be aborted prior to completion, if any conditions are encountered that indicate that it may be unprofitable (or otherwise incorrect). If inlining is successful, the inlinee compiler's trees are incorporated into the inliner compiler (the \"parent\"), with arguments and return values appropriately transformed.</p>"},{"location":"ryujit-overview/#struct-promotion","title":"Struct Promotion","text":"<p>Struct promotion (<code>fgPromoteStructs()</code>) analyzes the local variables and temps, and determines if their fields are candidates for tracking (and possibly enregistering) separately. It first determines whether it is possible to promote, which takes into account whether the layout may have holes or overlapping fields, whether its fields (flattening any contained structs) will fit in registers, etc.</p> <p>Next, it determines whether it is likely to be profitable, based on the number of fields, and whether the fields are individually referenced.</p> <p>When a lclVar is promoted, there are now N+1 lclVars for the struct, where N is the number of fields. The original struct lclVar is not considered to be tracked, but its fields may be.</p>"},{"location":"ryujit-overview/#mark-address-exposed-locals","title":"Mark Address-Exposed Locals","text":"<p>This phase traverses the expression trees, propagating the context (e.g. taking the address, indirecting) to determine which lclVars have their address taken, and which therefore will not be register candidates. If a struct lclVar has been promoted, and is then found to be address-taken, it will be considered \"dependently promoted\", which is an odd way of saying that the fields will still be separately tracked, but they will not be register candidates.</p>"},{"location":"ryujit-overview/#morph-blocks","title":"Morph Blocks","text":"<p>What is often thought of as \"morph\" involves localized transformations to the trees. In addition to performing simple optimizing transformations, it performs some normalization that is required, such as converting field and array accesses into pointer arithmetic. It can (and must) be called by subsequent phases on newly added or modified trees. During the main Morph phase, the boolean <code>fgGlobalMorph</code> is set on the <code>Compiler</code> argument, which governs which transformations are permissible.</p>"},{"location":"ryujit-overview/#eliminate-qmarks","title":"Eliminate Qmarks","text":"<p>This expands most <code>GT_QMARK</code>/<code>GT_COLON</code> trees into blocks, except for the case that is instantiating a condition.</p>"},{"location":"ryujit-overview/#flowgraph-analysis","title":"Flowgraph Analysis","text":"<p>At this point, a number of analyses and transformations are done on the flowgraph:</p> <ul> <li>Computing edge weights, if profile information is available</li> <li>Computing reachability and dominators</li> <li>Identifying and normalizing loops (transforming while loops to \"do while\")</li> <li>Cloning and unrolling of loops</li> </ul>"},{"location":"ryujit-overview/#normalize-ir-for-optimization","title":"Normalize IR for Optimization","text":"<p>At this point, a number of properties are computed on the IR, and must remain valid for the remaining phases. We will call this \"normalization\"</p> <ul> <li><code>lvaMarkLocalVars</code> \u2013 if this jit is optimizing, set the reference counts (raw and weighted) for lclVars, sort them, and determine which will be tracked (up to <code>JitMaxLocalsToTrack</code>, 1024 by default). If not optimizing, all locals are given an implicit reference count of one. Reference counts are not incrementally maintained. They can be recomputed if accurate counts are needed.</li> <li>Link the trees in evaluation order (setting <code>gtNext</code> and <code>gtPrev</code> fields): <code>fgFindOperOrder()</code> and <code>fgSetBlockOrder()</code>.</li> </ul>"},{"location":"ryujit-overview/#ssa-and-value-numbering-optimizations","title":"SSA and Value Numbering Optimizations","text":"<p>The next set of optimizations are built on top of SSA and value numbering. First, the SSA representation is built (during which dataflow analysis, aka liveness, is computed on the lclVars), then value numbering is done using SSA.</p>"},{"location":"ryujit-overview/#loop-invariant-code-hoisting","title":"Loop Invariant Code Hoisting","text":"<p>This phase traverses all the loop nests, in outer-to-inner order (thus hoisting expressions outside the largest loop in which they are invariant). It traverses all of the statements in the blocks in the loop that are always executed. If the statement is:</p> <ul> <li>A valid CSE candidate</li> <li>Has no side-effects</li> <li>Does not raise an exception OR occurs in the loop prior to any side-effects</li> <li>Has a valid value number, and it is a lclVar defined outside the loop, or its children (the value numbers from which it was computed) are invariant.</li> </ul>"},{"location":"ryujit-overview/#copy-propagation","title":"Copy Propagation","text":"<p>This phase walks each block in the graph (in dominator-first order, maintaining context between dominator and child) keeping track of every live definition. When it encounters a variable that shares the VN with a live definition, it is replaced with the variable in the live definition.</p> <p>The JIT currently requires that the IR be maintained in conventional SSA form, as there is no \"out of SSA\" translation (see the comments on <code>optVnCopyProp()</code> for more information).</p>"},{"location":"ryujit-overview/#common-subexpression-elimination-cse","title":"Common Subexpression Elimination (CSE)","text":"<p>Utilizes value numbers to identify redundant computations, which are then evaluated to a new temp lclVar, and then reused.</p>"},{"location":"ryujit-overview/#assertion-propagation","title":"Assertion Propagation","text":"<p>Utilizes value numbers to propagate and transform based on properties such as non-nullness.</p>"},{"location":"ryujit-overview/#induction-variable-optimization","title":"Induction variable optimization","text":"<p>Performs scalar evolution analysis and utilized it to optimize induction variables inside loops. Currently this entails IV widening which is done on x64 only.</p>"},{"location":"ryujit-overview/#range-analysis","title":"Range analysis","text":"<p>Optimize array index range checks based on value numbers and assertions.</p>"},{"location":"ryujit-overview/#vn-based-dead-store-elimination","title":"VN-based dead store elimination","text":"<p>Walks over the SSA descriptors and removes definitions where the new value is identical to the previous. This phase invalidates both SSA and value numbers; after this point both should be considered stale.</p>"},{"location":"ryujit-overview/#if-conversion","title":"If Conversion","text":"<p>Uses simple analysis to transform conditional definitions of locals into unconditional <code>GT_SELECT</code> nodes, which will can later be emitted as, e. g., conditional moves.</p>"},{"location":"ryujit-overview/#rationalization","title":"Rationalization","text":"<p>As the JIT has evolved, changes have been made to improve the ability to reason over the tree in both \"tree order\" and \"linear order\". These changes have been termed the \"rationalization\" of the IR. In the spirit of reuse and evolution, some of the changes have been made only in the later (\"backend\") components of the JIT. The corresponding transformations are made to the IR by a \"Rationalizer\" component. It is expected that over time some of these changes will migrate to an earlier place in the JIT phase order:</p> <ul> <li>Elimination of \"comma\" nodes (<code>GT_COMMA</code>). These nodes are introduced for convenience during importation, during which a single tree is constructed at a time, and not incorporated into the statement list until it is completed. When it is necessary, for example, to store a partially-constructed tree into a temporary variable, a <code>GT_COMMA</code> node is used to link it into the tree. However, in later phases, these comma nodes are an impediment to analysis, and thus are eliminated.</li> <li>In some cases, it is not possible to fully extract the tree into a separate statement, due to execution order dependencies. In these cases, an \"embedded\" statement is created. While these are conceptually very similar to the <code>GT_COMMA</code> nodes, they do not masquerade as expressions.</li> <li>Elimination of \"QMark\" (<code>GT_QMARK</code>/<code>GT_COLON</code>) nodes is actually done at the end of morphing, long before the current rationalization phase. The presence of these nodes made analyses (especially dataflow) overly complex.</li> <li>Elimination of statements. Without statements, the execution order of a basic block's contents is fully defined by the <code>gtNext</code>/<code>gtPrev</code> links between <code>GenTree</code> nodes.</li> </ul> <p>For our earlier example (Example of Post-Import IR), here is what the simplified dump looks like just prior to Rationalization (the $ annotations are value numbers). Note that some common subexpressions have been computed into new temporary lclVars, and that computation has been inserted as a <code>GT_COMMA</code> (comma) node in the IR:</p> <pre><code>STMT  (IL 0x000...0x026)\n\u258c  STOREIND  double $VN.Void\n\u251c\u2500\u2500\u258c  LCL_VAR   byref  V03 arg3         u:1 (last use) $c0\n\u2514\u2500\u2500\u258c  DIV       double $146\n   \u251c\u2500\u2500\u258c  ADD       double $144\n   \u2502  \u251c\u2500\u2500\u258c  COMMA     double $83\n   \u2502  \u2502  \u251c\u2500\u2500\u258c  STORE_LCL_VAR double V06 cse0         d:1 $83\n   \u2502  \u2502  \u2502  \u2514\u2500\u2500\u258c  INTRINSIC double sqrt $83\n   \u2502  \u2502  \u2502     \u2514\u2500\u2500\u258c  SUB       double $143\n   \u2502  \u2502  \u2502        \u251c\u2500\u2500\u258c  MUL       double $140\n   \u2502  \u2502  \u2502        \u2502  \u251c\u2500\u2500\u258c  LCL_VAR   double V01 arg1         u:1 $81\n   \u2502  \u2502  \u2502        \u2502  \u2514\u2500\u2500\u258c  LCL_VAR   double V01 arg1         u:1 $81\n   \u2502  \u2502  \u2502        \u2514\u2500\u2500\u258c  MUL       double $142\n   \u2502  \u2502  \u2502           \u251c\u2500\u2500\u258c  MUL       double $141\n   \u2502  \u2502  \u2502           \u2502  \u251c\u2500\u2500\u258c  LCL_VAR   double V00 arg0         u:1 $80\n   \u2502  \u2502  \u2502           \u2502  \u2514\u2500\u2500\u258c  CNS_DBL   double 4.0000000000000000 $180\n   \u2502  \u2502  \u2502           \u2514\u2500\u2500\u258c  LCL_VAR   double V02 arg2         u:1 $82\n   \u2502  \u2502  \u2514\u2500\u2500\u258c  LCL_VAR   double V06 cse0         u:1 $83\n   \u2502  \u2514\u2500\u2500\u258c  COMMA     double $84\n   \u2502     \u251c\u2500\u2500\u258c  STORE_LCL_VAR double V08 cse2         d:1 $84\n   \u2502     \u2502  \u2514\u2500\u2500\u258c  NEG       double $84\n   \u2502     \u2502     \u2514\u2500\u2500\u258c  LCL_VAR   double V01 arg1         u:1 $81\n   \u2502     \u2514\u2500\u2500\u258c  LCL_VAR   double V08 cse2         u:1 $84\n   \u2514\u2500\u2500\u258c  COMMA     double $145\n      \u251c\u2500\u2500\u258c  STORE_LCL_VAR double V07 cse1         d:1 $145\n      \u2502  \u2514\u2500\u2500\u258c  MUL       double $145\n      \u2502     \u251c\u2500\u2500\u258c  LCL_VAR   double V00 arg0         u:1 $80\n      \u2502     \u2514\u2500\u2500\u258c  CNS_DBL   double 2.0000000000000000 $181\n      \u2514\u2500\u2500\u258c  LCL_VAR   double V07 cse1         u:1 $145\n</code></pre> <p>After Rationalize, the nodes are presented in execution order, and the <code>GT_COMMA</code> (comma) and <code>Statement</code> nodes have been eliminated:</p> <pre><code>        IL_OFFSET void   IL offset: 0x0\nt3 =    LCL_VAR   double V01 arg1         u:1 $81\nt4 =    LCL_VAR   double V01 arg1         u:1 $81\n     \u250c\u2500\u2500\u258c  t3     double\n     \u251c\u2500\u2500\u258c  t4     double\nt5 = \u258c  MUL       double $140\nt7 =    LCL_VAR   double V00 arg0         u:1 $80\nt6 =    CNS_DBL   double 4.0000000000000000 $180\n     \u250c\u2500\u2500\u258c  t7     double\n     \u251c\u2500\u2500\u258c  t6     double\nt8 = \u258c  MUL       double $141\nt9 =    LCL_VAR   double V02 arg2         u:1 $82\n     \u250c\u2500\u2500\u258c  t8     double\n     \u251c\u2500\u2500\u258c  t9     double\n10 = \u258c  MUL       double $142\n     \u250c\u2500\u2500\u258c  t5     double\n     \u251c\u2500\u2500\u258c  t10    double\n11 = \u258c  SUB       double $143\n     \u250c\u2500\u2500\u258c  t11    double\n12 = \u258c  INTRINSIC double sqrt $83\n     \u250c\u2500\u2500\u258c  t12    double\n     \u258c  STORE_LCL_VAR double V06 cse0         d:1\n43 =    LCL_VAR   double V06 cse0         u:1 $83\nt1 =    LCL_VAR   double V01 arg1         u:1 $81\n     \u250c\u2500\u2500\u258c  t1     double\nt2 = \u258c  NEG       double $84\n     \u250c\u2500\u2500\u258c  t2     double\n     \u258c  STORE_LCL_VAR double V08 cse2         d:1\n53 =    LCL_VAR   double V08 cse2         u:1 $84\n     \u250c\u2500\u2500\u258c  t43    double\n     \u251c\u2500\u2500\u258c  t53    double\n13 = \u258c  ADD       double $144\n15 =    LCL_VAR   double V00 arg0         u:1 $80\n14 =    CNS_DBL   double 2.0000000000000000 $181\n     \u250c\u2500\u2500\u258c  t15    double\n     \u251c\u2500\u2500\u258c  t14    double\n16 = \u258c  MUL       double $145\n     \u250c\u2500\u2500\u258c  t16    double\n     \u258c  STORE_LCL_VAR double V07 cse1         d:1\n48 =    LCL_VAR   double V07 cse1         u:1 $145\n     \u250c\u2500\u2500\u258c  t13    double\n     \u251c\u2500\u2500\u258c  t48    double\n17 = \u258c  DIV       double $146\nt0 =    LCL_VAR   byref  V03 arg3         u:1 (last use) $c0\n     \u250c\u2500\u2500\u258c  t0     byref\n     \u251c\u2500\u2500\u258c  t17    double\n     \u258c  STOREIND  double\n        IL_OFFSET void   IL offset: 0x27\n55 =    LCL_VAR   double V08 cse2         u:1 $84\n45 =    LCL_VAR   double V06 cse0         u:1 $83\n     \u250c\u2500\u2500\u258c  t55    double\n     \u251c\u2500\u2500\u258c  t45    double\n33 = \u258c  SUB       double $147\n50 =    LCL_VAR   double V07 cse1         u:1 $145\n     \u250c\u2500\u2500\u258c  t33    double\n     \u251c\u2500\u2500\u258c  t50    double\n37 = \u258c  DIV       double $148\n20 =    LCL_VAR   byref  V04 arg4         u:1 (last use) $c1\n     \u250c\u2500\u2500\u258c  t20    byref\n     \u251c\u2500\u2500\u258c  t37    double\n     \u258c  STOREIND  double\n        IL_OFFSET void   IL offset: 0x4f\n        RETURN    void   $200\n</code></pre>"},{"location":"ryujit-overview/#lowering","title":"Lowering","text":"<p>Lowering is responsible for transforming the IR in such a way that the control flow, and any register requirements, are fully exposed.</p> <p>It does an execution-order traversal that performs context-dependent transformations such as * expanding switch statements (using a switch table or a series of conditional branches) * constructing addressing modes (<code>GT_LEA</code> nodes) * determining the code generation strategy for block assignments (e.g. <code>GT_STORE_BLK</code>) which may become helper calls, unrolled loops, or an instruction like <code>rep stos</code> * generating machine specific instructions (e.g. generating the <code>BT</code> x86/64 instruction) * mark \"contained\" nodes - such a node does not generate any code and relies on its user to include the node's operation in its own codegen (e.g. memory operands, immediate operands) * mark \"reg optional\" nodes - despite the name, such a node may produce a value in a register but its user does not require a register and can consume the value directly from a memory location</p> <p>For example, this:</p> <pre><code>t47 =    LCL_VAR   ref    V00 arg0\nt48 =    LCL_VAR   int    V01 arg1\n      \u250c\u2500\u2500\u258c  t48    int\nt51 = \u258c  CAST      long &lt;- int\nt52 =    CNS_INT   long   2\n      \u250c\u2500\u2500\u258c  t51    long\n      \u251c\u2500\u2500\u258c  t52    long\nt53 = \u258c  LSH       long\nt54 =    CNS_INT   long   16 Fseq[#FirstElem]\n      \u250c\u2500\u2500\u258c  t53    long\n      \u251c\u2500\u2500\u258c  t54    long\nt55 = \u258c  ADD       long\n      \u250c\u2500\u2500\u258c  t47    ref\n      \u251c\u2500\u2500\u258c  t55    long\nt56 = \u258c  ADD       byref\n      \u250c\u2500\u2500\u258c  t56    byref\nt44 = \u258c  IND       int\n</code></pre> <p>Is transformed into this, in which the addressing mode is explicit:</p> <p><pre><code>t47 =    LCL_VAR   ref    V00 arg0\nt48 =    LCL_VAR   int    V01 arg1\n      \u250c\u2500\u2500\u258c  t48    int\nt51 = \u258c  CAST      long &lt;- int\n      \u250c\u2500\u2500\u258c  t47    ref\n      \u251c\u2500\u2500\u258c  t51    long\nt79 = \u258c  LEA(b+(i*4)+16) byref\n      \u250c\u2500\u2500\u258c  t79    byref\nt44 = \u258c  IND       int\n</code></pre> Sometimes <code>Lowering</code> will insert nodes into the execution order before the node that it is currently handling. In such cases, it must ensure that they themselves are properly lowered. This includes:</p> <ul> <li>Generating only legal <code>LIR</code> nodes that do not themselves require lowering.</li> <li>Performing any needed containment analysis (e.g. <code>ContainCheckRange()</code>) on the newly added node(s).</li> </ul> <p>After all nodes are lowered, liveness is run in preparation for register allocation.</p>"},{"location":"ryujit-overview/#register-allocation","title":"Register allocation","text":"<p>The RyuJIT register allocator uses a Linear Scan algorithm, with an approach similar to [2]. In discussion it is referred to as either <code>LinearScan</code> (the name of the implementing class), or LSRA (Linear Scan Register Allocation). In brief, it operates on two main data structures:</p> <ul> <li><code>Intervals</code> (representing live ranges of variables or tree expressions) and <code>RegRecords</code> (representing physical registers), both of which derive from <code>Referenceable</code>.</li> <li><code>RefPositions</code>, which represent uses or defs (or variants thereof, such as ExposedUses) of either <code>Intervals</code> or physical registers.</li> </ul> <p><code>LinearScan::buildIntervals()</code> traverses the entire method building RefPositions and Intervals as required. For example, for the <code>STORE_BLK</code> node in this snippet:</p> <pre><code>t67 =    CNS_INT(h) long   0x2b5acef2c50 static Fseq[s1]\n     \u250c\u2500\u2500\u258c  t67    long\n t0 = \u258c  IND       ref\n t1 =    CNS_INT   long   8 Fseq[#FirstElem]\n     \u250c\u2500\u2500\u258c  t0     ref\n     \u251c\u2500\u2500\u258c  t1     long\n t2 = \u258c  ADD       byref\n     \u250c\u2500\u2500\u258c  t2     byref\n t3 = \u258c  IND       struct\nt31 =    LCL_VAR_ADDR byref  V08 tmp1\n     \u250c\u2500\u2500\u258c  t31    byref\n     \u251c\u2500\u2500\u258c  t3     struct\n      \u258c  STORE_BLK(40) struct (copy) (Unroll)\n</code></pre> <p>the following RefPositions are generated:</p> <pre><code>N027 (???,???) [000085] -A-XG-------              \u258c  STORE_BLK(40) struct (copy) (Unroll) REG NA\nInterval 16: int RefPositions {} physReg:NA Preferences=[allInt]\n&lt;RefPosition #40  @27  RefTypeDef &lt;Ivl:16 internal&gt; STORE_BLK BB01 regmask=[allInt] minReg=1&gt;\nInterval 17: float RefPositions {} physReg:NA Preferences=[allFloat]\n&lt;RefPosition #41  @27  RefTypeDef &lt;Ivl:17 internal&gt; STORE_BLK BB01 regmask=[allFloat] minReg=1&gt;\n&lt;RefPosition #42  @27  RefTypeUse &lt;Ivl:15&gt; BB01 regmask=[allInt] minReg=1 last&gt;\n&lt;RefPosition #43  @27  RefTypeUse &lt;Ivl:16 internal&gt; STORE_BLK BB01 regmask=[allInt] minReg=1 last&gt;\n&lt;RefPosition #44  @27  RefTypeUse &lt;Ivl:17 internal&gt; STORE_BLK BB01 regmask=[allFloat] minReg=1 last&gt;\n</code></pre> <p>The \"@ 27\" is the location number of the node. \"internal\" indicates a register that is internal to the node (in this case 2 internal registers are needed, one float (XMM on XARCH) and one int, as temporaries for copying). \"regmask\" indicates the register constraints for the <code>RefPosition</code>.</p>"},{"location":"ryujit-overview/#notable-features-of-ryujit-linearscan","title":"Notable features of RyuJIT LinearScan","text":"<p>Unlike most register allocators, LSRA performs register allocation on an IR (Intermediate Representation) that is not a direct representation of the target instructions. A given IR node may map to 0, 1 or multiple target instructions. Nodes that are \"contained\" are handled by code generation as part of their parent node and thus may map to 0 instructions. A simple node will have a 1-to-1 mapping to a target instruction, and a more complex node (e.g. <code>GT_STORE_BLK</code>) may map to multiple instructions.</p>"},{"location":"ryujit-overview/#pre-conditions","title":"Pre-conditions:","text":"<p>It is the job of the <code>Lowering</code> phase to transform the IR such that: * The nodes are in <code>LIR</code> form (i.e. all expression trees have been linearized, and the execution order of the nodes within a BasicBlock is specified by the <code>gtNext</code> and <code>gtPrev</code> links) * All contained nodes are identified (<code>gtFlags</code> has the <code>GTF_CONTAINED</code> bit set) * All nodes for which a register is optional are identified (<code>RefPosition::regOptional</code> is <code>true</code>)   * This is used for x86 and x64 on operands that can be directly consumed from memory if no register is allocated. * All unused values (nodes that produce a result that is not consumed) are identified (<code>gtLIRFlags</code> has the <code>LIR::Flags::UnusedValue</code> bit set)   * Since tree temps (the values produced by nodes and consumed by their parent) are expected to be single-def, single-use (SDSU), normally the live range can be determined to end at the use. If there is no use, the register allocator doesn't know where the live range ends. * Code can be generated without any context from the parent (consumer) of each node.</p> <p>After <code>Lowering</code> has completed, liveness analysis is performed: * It identifies which <code>lclVar</code>s should have their liveness computed.   * The reason this is done after <code>Lowering</code> is that it can introduce new <code>lclVar</code>s. * It then does liveness analysis on those <code>lclVar</code>s, updating the <code>bbLiveIn</code> and <code>bbLiveOut</code> sets for each <code>BasicBlock</code>.   * This tells the register allocator which <code>lclVars</code> are live at block boundaries.   * Note that \"tree temps\" cannot be live at block boundaries.</p>"},{"location":"ryujit-overview/#allocation-overview","title":"Allocation Overview","text":"<p>Allocation proceeds in 4 phases:</p> <ul> <li>Prepration:</li> <li>Determine the order in which the <code>BasicBlocks</code> will be allocated, and which predecessor of each block will be used to determine the starting location for variables live-in to the <code>BasicBlock</code>.</li> <li>Construct an <code>Interval</code> for each <code>lclVar</code> that may be enregistered.</li> <li>Construct a <code>RegRecord</code> for each physical register.</li> <li>Walk the <code>BasicBlocks</code> in the determined order building <code>RefPositions</code> for each register use, def, or kill.</li> <li>Just prior to building <code>RefPosition</code>s for the node, the <code>TreeNodeInfoInit()</code> method is called to determine its register requirements.</li> <li>Allocate the registers by traversing the <code>RefPositions</code>.</li> <li>Write back the register assignments, and perform any necessary moves at block boundaries where the allocations don't match.</li> </ul> <p>Post-conditions:</p> <ul> <li>The <code>gtRegNum</code> property of all <code>GenTree</code> nodes that require a register has been set to a valid register number.</li> <li>For reg-optional nodes, the <code>GTF_NOREG_AT_USE</code> bit is set in <code>gtFlags</code> if a register was not allocated.</li> <li>The <code>gtRsvdRegs</code> field (a set/mask of registers) has the requested number of registers specified for internal use.</li> <li>All spilled values (lclVar or expression) are marked with <code>GTF_SPILL</code> at their definition. For lclVars, they are also marked with <code>GTF_SPILLED</code> at any use at which the value must be reloaded.</li> <li>For all lclVars that are register candidates:</li> <li><code>lvRegNum</code> = initial register location (or <code>REG_STK</code>)</li> <li><code>lvRegister</code> flag set if it always lives in the same register</li> <li><code>lvSpilled</code> flag is set if it is ever spilled</li> <li>The maximum number of simultaneously-live spill locations of each type (used for spilling expression trees) has been communicated via calls to <code>compiler-&gt;tmpPreAllocateTemps(type)</code>.</li> </ul>"},{"location":"ryujit-overview/#code-generation","title":"Code Generation","text":"<p>The process of code generation is relatively straightforward, as Lowering has done some of the work already. Code generation proceeds roughly as follows:</p> <ul> <li>Determine the frame layout \u2013 allocating space on the frame for any lclVars that are not fully enregistered, as well as any spill temps required for spilling non-lclVar expressions.</li> <li>For each <code>BasicBlock</code>, in layout order, and each <code>GenTree</code> node in the block, in execution order:</li> <li>If the node is \"contained\" (i.e. its operation is subsumed by a parent node), do nothing.</li> <li>Otherwise, \"consume\" all the register operands of the node.<ul> <li>This updates the liveness information (i.e. marking a lclVar as dead if this is the last use), and performs any needed copies.</li> <li>This must be done in \"spill order\" so that any spill/restore code inserted by the register allocator to resolve register conflicts is generated in the correct order. \"</li> </ul> </li> <li>Track the live variables in registers, as well as the live stack variables that contain GC refs.</li> <li>Produce the <code>instrDesc(s)</code> for the operation, with the current live GC references.</li> <li>Update the scope information (debug info) at block boundaries.</li> <li>Generate the prolog and epilog code.</li> <li>Write the final instruction bytes. It does this by invoking the emitter, which holds all the <code>instrDescs</code>.</li> </ul>"},{"location":"ryujit-overview/#phase-dependent-properties-and-invariants-of-the-ir","title":"Phase-dependent Properties and Invariants of the IR","text":"<p>There are several properties of the IR that are valid only during (or after) specific phases of the JIT. This section describes the phase transitions, and how the IR properties are affected.</p>"},{"location":"ryujit-overview/#phase-transitions","title":"Phase Transitions","text":"<ul> <li>Flowgraph analysis</li> <li>Computes reachability and dominators. These may be invalidated by changes to the flowgraph.</li> <li>Computes edge weights, if profile information is available.</li> <li>Identifies and normalizes loops. These may be invalidated, but must be marked as such.</li> <li>Normalization</li> <li>The lclVar reference counts are set by <code>lvaMarkLocalVars()</code>.</li> <li>Statement ordering is determined by <code>fgSetBlockOrder()</code>. Execution order is a depth-first preorder traversal of the nodes, with the operands usually executed in order. The exceptions are:<ul> <li>Binary operators, which can have the <code>GTF_REVERSE_OPS</code> flag set to indicate that the RHS (<code>gtOp2</code>) should be evaluated before the LHS (<code>gtOp1</code>).</li> </ul> </li> <li>Rationalization</li> <li>All <code>GT_COMMA</code> and <code>Statement</code> nodes are removed and their constituent nodes linked into execution order.</li> <li>Lowering</li> <li><code>GenTree</code> nodes are split or transformed as needed to expose all of their register requirements and any necessary <code>flowgraph</code> changes (e.g., for switch statements).</li> </ul>"},{"location":"ryujit-overview/#gentree-phase-dependent-properties","title":"GenTree phase-dependent properties","text":"<p>Ordering:</p> <ul> <li>For <code>Statement</code> nodes, the <code>m_next</code> and <code>m_prev</code> fields must always be consistent. The last statement in the <code>BasicBlock</code> must have <code>m_next</code> equal to <code>nullptr</code>. By convention, the <code>m_prev</code> of the first statement in the <code>BasicBlock</code> must be the last statement of the <code>BasicBlock</code>.</li> <li>In all phases, <code>m_rootNode</code> points to the top-level node of the expression.</li> <li>For 'GenTree' nodes, the <code>gtNext</code> and <code>gtPrev</code> fields are either <code>nullptr</code>, prior to ordering, or they are consistent (i.e. <code>A-&gt;gtPrev-&gt;gtNext = A</code>, and <code>A-&gt;gtNext-&gt;gtPrev == A</code>, if they are non-<code>nullptr</code>).</li> <li>After normalization the <code>m_treeList</code> of the containing statement points to the first node to be executed.</li> <li>Prior to normalization, the <code>gtNext</code> and <code>gtPrev</code> pointers on the expression <code>GenTree</code> nodes are invalid. The expression nodes are only traversed via the links from parent to child (e.g. <code>node-&gt;gtGetOp1()</code>, or <code>node-&gt;gtOp.gtOp1</code>). The <code>gtNext/gtPrev</code> links are set by <code>fgSetBlockOrder()</code>.</li> <li>After normalization, and prior to rationalization, the parent/child links remain the primary traversal mechanism. The evaluation order of any nested expression-statements (usually stores) is enforced by the <code>GT_COMMA</code> in which they are contained.</li> <li>After rationalization, all <code>GT_COMMA</code> nodes are eliminated, statements are flattened, and the primary traversal mechanism becomes the <code>gtNext/gtPrev</code> links which define the execution order.</li> <li>In tree ordering:</li> <li>The <code>gtPrev</code> of the first node (<code>m_treeList</code>) is always <code>nullptr</code>.</li> <li>The <code>gtNext</code> of the last node (<code>m_rootNode</code>) is always <code>nullptr</code>.</li> </ul>"},{"location":"ryujit-overview/#lclvar-phase-dependent-properties","title":"LclVar phase-dependent properties","text":"<p>LclVar ref counts track the number of uses and weighted used of a local in the jit IR. There are two sequences of phases over which ref counts are valid, tracked via <code>lvaRefCountState</code>: an early sequence (state <code>RCS_EARLY</code>) and the normal sequence (state <code>RCS_NORMAL</code>). Requests for ref counts via <code>lvRefCnt</code> and <code>lvRefCntWtd</code> must be aware of the ref count state.</p> <p>Before struct promotion the ref counts are invalid. Struct promotion enables <code>RCS_EARLY</code> and it and subsequent phases through morph compute and uses ref counts on some locals to guide some struct optimizations. After morph the counts go back to longer being valid.</p> <p>The <code>RCS_NORMAL</code> sequence begins at normalization. Ref counts are computed and generally available via for the rest of the compilation phases. The counts are not incrementally maintained and may go stale as the IR is optimized or transformed, or maybe very approximate if the jit is not optimizing. They can be recomputed via <code>lvaComputeRefCounts</code> at points where accurate counts are valuable. Currently this happens before and after lower.</p>"},{"location":"ryujit-overview/#supporting-technologies-and-components","title":"Supporting technologies and components","text":""},{"location":"ryujit-overview/#instruction-encoding","title":"Instruction encoding","text":"<p>Instruction encoding is performed by the emitter (emit.h), using the <code>insGroup</code>/<code>instrDesc</code> representation. The code generator calls methods on the emitter to construct <code>instrDescs</code>. The encodings information is captured in the following:</p> <ul> <li>The \"instruction\" enumeration itemizes the different instructions available on each target, and is used as an index into the various encoding tables (e.g. <code>instInfo[]</code>, <code>emitInsModeFmtTab[]</code>) generated from the <code>instrs{tgt}.h</code> (e.g., instrsxarch.h).</li> <li>The skeleton encodings are contained in the tables, and then there are methods on the emitter that handle the special encoding constraints for the various instructions, addressing modes, register types, etc.</li> </ul>"},{"location":"ryujit-overview/#gc-info","title":"GC Info","text":"<p>Reporting of live GC references is done in two ways:</p> <ul> <li>For stack locations that are not tracked (these could be spill locations or lclVars \u2013 local variables or temps \u2013 that are not register candidates), they are initialized to <code>nullptr</code> in the prolog, and reported as live for the entire method.</li> <li>For lclVars with tracked lifetimes, or for expression involving GC references, we report the range over which the reference is live. This is done by the emitter, which adds this information to the instruction group, and which terminates instruction groups when the GC info changes.</li> </ul> <p>The tracking of GC reference lifetimes is done via the <code>GCInfo</code> class in the JIT. It is declared in src/jit/jitgcinfo.h (to differentiate it from src/inc/gcinfo.h), and implemented in src/jit/gcinfo.cpp.</p> <p>In a JitDump, the generated GC info can be seen following the \"In gcInfoBlockHdrSave()\" line.</p>"},{"location":"ryujit-overview/#debugger-info","title":"Debugger info","text":"<p>Debug info consists primarily of two types of information in the JIT:</p> <ul> <li>Mapping of IL offsets to native code offsets. This is accomplished via:</li> <li>the <code>m_ILOffsetX</code> on the statement nodes (<code>Statement</code>)</li> <li>the <code>gtLclILoffs</code> on lclVar references (<code>GenTreeLclVar</code>)</li> <li>The IL offsets are captured during CodeGen by calling <code>CodeGen::genIPmappingAdd()</code>, and then written to debug tables by <code>CodeGen::genIPmappingGen()</code>.</li> <li>Mapping of user locals to location (register or stack). This is accomplished via:</li> <li>Struct <code>siVarLoc</code> (in compiler.h) captures the location</li> <li><code>VarScopeDsc</code> (compiler.h) captures the live range of a local variable in a given location.</li> </ul>"},{"location":"ryujit-overview/#exception-handling","title":"Exception handling","text":"<p>Exception handling information is captured in an <code>EHblkDsc</code> for each exception handling region. Each region includes the first and last blocks of the try and handler regions, exception type, enclosing region, among other things. Look at jiteh.h and jiteh.cpp, especially, for details. Look at <code>Compiler::fgVerifyHandlerTab()</code> to see how the exception table constraints are verified.</p>"},{"location":"ryujit-overview/#reading-a-jitdump","title":"Reading a JitDump","text":"<p>One of the best ways of learning about the JIT compiler is examining a compilation dump in detail. The dump shows you all the really important details of the basic data structures without all the implementation detail of the code. Debugging a JIT bug almost always begins with a JitDump. Only after the problem is isolated by the dump does it make sense to start debugging the JIT code itself.</p> <p>Dumps are also useful because they give you good places to place breakpoints. If you want to see what is happening at some point in the dump, simply search for the dump text in the source code. This gives you a great place to put a conditional breakpoint.</p> <p>There is not a strong convention about what or how the information is dumped, but generally you can find phase-specific information by searching for the phase name. Some useful points follow.</p>"},{"location":"ryujit-overview/#how-to-create-a-jitdump","title":"How to create a JitDump","text":"<p>You can enable dumps by setting the <code>DOTNET_JitDump</code> environment variable to a space-separated list of the method(s) you want to dump. For example:</p> <pre><code>:: Print out lots of useful info when\n:: compiling methods named Main/GetEnumerator\nset \"DOTNET_JitDump=Main GetEnumerator\"\n</code></pre> <p>See Setting configuration variables for more details on this.</p> <p>Full instructions for dumping the compilation of some managed code can be found here: viewing-jit-dumps.md</p>"},{"location":"ryujit-overview/#reading-expression-trees","title":"Reading expression trees","text":"<p>Expression trees are displayed using a pre-order traversal, with the subtrees of a node being displayed under the node in operand order (e.g. <code>gtOp1</code>, <code>gtOp2</code>). This is similar to the way trees are displayed in typical user interfaces (e.g. folder trees). Note that the operand order may be different from the actual execution order, determined by <code>GTF_REVERSE_OPS</code> or other means. The operand order usually follows the order in high level languages so that the typical infix, left to right expression <code>a - b</code> becomes prefix, top to bottom tree:</p> <pre><code>\u258c  SUB       double\n\u251c\u2500\u2500\u258c  LCL_VAR   double V03 a\n\u2514\u2500\u2500\u258c  LCL_VAR   double V02 b\n</code></pre> <p>Calls initially display in source order - <code>Order(1, 2, 3, 4)</code> is:</p> <pre><code>[000004] --C-G-------              *  CALL      void   Program.Order\n[000000] ------------ arg0         +--*  CNS_INT   int    1\n[000001] ------------ arg1         +--*  CNS_INT   int    2\n[000002] ------------ arg2         +--*  CNS_INT   int    3\n[000003] ------------ arg3         \\--*  CNS_INT   int    4\n</code></pre> <p>but call morphing may change the order depending on the ABI so the above may become:</p> <pre><code>[000004] --CXG+------              *  CALL      void   Program.Order\n[000002] -----+------ arg2 on STK  +--*  CNS_INT   int    3\n[000003] -----+------ arg3 on STK  +--*  CNS_INT   int    4\n[000000] -----+------ arg0 in ecx  +--*  CNS_INT   int    1\n[000001] -----+------ arg1 in edx  \\--*  CNS_INT   int    2\n</code></pre> <p>where the node labels (e.g. arg0) help identifying the call arguments after reordering.</p> <p>Here is a full dump of an entire statement:</p> <pre><code>STMT00000 (IL 0x010...  ???)\n[000025] --C-G-------              \u2514\u2500\u2500\u258c  RETURN    double\n[000023] --C-G-------                 \u2514\u2500\u2500\u258c  CALL      double C.DblSqrt\n[000022] ------------ arg0               \u2514\u2500\u2500\u258c  MUL       double\n[000018] ------------                       \u251c\u2500\u2500\u258c  MUL       double\n[000014] ------------                       \u2502  \u251c\u2500\u2500\u258c  MUL       double\n[000010] ------------                       \u2502  \u2502  \u251c\u2500\u2500\u258c  LCL_VAR   double V03 loc0\n[000013] ------------                       \u2502  \u2502  \u2514\u2500\u2500\u258c  SUB       double\n[000011] ------------                       \u2502  \u2502     \u251c\u2500\u2500\u258c  LCL_VAR   double V03 loc0\n[000012] ------------                       \u2502  \u2502     \u2514\u2500\u2500\u258c  LCL_VAR   double V00 arg0\n[000017] ------------                       \u2502  \u2514\u2500\u2500\u258c  SUB       double\n[000015] ------------                       \u2502     \u251c\u2500\u2500\u258c  LCL_VAR   double V03 loc0\n[000016] ------------                       \u2502     \u2514\u2500\u2500\u258c  LCL_VAR   double V01 arg1\n[000021] ------------                       \u2514\u2500\u2500\u258c  SUB       double\n[000019] ------------                          \u251c\u2500\u2500\u258c  LCL_VAR   double V03 loc0\n[000020] ------------                          \u2514\u2500\u2500\u258c  LCL_VAR   double V02 arg2\n</code></pre> <p>Tree nodes are identified by their <code>gtTreeID</code>. This field only exists in DEBUG builds, but is quite useful for debugging, since all tree nodes are created via the <code>GenTree::GenTree</code> constructor (in src/jit/compiler.hpp). If you find a bad tree and wish to understand how it got corrupted, you can place a conditional breakpoint at the end of <code>GenTree::GenTree</code> to see when it is created, and then a data breakpoint on the field that you believe is corrupted.</p> <p>The trees are connected by line characters (either in ASCII, by default, or in slightly more readable Unicode when <code>DOTNET_JitDumpASCII=0</code> is specified), to make it a bit easier to read.</p>"},{"location":"ryujit-overview/#variable-naming","title":"Variable naming","text":"<p>The dump uses the index into the local variable table as its name. The arguments to the function come first, then the local variables, then any compiler generated temps. Thus in a function with 2 parameters (remember \"this\" is also a parameter), and one local variable, the first argument would be variable 0, the second argument variable 1, and the local variable would be variable 2. As described earlier, tracked variables are given a tracked variable index which identifies the bit for that variable in the dataflow bit vectors. This can lead to confusion as to whether the variable number is its index into the local variable table, or its tracked index. In the dumps when we refer to a variable by its local variable table index we use the 'V' prefix, and when we print the tracked index we prefix it by a 'T'.</p>"},{"location":"ryujit-overview/#references","title":"References","text":"<p> [1] P. Briggs, K. D. Cooper, T. J. Harvey, and L. T. Simpson, \"Practical improvements to the construction and destruction of static single assignment form,\" Software --- Practice and Experience, vol. 28, no. 8, pp. 859---881, Jul. 1998.</p> <p> [2] Wimmer, C. and M\u00f6ssenb\u00f6ck, D. \"Optimized Interval Splitting in a Linear Scan Register Allocator,\" ACM VEE 2005, pp. 132-141. http://portal.acm.org/citation.cfm?id=1064998&amp;dl=ACM&amp;coll=ACM&amp;CFID=105967773&amp;CFTOKEN=80545349</p>"},{"location":"shared-generics/","title":"Shared Generics Design","text":"<p>Author: Fadi Hanna - 2019</p>"},{"location":"shared-generics/#introduction","title":"Introduction","text":"<p>Shared generics is a runtime+JIT feature aimed at reducing the amount of code the runtime generates for generic methods of various instantiations (supports methods on generic types and generic methods). The idea is that for certain instantiations, the generated code will almost be identical with the exception of a few instructions, so in order to reduce the memory footprint, and the amount of time we spend jitting these generic methods, the runtime will generate a single special canonical version of the code, which can be used by all compatible instantiations of the method.</p>"},{"location":"shared-generics/#canonical-codegen-and-generic-dictionaries","title":"Canonical Codegen and Generic Dictionaries","text":"<p>Consider the following C# code sample:</p> <pre><code>string Method&lt;T&gt;()\n{\n    return typeof(List&lt;T&gt;).ToString();\n}\n</code></pre> <p>Without shared generics, the code for instantiations like <code>Method&lt;object&gt;</code> or <code>Method&lt;string&gt;</code> would look identical except for one single instruction: the one that loads the correct TypeHandle of type <code>List&lt;T&gt;</code>: <pre><code>    mov rcx, type handle of List&lt;string&gt; or List&lt;object&gt;\n    call ToString()\n    ret\n</code></pre></p> <p>With shared generics, the canonical code will not have any hard-coded versions of the type handle of <code>List&lt;T&gt;</code>, but instead looks up the exact type handle either through a call to a runtime helper API, or by loading it up from the generic dictionary of the instantiation of <code>Method&lt;T&gt;</code> that is executing. The code would look more like the following: <pre><code>    mov rcx, generic context                                                // MethodDesc of Method&lt;string&gt; or Method&lt;object&gt;\n    mov rcx, [rcx + offset of InstantiatedMethodDesc::m_pPerInstInfo]       // This is the generic dictionary\n    mov rcx, [rcx + dictionary slot containing type handle of List&lt;T&gt;]\n    call ToString()\n    ret\n</code></pre></p> <p>The generic context in this example is the <code>InstantiatedMethodDesc</code> of <code>Method&lt;object&gt;</code> or <code>Method&lt;string&gt;</code>. The generic dictionary is a data structure used by shared generic code to fetch instantiation-specific information. It is basically an array where the entries are instantiation-specific type handles, method handles, field handles, method entry points, etc... The \"PerInstInfo\" fields on MethodTable and <code>InstantiatedMethodDesc</code> structures point at the generic dictionary structure for a generic type and method respectively.</p> <p>In this example, the generic dictionary for <code>Method&lt;object&gt;</code> will contain a slot with the type handle for type <code>List&lt;object&gt;</code>, and the generic dictionary for <code>Method&lt;string&gt;</code> will contain a slot with the type handle for type <code>List&lt;string&gt;</code>.</p> <p>This feature is currently only supported for instantiations over reference types because they all have the same size/properties/layout/etc... For instantiations over primitive types or value types, the runtime will generate separate code bodies for each instantiation.</p>"},{"location":"shared-generics/#layouts-and-algorithms","title":"Layouts and Algorithms","text":""},{"location":"shared-generics/#dictionaries-pointers-on-types-and-methods","title":"Dictionaries Pointers on Types and Methods","text":"<p>The dictionary used by any given generic method is pointed at by the <code>m_pPerInstInfo</code> field on the <code>InstantiatedMethodDesc</code> structure of that method. It's a direct pointer to the contents of the generic dictionary data.</p> <p>On generic types, there's an extra level of indirection: the <code>m_pPerInstInfo</code> field on the <code>MethodTable</code> structure is a pointer to a table of dictionaries, and each entry in that table is a pointer to the actual generic dictionary data. This is because types have inheritance, and derived generic types inherit the dictionaries of their base types.</p> <p>Here's an example: <pre><code>class BaseClass&lt;T&gt; { }\n\nclass DerivedClass&lt;U&gt; : BaseClass&lt;U&gt; { }\n\nclass AnotherDerivedClass : DerivedClass&lt;string&gt; { }\n</code></pre></p> <p>The MethodTables of each of these types will look like the following:</p> BaseClass[T]'s MethodTable ... <code>m_PerInstInfo</code>: points at dictionary table below ... <code>dictionaryTable[0]</code>: points at dictionary data below <code>BaseClass's dictionary data here</code> **DerivedClass[U]'s MethodTable ** ... <code>m_PerInstInfo</code>: points at dictionary table below ... <code>dictionaryTable[0]</code>: points at dictionary data of <code>BaseClass</code> <code>dictionaryTable[1]</code>: points at dictionary data below <code>DerivedClass's dictionary data here</code> AnotherDerivedClass's MethodTable ... <code>m_PerInstInfo</code>: points at dictionary table below ... <code>dictionaryTable[0]</code>: points at dictionary data of <code>BaseClass</code> <code>dictionaryTable[1]</code>: points at dictionary data of <code>DerivedClass</code> <p>Note that <code>AnotherDerivedClass</code> doesn't have a dictionary of its own given that it is not a generic type, but inherits the dictionary pointers of its base types.</p>"},{"location":"shared-generics/#dictionary-slots","title":"Dictionary Slots","text":"<p>As described earlier, a generic dictionary is an array of multiple slots containing instantiation-specific information. When a dictionary is initially allocated for a certain generic type or method, all of its slots are initialized to <code>NULL</code>, and are lazily populated on demand as code executes (see: <code>Dictionary::PopulateEntry(...)</code>).</p> <p>The first N slots in an instantiation of N arguments are always going to be the type handles of the instantiation type arguments (this is kind of an optimization as well). The slots that follow contain instantiation-based information.</p> <p>For instance, here is an example of the contents of the generic dictionary for our <code>Method&lt;string&gt;</code> example:</p> <code>Method&lt;string&gt;'s dictionary</code> <code>slot[0]: TypeHandle(string)</code> <code>slot[1]: Total dictionary size</code> <code>slot[2]: TypeHandle(List&lt;string&gt;)</code> <code>slot[3]: NULL (not used)</code> <code>slot[4]: NULL (not used)</code> <p>Note: the size slot is never used by generic code, and is part of the dynamic dictionary expansion feature. More on that below.</p> <p>When this dictionary is first allocated, only <code>slot[0]</code> is initialized because it contains the instantiation type arguments (and of course the size slot is also initialized with the dictionary expansion feature), but the rest of the slots (for example, <code>slot[2]</code>) are <code>NULL</code>, and get lazily populated with values if we ever hit a code path that attempts to use them.</p> <p>When loading information from a slot that is still <code>NULL</code>, the generic code will call one of these runtime helper functions to populate the dictionary slot with a value: - <code>JIT_GenericHandleClass</code>: Used to lookup a value in a generic type dictionary. This helper is used by all instance methods on generic types. - <code>JIT_GenericHandleMethod</code>: Used to lookup a value in a generic method dictionary. This helper used by all generic methods, or non-generic static methods on generic types.</p> <p>When generating shared generic code, the JIT knows which slots to use for the various lookups, and the kind of information contained in each slot using the help of the <code>DictionaryLayout</code> implementation (genericdict.cpp).</p>"},{"location":"shared-generics/#dictionary-layouts","title":"Dictionary Layouts","text":"<p>The <code>DictionaryLayout</code> structure is what tells the JIT which slot to use when performing a dictionary lookup. This <code>DictionaryLayout</code> structure has a couple of important properties: - It is shared across all compatible instantiations of a certain type of method. In other words, a dictionary layout is associated with the canonical instantiation of a type or a method. For instance, in our example above, <code>Method&lt;object&gt;</code> and <code>Method&lt;string&gt;</code> are compatible instantiations, each with their own separate dictionaries, however they all share the same dictionary layout, which is associated with the canonical instantiation <code>Method&lt;__Canon&gt;</code>. - The dictionaries of generic types or methods have the same number of slots as their dictionary layouts. Note: historically before the introduction of the dynamic dictionary expansion feature, the generic dictionaries could be smaller than their layouts, meaning that for certain lookups, we had to invoke some runtime helper APIs (slow path).</p> <p>When a generic type or method is first created, its dictionary layout contains 'unassigned' slots. Assignments happen as part of code generation, whenever the JIT needs to emit a dictionary lookup sequence. This assignment happens during calls to the <code>DictionaryLayout::FindToken(...)</code> APIs. Once a slot has been assigned, it becomes associated with a certain signature, which describes the kind of value that will go in every instantiated dictionary at that slot index.</p> <p>Given an input signature, slot assignment is performed with the following algorithm:</p> <pre><code>Begin with slot = 0\nForeach entry in dictionary layout\n    If entry.signature != NULL\n        If entry.signature == inputSignature\n            return slot\n        EndIf\n    Else\n        entry.signature = inputSignature\n        return slot\n    EndIf\n    slot++\nEndForeach\n</code></pre> <p>So what happens when the above algorithm runs, but no existing slot with the same signature is found, and we're out of 'unassigned' slots? This is where the dynamic dictionary expansion kicks in to resize the layout by adding more slots to it, and resizing all dictionaries associated with this layout.</p>"},{"location":"shared-generics/#dynamic-dictionary-expansion","title":"Dynamic Dictionary Expansion","text":""},{"location":"shared-generics/#history","title":"History","text":"<p>Before the dynamic dictionary expansion feature, dictionary layouts were organized into buckets (a linked list of fixed-size <code>DictionaryLayout</code> structures). The size of the initial layout bucket was always fixed to some number which was computed based on some heuristics for generic types, and always fixed to 4 slots for generic methods. The generic types and methods also had fixed-size generic dictionaries which could be used for lookups (also known as \"fast lookup slots\").</p> <p>When a bucket gets filled with entries, we would just allocate a new <code>DictionaryLayout</code> bucket, and add it to the list. The problem however is that we couldn't resize the generic dictionaries of types or methods, because they are already allocated with a fixed size, and the JIT does not support generating instructions that could indirect into a linked-list of dictionaries. Given that limitation, we could only lookup a generic dictionary for a fixed number of values (the ones associated with the entries of the first <code>DictionaryLayout</code> bucket), and were forced to go through a slower runtime helper for additional lookups.</p> <p>This was acceptable, until we introduced the ReadyToRun and the Tiered Compilation technologies. Slots were getting assigned quickly when used by ReadyToRun code, and when the runtime decided re-jitted certain methods for better performance, it could not in some cases find any remaining \"fast lookup slots\", and was forced to generate code that goes through the slower runtime helpers. This ended up hurting performance in some scenarios, and a decision was made to not use the fast lookup slots for ReadyToRun code, and instead keep them reserved for re-jitted code. This decision however hurt the ReadyToRun performance, but it was a necessary compromise since we cared more about re-jitted code throughput over R2R throughput.</p> <p>For this reason, the dynamic dictionary expansion feature was introduced.</p>"},{"location":"shared-generics/#description-and-algorithms","title":"Description and Algorithms","text":"<p>The feature is simple in concept: change dictionary layouts from a linked list of buckets into dynamically expandable arrays instead. Sounds simple, but great care had to be taken when implementing it, because: - We can't just resize <code>DictionaryLayout</code> structures alone. If the size of the layout is larger than the size of the actual generic dictionary, this would cause the JIT to generate indirection instructions that do not match the size of the dictionary data, leading to access violations. - We can't just resize generic dictionaries on types and methods:     - For types, the generic dictionary is part of the <code>MethodTable</code> structure, which can't be reallocated (already in use by managed code)     - For methods, the generic dictionary is not part of the <code>MethodDesc</code> structure, but can still be in use by some generic code.     - We can't have multiple MethodTables or MethodDescs for the same type or method anyways, so reallocations are not an option. - We can't just resize the generic dictionary for a single instantiation. For instance, in our example above, let's say we wanted to expand the dictionary for <code>Method&lt;string&gt;</code>. The resizing of the layout would have an impact on the shared canonical code that the JIT generates for <code>Method&lt;__Canon&gt;</code>. If we only resized the dictionary of <code>Method&lt;string&gt;</code>, the shared generic code would work for that instantiation only, but when we attempt to use it with another instantiation like <code>Method&lt;object&gt;</code>, the jitted instructions would no longer match the size of the dictionary structure, and would cause access violations. - The runtime is multi-threaded, which adds to the complexity.</p> <p>The current implementation expands the dictionary layout and the actual dictionaries separately to keep things simple:</p> <ul> <li>Dictionary layouts are expanded when we are out of empty slots. See implementations of <code>DictionaryLayout::FindToken()</code> in genericdict.cpp.</li> <li>Instantiated type and method dictionaries are expanded lazily on demand whenever any code is attempting to read the value of a slot beyond the size of the dictionary of that type or method. This is done through a call to the helper functions mentioned previously (<code>JIT_GenericHandleClass</code> and <code>JIT_GenericHandleMethod</code>).</li> </ul> <p>The dictionary access codegen is equivalent to the following (both in JITted code and ReadyToRun code): <pre><code>void* pMethodDesc = &lt;some value&gt;;                  // Input MethodDesc for the instantiated generic method\nint requiredOffset = &lt;some value&gt;;                 // Offset we need to access\n\nvoid* pDictionary = pMethodDesc-&gt;m_pPerInstInfo;\n\n// Note how we check for the dictionary size first before indirecting at 'requiredOffset'\nif (pDictionary[sizeOffset] &lt;= requiredOffset || pDictionary[requiredOffset] == NULL)\n    pResult = JIT_GenericHandleMethod(pMethodDesc, &lt;signature&gt;);\nelse\n    pResult = pDictionary[requiredOffset];\n</code></pre></p> <p>This size check is not done unconditionally every time we need to read a value from the dictionary, otherwise this would cause a noticeable performance regression. When a dictionary layout is first allocated, we keep track of the initial number of slots that were allocated, and only perform the size checks if we are attempting to read the value of a slot beyond those initial number of slots.</p> <p>Dictionaries on types and methods are expanded by the <code>Dictionary::GetTypeDictionaryWithSizeCheck()</code> and <code>Dictionary::GetMethodDictionaryWithSizeCheck()</code> helper functions in genericdict.cpp.</p> <p>One thing to note regarding types is that they can inherit dictionary pointers from their base types. This means that if we resize the generic dictionary on any given generic type, we will need to propagate the new dictionary pointer to all of its derived types. This propagation is also done in a lazy way whenever the code calls into the <code>JIT_GenericHandleWorker</code> helper function with a derived type MethodTable pointer. In that helper, if we find that the dictionary pointer on the base type has been updated, we copy it to the derived type.</p> <p>Old dictionaries are not deallocated after resizing, but once a new dictionary gets published on a MethodTable or MethodDesc, any subsequent dictionary lookup by generic code will make use of that newly allocated dictionary. Deallocating old dictionaries would be extremely complicated, especially in a multi-threaded environment, and won't give any useful benefit.</p>"},{"location":"stackwalking/","title":"Stackwalking in the CLR","text":"<p>Author: Rudi Martin (@Rudi-Martin) - 2008</p> <p>The CLR makes heavy use of a technique known as stack walking (or stack crawling). This involves iterating the sequence of call frames for a particular thread, from the most recent (the thread's current function) back down to the base of the stack.</p> <p>The runtime uses stack walks for a number of purposes:</p> <ul> <li>The runtime walks the stacks of all threads during garbage collection, looking for managed roots (local variables holding object references in the frames of managed methods that need to be reported to the GC to keep the objects alive and possibly track their movement if the GC decides to compact the heap).</li> <li>On some platforms the stack walker is used during the processing of exceptions (looking for handlers in the first pass and unwinding the stack in the second).</li> <li>The debugger uses the functionality when generating managed stack traces.</li> <li>Various miscellaneous methods, usually those close to some public managed API, perform a stack walk to pick up information about their caller (such as the method, class or assembly of that caller).</li> </ul>"},{"location":"stackwalking/#the-stack-model","title":"The Stack Model","text":"<p>Here we define some common terms and describe the typical layout of a thread's stack.</p> <p>Logically, a stack is divided up into some number of frames. Each frame represents some function (managed or unmanaged) that is either currently executing or has called into some other function and is waiting for it to return. A frame contains state required by the specific invocation of its associated function. Typically this includes space for local variables, pushed arguments for a call to another function, saved caller registers etc.</p> <p>The exact definition of a frame varies from platform to platform and on many platforms there isn't a hard definition of a frame format that all functions adhere to (x86 is an example of this). Instead the compiler is often free to optimize the exact format of frames. On such systems it is not possible to guarantee that a stackwalk will return 100% correct or complete results (for debugging purposes, debug symbols such as pdbs are used to fill in the gaps so that debuggers can generate more accurate stack traces).</p> <p>This is not a problem for the CLR, however, since we do not require a fully generalized stack walk. Instead we are only interested in those frames that are managed (i.e. represent a managed method) or, to some extent, frames coming from unmanaged code used to implement part of the runtime itself. In particular there is no guarantee about fidelity of 3rd party unmanaged frames other than to note where such frames transition into or out of the runtime itself (i.e. one of the frame types we do care about).</p> <p>Because we control the format of the frames we're interested in (we'll delve into the details of this later) we can ensure that those frames are crawlable with 100% fidelity. The only additional requirement is a mechanism to link disjoint groups of runtime frames together such that we can skip over any intervening unmanaged (and otherwise uncrawlable) frames.</p> <p>The following diagram illustrates a stack containing all the frames types (note that this document uses a convention where stacks grow towards the top of the page):</p> <p></p>"},{"location":"stackwalking/#making-frames-crawlable","title":"Making Frames Crawlable","text":""},{"location":"stackwalking/#managed-frames","title":"Managed Frames","text":"<p>Because the runtime owns and controls the JIT (Just-in-Time compiler) it can arrange for managed methods to always leave a crawlable frame. One solution here would be to utilize a rigid frame format for all methods (e.g. the x86 EBP frame format). In practice, however, this can be inefficient, especially for small leaf methods (such as typical property accessors).</p> <p>Since methods are typically called more times than their frames are crawled (stack crawls are relatively rare in the runtime, at least with respect to the rate at which methods are typically called) it makes sense to trade method call performance for some additional crawl time processing. As a result the JIT generates additional metadata for each method it compiles that includes sufficient information for the stack crawler to decode a stack frame belonging to that method.</p> <p>This metadata can be found via a hash-table lookup with an instruction pointer somewhere within the method as the key. The JIT utilizes compression techniques in order to minimize the impact of this additional per-method metadata.</p> <p>Given initial values for a few important registers (e.g. EIP, ESP and EBP on x86 based systems) the stack crawler can locate a managed method and its associated JIT metadata and use this information to roll back the register values to those current in the method's caller. In this fashion a sequence of managed method frames can be traversed from the most recent to the oldest caller. This operation is sometimes referred to as a virtual unwind (virtual because we're not actually updating the real values of ESP etc., leaving the stack intact).</p>"},{"location":"stackwalking/#runtime-unmanaged-frames","title":"Runtime Unmanaged Frames","text":"<p>The runtime is partially implemented in unmanaged code (e.g. coreclr.dll). Most of this code is special in that it operates as manually managed code. That is, it obeys many of the rules and protocols of managed code but in an explicitly controlled fashion. For instance such code can explicitly enable or disable GC pre-emptive mode and needs to manage its use of object references accordingly.</p> <p>Another area where this careful interaction with managed code comes into play is during stackwalks. Since the majority of the runtime's unmanaged code is written in C++ we don't have the same control over method frame format as managed code. At the same time there are many instances where runtime unmanaged frames contain information that is important during a stack walk. These include cases where unmanaged functions hold object references in local variables (which must be reported during garbage collections) and exception processing.</p> <p>Rather than attempt to make each unmanaged frame crawable, unmanaged functions with interesting data to report to stack crawls bundle up the information into a data structure called a Frame. The choice of name is unfortunate as it can lead to ambiguity in stack related discussions. This document will always refer to the data structure variant as a capitalized Frame.</p> <p>Frame is actually the abstract base class of an entire hierarchy of Frame types. Frame is sub-typed in order to express different types of information that might be interesting to a stack walk.</p> <p>But how does the stack walker find these Frames and how do they relate to the frames utilized by managed methods?</p> <p>Each Frame is part of a singly linked list, having a next pointer to the next oldest Frame on this thread's stack (or null if the Frame is the oldest). The CLR Thread structure holds a pointer to the newest Frame. Unmanaged runtime code can push or pop Frames as needed by manipulating the Thread structure and Frame list.</p> <p>In this fashion the stack walker can iterate unmanaged Frames in newest to oldest order (the same order in which managed frames are iterated). But managed and unmanaged methods can be interleaved, and it would be wrong to process all managed frames followed by unmanaged Frames or vice versa since that would not accurately represent the real calling sequence.</p> <p>To solve this problem Frames are further restricted in that they must be allocated on the stack in the frame of the method that pushes them onto the Frame list. Since the stack walker knows the stack bounds of each managed frame it can perform simple pointer comparisons to determine whether a given Frame is older or newer than a given managed frame.</p> <p>Essentially the stack walker, having decoded the current frame, always has two possible choices for the next (older) frame: the next managed frame determined via a virtual unwind of the register set or the next oldest Frame on the Thread's Frame list. It can decide which is appropriate by determining which occupies stack space nearer the stack top. The actual calculation involved is platform dependent but usually devolves to one or two pointer comparisons.</p> <p>When managed code calls into the unmanaged runtime one of several forms of transition Frame is often pushed by the unmanaged target method. This is needed both to record the register state of the calling managed method (so that the stack walker can resume virtual unwinding of managed frames once it has finished enumerating the unmanaged Frames) and in many cases because managed object references are passed as arguments to the unmanaged method and must be reported to the GC in the event of a garbage collection.</p> <p>A full description of the available Frame types and their uses is beyond the scope of the document. Further details can be found in the frames.h header file.</p>"},{"location":"stackwalking/#stackwalker-interface","title":"Stackwalker Interface","text":"<p>The full stack walk interface is exposed to runtime unmanaged code only (a simplified subset is available to managed code via the System.Diagnostics.StackTrace class). The typical entrypoint is via the StackWalkFramesEx() method on the runtime Thread class.</p> <p>The caller of this method provides three main inputs:</p> <ol> <li>Some context indicating the starting point of the walk. This is either an initial register set (for instance if you've suspended the target thread and can call GetThreadContext() on it) or an initial Frame (in cases where you know the code in question is in runtime unmanaged code). Although most stack walks are made from the top of the stack it's possible to start lower down if you can determine the correct starting context.</li> <li>A function pointer and associated context. The function provided is called by the stack walker for each interesting frame (in order from the newest to the oldest). The context value provided is passed to each invocation of the callback so that it can record or build up state during the walk.</li> <li>Flags indicating what sort of frames should trigger a callback. This allows the caller to specify that only pure managed method frames should be reported for instance. For a full list see threads.h (just above the declaration of StackWalkFramesEx()).</li> </ol> <p>StackWalkFramesEx() returns an enum value that indicates whether the walk terminated normally (got to the stack base and ran out of methods to report), was aborted by one of the callbacks (the callbacks return an enum of the same type to the stack walk to control this) or suffered some other miscellaneous error.</p> <p>Aside from the context value passed to StackWalkFramesEx(), stack callback functions are passed one other piece of context: the CrawlFrame. This class is defined in stackwalk.h and contains all sorts of context gathered as the stack walk proceeds.</p> <p>For instance the CrawlFrame indicates the MethodDesc for managed frames and the Frame for unmanaged Frames. It also provides the current register set inferred by virtually unwinding frames up to that point.</p>"},{"location":"stackwalking/#stackwalk-implementation-details","title":"Stackwalk Implementation Details","text":"<p>Further low-level details of the stack walk implementation are currently outside the scope of this document. If you have knowledge of these and would care to share that knowledge please feel free to update this document.</p>"},{"location":"threading/","title":"CLR Threading Overview","text":""},{"location":"threading/#managed-vs-native-threads","title":"Managed vs. Native Threads","text":"<p>Managed code executes on \"managed threads,\" which are distinct from the native threads provided by the operating system. A native thread is a thread of execution of native code on a physical machine; a managed thread is a virtual thread of execution on the CLR's virtual machine.</p> <p>Just as the JIT compiler maps \"virtual\" IL instructions into native instructions that execute on the physical machine, the CLR's threading infrastructure maps \"virtual\" managed threads onto the native threads provided by the operating system.</p> <p>At any given time, a managed thread may or may not be assigned to a native thread for execution. For example, a managed thread that has been created (via \"new System.Threading.Thread\") but not yet started (via System.Threading.Thread.Start) is a managed thread that has not yet been assigned to a native thread. Similarly, a managed thread may, in principle, move between multiple native threads over the course of its execution, though in practice the CLR does not currently support this.</p> <p>The public Thread interface available to managed code intentionally hides the details of the underlying native threads. because:</p> <ul> <li>Managed threads are not necessarily mapped to a single native thread (and may not be mapped to a native thread at all).</li> <li>Different operating systems expose different abstractions for native threads.</li> <li>In principle, managed threads are \"virtualized\".</li> </ul> <p>The CLR provides equivalent abstractions for managed threads, implemented by the CLR itself. For example, it does not expose the operating system's thread-local storage (TLS) mechanism, but instead provides managed \"thread-static\" variables. Similarly, it does not expose the native thread's \"thread ID,\" but instead provides a \"managed thread ID\" which is generated independently of the OS. However, for diagnostic purposes, some details of the underlying native thread may be obtained via types in the System.Diagnostics namespace.</p> <p>Managed threads require additional functionality typically not needed by native threads. First, managed threads hold GC references on their stacks, so the CLR must be able to enumerate (and possibly modify) these references every time a GC occurs. To do this, the CLR must \"suspend\" each managed thread (stop it at a point where all of its GC references can be found).  Second, when an AppDomain is unloaded, the CLR must ensure that no thread is executing code in that AppDomain. This requires the ability to force a thread to unwind out of that AppDomain. The CLR does this by injecting a ThreadAbortException into such threads.</p>"},{"location":"threading/#data-structures","title":"Data Structures","text":"<p>Every managed thread has an associated Thread object, defined in threads.h. This object tracks everything the VM needs to know about the managed thread. This includes things that are necessary, such as the thread's current GC mode and Frame chain, as well as many things that are allocated per-thread simply for performance reasons (such as some fast arena-style allocators).</p> <p>All Thread objects are stored in the ThreadStore (also defined in threads.h), which is a simple list of all known Thread objects. To enumerate all managed threads, one must first acquire the ThreadStoreLock, then use ThreadStore::GetAllThreadList to enumerate all Thread objects. This list may include managed threads which are not currently assigned to native threads (for example, they may not yet be started, or the native thread may already have exited).</p> <p>Each managed thread that is currently assigned to a native thread is reachable via a native thread-local storage (TLS) slot on that native thread. This allows code that is executing on that native thread to get the corresponding Thread object, via GetThread().</p> <p>Additionally, many managed threads have a managed Thread object (System.Threading.Thread) which is distinct from the native Thread object. The managed Thread object provides methods for managed code to interact with the thread, and is mostly a wrapper around functionality offered by the native Thread object. The current managed Thread object is reachable (from managed code) via Thread.CurrentThread.</p> <p>In a debugger, the SOS extension command \"!Threads\" can be used to enumerate all Thread objects in the ThreadStore.</p>"},{"location":"threading/#thread-lifetimes","title":"Thread Lifetimes","text":"<p>A managed thread is created in the following situations:</p> <ol> <li>Managed code explicitly asks the CLR to create a new thread via System.Threading.Thread.</li> <li>The CLR creates the managed thread directly (see \"special threads\" below).</li> <li>Native code calls managed code on a native thread which is not yet associated with a managed thread (via \"reverse p/invoke\" or COM interop).</li> <li>A managed process starts (invoking its Main method on the process' Main thread).</li> </ol> <p>In cases #1 and #2, the CLR is responsible for creating a native thread to back the managed thread. This is not done until the thread is actually started. In such cases, the native thread is \"owned\" by the CLR; the CLR is responsible for the native thread's lifetime. In these cases, the CLR is aware of the existence of the thread by virtue of the fact that the CLR created it in the first place.</p> <p>In cases #3 and #4, the native thread already existed prior to the creation of the managed thread, and is owned by code external to the CLR. The CLR is not responsible for the native thread's lifetime. The CLR becomes aware of these threads the first time they attempt to call managed code.</p> <p>When a native thread dies, the CLR is notified via its DllMain function. This happens inside of the OS \"loader lock,\" so there is little that can be done (safely) while processing this notification. So rather than destroying the data structures associated with the managed thread, the thread is simply marked as \"dead\" and signals the finalizer thread to run. The finalizer thread then sweeps through the threads in the ThreadStore and destroys any that are both dead and unreachable via managed code.</p>"},{"location":"threading/#suspension","title":"Suspension","text":"<p>The CLR must be able to find all references to managed objects in order to perform a GC. Managed code is constantly accessing the GC heap, and manipulating references stored on the stack and in registers. The CLR must ensure that all managed threads are stopped (so they aren't modifying the heap) to safely and reliably find all managed objects. It only stops at safe points, when registers and stack locations can be inspected for live references.</p> <p>Another way of putting this is that the GC heap, and every thread's stack and register state, are \"shared state,\" accessed by multiple threads. As with most shared state, some sort of \"lock\" is required to protect it. Managed code must hold this lock while accessing the heap, and can only release the lock at safe points.</p> <p>The CLR refers to this \"lock\" as the thread's \"GC mode.\" A thread which is in \"cooperative mode\" holds its lock; it must \"cooperate\" with the GC (by releasing the lock) in order for a GC to proceed. A thread which is in \"preemptive\" mode does not hold its lock \u2013 the GC may proceed \"preemptively\" because the thread is known to not be accessing the GC heap.</p> <p>A GC may only proceed when all managed threads are in \"preemptive\" mode (not holding the lock). The process of moving all managed threads to preemptive mode is known as \"GC suspension\" or \"suspending the Execution Engine (EE).\"</p> <p>A na\u00efve implementation of this \"lock\" would be for each managed thread to actually acquire and release a real lock around each access to the GC heap. Then the GC would simply attempt to acquire the lock on each thread; once it had acquired all threads' locks, it would be safe to perform the GC.</p> <p>However, this na\u00efve approach is unsatisfactory for two reasons. First, it would require managed code to spend a lot of time acquiring and releasing the lock (or at least checking whether the GC was attempting to acquire the lock \u2013 known as \"GC polling.\") Second, it would require the JIT to emit \"GC info\" describing the layout of the stack and registers for every point in JIT'd code; this information would consume large amounts of memory.</p> <p>We refined this na\u00efve approach by separating JIT'd managed code into \"partially interruptible\" and \"fully interruptible\" code. In partially interruptible code, the only safe points are calls to other methods, and explicit \"GC poll\" locations where the JIT emits code to check whether a GC is pending. GC info need only be emitted for these locations. In fully interruptible code, every instruction is a safe point, and the JIT emits GC info for every instruction \u2013 but it does not emit GC polls. Instead, fully interruptible code may be \"interrupted\" by hijacking the thread (a process which is discussed later in this document). The JIT chooses whether to emit fully- or partially-interruptible code based on heuristics to find the best tradeoff between code quality, size of the GC info, and GC suspension latency.</p> <p>Given the above, there are three fundamental operations to define: entering cooperative mode, leaving cooperative mode, and suspending the EE.</p>"},{"location":"threading/#entering-cooperative-mode","title":"Entering Cooperative Mode","text":"<p>A thread enters cooperative mode by calling Thread::DisablePreemptiveGC. This acquires the \"lock\" for the current thread, as follows:</p> <ol> <li>If a GC is in progress (the GC holds the lock) then block until the GC is complete.</li> <li>Mark the thread as being in cooperative mode. No GC may proceed until the thread reenters preemptive mode.</li> </ol> <p>These two steps proceed as if they were atomic.</p>"},{"location":"threading/#entering-preemptive-mode","title":"Entering Preemptive Mode","text":"<p>A thread enters preemptive mode (releases the lock) by calling Thread::EnablePreemptiveGC. This simply marks the thread as no longer being in cooperative mode, and informs the GC thread that it may be able to proceed.</p>"},{"location":"threading/#suspending-the-ee","title":"Suspending the EE","text":"<p>When a GC needs to occur, the first step is to suspend the EE. This is done by GCHeap::SuspendEE, which proceeds as follows:</p> <ol> <li>Set a global flag (g_fTrapReturningThreads) to indicate that a GC is in progress. Any threads that attempt to enter cooperative mode will block until the GC is complete.</li> <li>Find all threads currently executing in cooperative mode. For each such thread, attempt to hijack the thread and force it to leave cooperative mode.</li> <li>Repeat until no threads are running in cooperative mode.</li> </ol>"},{"location":"threading/#hijacking","title":"Hijacking","text":"<p>Hijacking for GC suspension is done by Thread::SysSuspendForGC. This method attempts to force any managed thread that is currently running in cooperative mode, to leave cooperative mode at a \"safe point.\" It does this by enumerating all managed threads (walking the ThreadStore), and for each managed thread currently running in cooperative mode.</p> <ol> <li>Suspend the underlying native thread. This is done with the Win32 SuspendThread API. This API forcibly stops the thread from running, at some random point in its execution (not necessarily a safe point).</li> <li>Get the current CONTEXT for the thread, via GetThreadContext. This is an OS concept; CONTEXT represents the current register state of the thread. This allows us to inspect its instruction pointer, and thus determine what type of code it is currently executing.</li> <li>Check again if the thread is in cooperative mode, as it may have already left cooperative mode before it could be suspended. If so, the thread is in dangerous territory: the thread may be executing arbitrary native code, and must be resumed immediately to avoid deadlocks.</li> <li>Check if the thread is running managed code. It is possible that it is executing native VM code in cooperative mode (see Synchronization, below), in which case the thread must be immediately resumed as in the previous step.</li> <li>Now the thread is suspended in managed code. Depending on whether that code is fully- or partially-interruptible, one of the following is performed:</li> <li>If fully interruptible, it is safe to perform a GC at any point, since the thread is, by definition, at a safe point. It is reasonable to leave the thread suspended at this point (because it's safe) but various historical OS bugs prevent this from working, because the CONTEXT retrieved earlier may be corrupt. Instead, the thread's instruction pointer is overwritten, redirecting it to a stub that will capture a more complete CONTEXT, leave cooperative mode, wait for the GC to complete, reenter cooperative mode, and restore the thread to its previous state.</li> <li>If partially-interruptible, the thread is, by definition, not at a safe point. However, the caller will be at a safe point (method transition). Using that knowledge, the CLR \"hijacks\" the top-most stack frame's return address (physically overwrite that location on the stack) with a stub similar to the one used for fully-interruptible code. When the method returns, it will no longer return to its actual caller, but rather to the stub (the method may also perform a GC poll, inserted by the JIT, before that point, which will cause it to leave cooperative mode and undo the hijack).</li> </ol>"},{"location":"threading/#threadabort-appdomain-unload","title":"ThreadAbort / AppDomain-Unload","text":"<p>In order to unload an AppDomain, the CLR must ensure that no thread is running in that AppDomain. To accomplish this, all managed threads are enumerated, and \"abort\" any threads which have stack frames belonging to the AppDomain being unloaded. A ThreadAbortException is \"injected\" into the running thread, which  causes the thread to unwind (executing backout code along the way) until it is no longer executing in the AppDomain, at which point the ThreadAbortException is translated into an AppDomainUnloaded exception.</p> <p>ThreadAbortException is a special type of exception. It can be caught by user code, but the CLR ensures that the exception will be rethrown after the user's exception handler is executed. Thus ThreadAbortException is sometimes referred to as \"uncatchable,\" though this is not strictly true.</p> <p>A ThreadAbortException is typically 'thrown' by simply setting a bit on the managed thread marking it as \"aborting.\" This bit is checked by various parts of the CLR (most notably, every return from a p/invoke) and often times setting this bit is all that is needed to get the thread aborted in a timely manner.</p> <p>However, if the thread is, for example, executing a long-running managed loop, it may never check this bit. To get such a thread to abort faster, the thread is \"hijacked\" and forced to raise a ThreadAbortException. This hijacking is done in the same way as GC suspension, except that the stubs that the thread is redirected to will cause a ThreadAbortException to be raised, rather than waiting for a GC to complete.</p> <p>This hijacking means that a ThreadAbortException can be raised at essentially any arbitrary point in managed code. This makes it extremely difficult for managed code to deal successfully with a ThreadAbortException. It is therefore unwise to use this mechanism for any purpose other than AppDomain-Unload, which ensures that any state corrupted by the ThreadAbort will be cleaned up along with the AppDomain.</p>"},{"location":"threading/#synchronization-managed","title":"Synchronization: Managed","text":"<p>Managed code has access to many synchronization primitives, collected within the System.Threading namespace. These include wrappers for native OS primitives like Mutex, Event, and Semaphore objects, as well as some abstractions such as Barriers and SpinLocks. However, the primary synchronization mechanism used by most managed code is System.Threading.Monitor, which provides a high-performance locking facility on any managed object, and additionally provides \"condition variable\" semantics for signaling changes in the state protected by a lock.</p> <p>Monitor is implemented as a \"hybrid lock;\" it has features of both a spin-lock and a kernel-based lock like a Mutex. The idea is that most locks are held only briefly, so it takes less time to simply spin-wait for the lock to be released, than it would to make a call into the kernel to block the thread. It is important not to waste CPU cycles spinning, so if the lock has not been acquired after a brief period of spinning, the implementation falls back to blocking in the kernel.</p> <p>Because any object may potentially be used as a lock/condition variable, every object must have a location in which to store the lock information. This is done with \"object headers\" and \"sync blocks.\"</p> <p>The object header is a machine-word-sized field that precedes every managed object. It is used for many purposes, such as storing the object's hash code. One such purpose is holding the object's lock state. If more per-object data is needed than will fit in the object header, we \"inflate\" the object by creating a \"sync block.\"</p> <p>Sync blocks are stored in the Sync Block Table, and are addressed by sync block indexes. Each object with an associated sync block has the index of that index in the object's object header.</p> <p>The details of object headers and sync blocks are defined in syncblk.h/.cpp.</p> <p>If there is room on the object header, Monitor stores the managed thread ID of the thread that currently holds the lock on the object (or zero (0) if no thread holds the lock). Acquiring the lock in this case is a simple matter of spin-waiting until the object header's thread ID is zero, and then atomically setting it to the current thread's managed thread ID.</p> <p>If the lock cannot be acquired in this manner after some number of spins, or the object header is already being used for other purposes, a sync block must be created for the object. This has additional data, including an event that can be used to block the current thread, allowing us to stop spinning and efficiently wait for the lock to be released.</p> <p>An object that is used as a condition variable (via Monitor.Wait and Monitor.Pulse) must always be inflated, as there is not enough room in the object header to hold the required state.</p>"},{"location":"threading/#synchronization-native","title":"Synchronization: Native","text":"<p>The native portion of the CLR must also be aware of threading, as it will be invoked by managed code on multiple threads. This requires native synchronization mechanisms, such as locks, events, etc.</p> <p>The ITaskHost API allows a host to override many aspects of managed threading, including thread creation, destruction, and synchronization. The ability of a host to override native synchronization means that VM code can generally not use native synchronization primitives (Critical Sections, Mutexes, Events, etc.) directly, but rather must use the VM's wrappers over these.</p> <p>Additionally, as described above, GC suspension is a special kind of \"lock\" that affects nearly every aspect of the CLR. Native code in the VM may enter \"cooperative\" mode if it must manipulate GC heap objects, and thus the \"GC suspension lock\" becomes one of the most important synchronization mechanisms in native VM code, as well as managed.</p> <p>The major synchronization mechanisms used in native VM code are the GC mode, and Crst.</p>"},{"location":"threading/#gc-mode","title":"GC Mode","text":"<p>As discussed above, all managed code runs in cooperative mode, because it may manipulate the GC heap. Generally, native code does not touch managed objects, and thus runs in preemptive mode. But some native code in the VM must access the GC heap, and thus must run in cooperative mode.</p> <p>Native code generally does not manipulate the GC mode directly, but rather uses two macros: GCX_COOP and GCX_PREEMP.  These enter the desired mode, and erect \"holders\" to cause the thread to revert to the previous mode when the scope is exited.</p> <p>It is important to understand that GCX_COOP effectively acquires a lock on the GC heap. No GC may proceed while the thread is in cooperative mode. And native code cannot be \"hijacked\" as is done for managed code, so the thread will remain in cooperative mode until it explicitly switches back to preemptive mode.</p> <p>Thus entering cooperative mode in native code is discouraged. In cases where cooperative mode must be entered, it should be kept to as short a time as possible. The thread should not be blocked in this mode, and in particular cannot generally acquire locks safely.</p> <p>Similarly, GCX_PREEMP potentially releases a lock that had been held by the thread. Great care must be taken to ensure that all GC references are properly protected before entering preemptive mode.</p> <p>The Rules of the Code document describes the disciplines needed to ensure safety around GC mode switches.</p>"},{"location":"threading/#crst","title":"Crst","text":"<p>Just as Monitor is the preferred locking mechanism for managed code, Crst is the preferred mechanism for VM code. Like Monitor, Crst is a hybrid lock that is aware of hosts and GC modes. Crst also implements deadlock avoidance via \"lock leveling,\" described in the Crst Leveling chapter of the BotR.</p> <p>It is generally illegal to acquire a Crst while in cooperative mode, though exceptions are made where absolutely necessary.</p>"},{"location":"threading/#special-threads","title":"Special Threads","text":"<p>In addition to managing threads created by managed code, the CLR creates several \"special\" threads for its own use.</p>"},{"location":"threading/#finalizer-thread","title":"Finalizer Thread","text":"<p>This thread is created in every process that runs managed code. When the GC determines that a finalizable object is no longer reachable, it places that object on a finalization queue. At the end of a GC, the finalizer thread is signaled to process all finalizers currently in this queue. Each object is then dequeued, one by one, and its finalizer is executed.</p> <p>This thread is also used to perform various CLR-internal housekeeping tasks, and to wait for notifications of some external events (such as a low-memory condition, which signals the GC to collect more aggressively). See GCHeap::FinalizerThreadStart for the details.</p>"},{"location":"threading/#gc-threads","title":"GC Threads","text":"<p>When running in \"concurrent\" or \"server\" modes, the GC creates one or more background threads to perform various stages of garbage collection in parallel. These threads are wholly owned and managed by the GC, and never run managed code.</p>"},{"location":"threading/#debugger-thread","title":"Debugger Thread","text":"<p>The CLR maintains a single native thread in each managed process, which performs various tasks on behalf of attached managed debuggers.</p>"},{"location":"threading/#appdomain-unload-thread","title":"AppDomain-Unload Thread","text":"<p>This thread is responsible for unloading AppDomains. This is done on a separate, CLR-internal thread, rather than the thread that requests the AD-unload, to a) provide guaranteed stack space for the unload logic, and b) allow the thread that requested the unload to be unwound out of the AD, if needed.</p>"},{"location":"threading/#threadpool-threads","title":"ThreadPool Threads","text":"<p>The CLR's ThreadPool maintains a collection of managed threads for executing user \"work items.\"  These managed threads are bound to native threads owned by the ThreadPool. The ThreadPool also maintains a small number of native threads to handle functions like \"thread injection,\" timers, and \"registered waits.\"</p>"},{"location":"type-loader/","title":"Type Loader Design","text":"<p>Author: Ladi Prosek - 2007</p>"},{"location":"type-loader/#introduction","title":"Introduction","text":"<p>In a class-based object oriented system, types are templates describing the data that individual instances will contain, and the functionality that they will provide. It is not possible to create an object without first defining its type<sup>1</sup>.  Two objects are said to be of the same type if they are instances of the same type. The fact that they define the exact same set of members does not make them related in any way.</p> <p>The previous paragraph could as well describe a typical C++ system. One additional feature essential to CLR is the availability of full runtime type information. In order to \"manage\" the managed code and provide type safe environment, the runtime must know the type of any object at any time. Such a type information must be readily available without extensive computation because the type identity queries are expected to be rather frequent (e.g. any type-cast involves querying the type identity of the object to verify that the cast is safe and can be done).</p> <p>This performance requirement rules out any dictionary look up approaches and leaves us with the following high-level architecture.</p> <p></p> <p>Figure 1 The abstract high-level object design</p> <p>Apart from the actual instance data, each object contains a type id which is simply a pointer to the structure that represents the type. This concept is similar to C++ v-table pointers, but the structure, which we call TYPE now and will define it more precisely later, contains more than just a v-table. For instance, it has to contain information about the hierarchy so that \"is-a\" subsumption questions can be answered.</p> <p><sup>1</sup> The C# 3.0 feature called \"anonymous types\" lets you define an object without explicit reference to a type - simply by directly listing its fields. Don't let this fool you, there is in fact a type created behind the scenes for you by the compiler.</p>"},{"location":"type-loader/#11-related-reading","title":"1.1 Related Reading","text":"<p>[1] Martin Abadi, Luca Cardelli, A Theory of Objects, ISBN 978-0387947754</p> <p>[2] Andrew Kennedy (@andrewjkennedy), Don Syme (@dsyme), Design and Implementation of Generics for the .NET Common Language Runtime</p> <p>[3] ECMA Standard for the Common Language Infrastructure (CLI)</p>"},{"location":"type-loader/#12-design-goals","title":"1.2 Design Goals","text":"<p>The ultimate purpose of the type loader (sometimes referred to as the class loader, which is strictly speaking not correct, because classes constitute just a subset of types - namely reference types - and the loader loads value types as well) is to build data structures representing the type which it is asked to load. These are the properties that the loader should have:</p> <ul> <li>Fast type lookup ([module, token] =&gt; handle and [assembly, name] =&gt; handle).</li> <li>Optimized memory layout to achieve good working set size, cache hit rate, and JITted code performance.</li> <li>Type safety - malformed types are not loaded and a TypeLoadException is thrown.</li> <li>Concurrency - scales well in multi-threaded environments.</li> </ul>"},{"location":"type-loader/#2-type-loader-architecture","title":"2 Type Loader Architecture","text":"<p>There is a relatively small number of entry-points to the loader. Although the signature of each individual entry-point is slightly different, they all have the similar semantics. They take a type/member designation in the form of a metadata token or a name string, a scope for the token (a module or an assembly ), and some additional information like flags. They return the loaded entity in the form of a handle.</p> <p>There are usually many calls to the type loader during JITting. Consider:</p> <pre><code>object CreateClass()\n{\n    return new MyClass();\n}\n</code></pre> <p>In the IL, MyClass is referred to using a metadata token. In order to generate a call to the <code>JIT_New</code> helper which takes care of the actual instantiation, the JIT will ask the type loader to load the type and return a handle to it. This handle will be then directly embedded in the JITted code as an immediate value. The fact that types and members are usually resolved and loaded at JIT time and not at run-time also explains the sometimes confusing behavior easily hit with code like this:</p> <pre><code>object CreateClass()\n{\n    try {\n        return new MyClass();\n    } catch (TypeLoadException) {\n        return null;\n    }\n}\n</code></pre> <p>If <code>MyClass</code> fails to load, for example because it's supposed to be defined in another assembly and it was accidentally removed in the newest build, then this code will still throw <code>TypeLoadException</code>. The reason that the catch block did not catch it is that it never ran! The exception occurred during JITting and would only be catchable in the method that called <code>CreateClass</code> and caused it to be JITted. In addition, it may not be always obvious at which point the JITting is triggered due to inlining, so users should not expect and rely on deterministic behavior.</p>"},{"location":"type-loader/#key-data-structures","title":"Key Data Structures","text":"<p>The most universal type designation in the CLR is the <code>TypeHandle</code>. It's an abstract entity which encapsulates a pointer to either a <code>MethodTable</code> (representing \"ordinary\" types like <code>System.Object</code> or <code>List&lt;string&gt;</code>) or a <code>TypeDesc</code> (representing byrefs, pointers, function pointers and generic variables). It constitutes the identity of a type in that two handles are equal if and only if they represent the same type. To save space, the fact that a <code>TypeHandle</code> contains a <code>TypeDesc</code> is indicated by setting the second lowest bit of the pointer to 1 (i.e. (ptr | 2)) instead of using additional flags<sup>2</sup>. <code>TypeDesc</code> is \"abstract\" and has the following inheritance hierarchy.</p> <p></p> <p>Figure 2 The TypeDesc hierarchy</p> <p><code>TypeDesc</code></p> <p>Abstract type descriptor. The concrete descriptor type is determined by flags.</p> <p><code>TypeVarTypeDesc</code></p> <p>Represents a type variable, i.e. the <code>T</code> in <code>List&lt;T&gt;</code> or in <code>Array.Sort&lt;T&gt;</code> (see the part about generics below). Type variables are never shared between multiple types or methods so each variable has its one and only owner.</p> <p><code>FnPtrTypeDesc</code></p> <p>Represents a function pointer, essentially a variable-length list of type handles referring to the return type and parameters. It was originally only used by managed C++. C# supported it since C# 9.</p> <p><code>ParamTypeDesc</code></p> <p>This descriptor represents a byref and pointer types. Byrefs are the results of the <code>ref</code> and <code>out</code> C# keywords applied to method parameters<sup>3</sup> whereas pointer types are unmanaged pointers to data used in unsafe C# and managed C++.</p> <p><code>MethodTable</code></p> <p>This is by far the central data structure of the runtime. It represents any type which does not fall into one of the categories above (this includes primitive types, and generic types, both \"open\" and \"closed\"). It contains everything about the type that needs to be looked up quickly, such as its parent type, implemented interfaces, and the v-table.</p> <p><code>EEClass</code></p> <p><code>MethodTable</code> data are split into \"hot\" and \"cold\" structures to improve working set and cache utilization. <code>MethodTable</code> itself is meant to only store \"hot\" data that are needed in program steady state. <code>EEClass</code> stores \"cold\" data that are typically only needed by type loading, JITing or reflection. Each <code>MethodTable</code> points to one <code>EEClass</code>.</p> <p>Moreover, <code>EEClass</code>es are shared by generic types. Multiple generic type <code>MethodTable</code>s can point to single <code>EEClass</code>. This sharing adds additional constrains on data that can be stored on <code>EEClass</code>.</p> <p><code>MethodDesc</code></p> <p>It is no surprise that this structure describes a method. It actually comes in a few flavors which have their corresponding <code>MethodDesc</code> subtypes but most of them really are out of the scope of this document. Suffice it to say that there is one subtype called <code>InstantiatedMethodDesc</code> which plays an important role for generics. For more information please see Method Descriptor Design.</p> <p><code>FieldDesc</code></p> <p>Analogous to <code>MethodDesc</code> , this structure describes a field. Except for certain COM interop scenarios, the EE does not care about properties and events at all because they boil down to methods and fields at the end of the day, and it's just compilers and reflection who generate and understand them in order to provide that syntactic sugar kind of experience.</p> <p><sup>2</sup> This is useful for debugging. If the value of a <code>TypeHandle</code> ends with 2, 6, A, or E, then it's not a <code>MethodTable</code> and the extra bit has to be cleared in order to successfully inspect the <code>TypeDesc</code>.</p> <p><sup>3</sup> Note that the difference between <code>ref</code> and <code>out</code> is just in a parameter attribute. As far as the type system is concerned, they are both the same type.</p>"},{"location":"type-loader/#21-load-levels","title":"2.1 Load Levels","text":"<p>When the type loader is asked to load a specified type, identified for example by a typedef/typeref/typespec token and a Module , it does not do all the work atomically at once. The loading is done in phases instead. The reason for this is that the type usually depends on other types and requiring it to be fully loaded before it can be referred to by other types would result in infinite recursion and deadlocks. Consider:</p> <pre><code>class A&lt;T&gt; : C&lt;B&lt;T&gt;&gt;\n{ }\n\nclass B&lt;T&gt; : C&lt;A&lt;T&gt;&gt;\n{ }\n\nclass C&lt;T&gt;\n{ }\n</code></pre> <p>These are valid types and apparently <code>A</code> depends on <code>B</code> and <code>B</code> depends on <code>A</code>.</p> <p>The loader initially creates the structure(s) representing the type and initializes them with data that can be obtained without loading other types. When this \"no-dependencies\" work is done, the structure(s) can be referred from other places, usually by sticking pointers to them into another structures. After that the loader progresses in incremental steps and fills the structure(s) with more and more information until it finally arrives at a fully loaded type. In the above example, the base types of <code>A</code> and <code>B</code> will be approximated by something that does not include the other type, and substituted by the real thing later.</p> <p>The exact half-loaded state is described by the so-called load level, starting with CLASS_LOAD_BEGIN, ending with CLASS_LOADED, and having a couple of intermediate levels in between. There are rich and useful comments about individual load levels in the classloadlevel.h source file.</p> <p>See Design and Implementation of Generics for the .NET Common Language Runtime for more detailed explanation of load levels.</p>"},{"location":"type-loader/#211-use-of-load-levels-within-the-type-loader","title":"2.1.1 Use of load levels within the type loader.","text":"<p>Within the type loader, while operating in various portions of the type loader, various different rules apply to what type load level can be used.</p>"},{"location":"type-loader/#2111-code-within-classloadercreatetypehandlefortypedefthrowing-and-methodtablebuilderbuildmethodtablethrowing","title":"2.1.1.1 Code within <code>ClassLoader::CreateTypeHandleForTypeDefThrowing</code> and <code>MethodTableBuilder::BuildMethodTableThrowing</code>","text":"<p>While executing the code in <code>ClassLoader::CreateTypeHandleForTypeDefThrowing</code> before the call to <code>MethodTableBuilder::BuildMethodTableThrowing</code> no logic can rely on the <code>MethodTable</code> of the type that is being loaded. This is due to the detail that these are the routines which construct the <code>MethodTable</code>.</p> <p>This has various implications, but the most obvious is that the base type of the type being loaded and any associated interfaces or field types cannot be loaded past <code>CLASS_LOAD_APPROXPARENTS</code> without creating a risk of triggering a <code>TypeLoadException</code>. For instance, if we load the Base type to <code>CLASS_LOAD_EXACTPARENTS</code> then we could not load a type <code>A</code> which was derived from type <code>B&lt;A&gt;</code>. Exceptions to this rule exist, and are necessary to actually implement the type loading process, but generally should be avoided, as they cause behavior which does not match the ECMA specification.</p>"},{"location":"type-loader/#2112-code-within-classloaderdoincrementalload","title":"2.1.1.2 Code within <code>ClassLoader::DoIncrementalLoad</code>","text":"<p>Code that runs during <code>DoIncrementalLoad</code> is generally allowed to require a type load to either the level that is being incrementally loaded to, OR the level at which the type being loaded is already at. The distinction here is that if the relationship between the types is circular, or non-circular. Circular relationships such as the relationship of a type to its type parameters can only be loaded to a level below the desired load level. Non-circular relationships can be required to be loaded to the load level that the incremental operation will eventually reach.</p> <p>For instance, the relationship of a type to its base type is non-circular, as a type cannot transitively be its own exact base type. However, the relationship of a type to the instantiation arguments of its base type can be circular.</p> <p>As an example for the rules above consider the type <code>class A : B&lt;A&gt; {}</code>. When loading class <code>A</code> to <code>CLASS_LOAD_EXACTPARENTS</code> we can require the base type <code>B&lt;A&gt;</code> to be loaded to <code>CLASS_LOAD_EXACTPARENTS</code> as that is a non-circular relationship, but when we load <code>B&lt;A&gt;</code> to <code>CLASS_LOAD_EXACTPARENTS</code> we cannot require type <code>A</code> to be loaded to <code>CLASS_LOAD_EXACTPARENTS</code> as that would cause a circularity issue, and thus loading <code>B&lt;A&gt;</code> to <code>CLASS_LOAD_EXACTPARENTS</code> can only force <code>A</code> to be loaded to <code>CLASS_LOAD_APPROXPARENTS</code>.</p> <p>Code that runs in <code>ClassLoader::DoIncrementalLoad</code> follows a fairly straightforward pattern where code can depend on types being loaded to a specific load level, and when the incremental load process completes at a given level, the type being loaded is incremented in load level.</p>"},{"location":"type-loader/#2113-code-within-pushfinallevels","title":"2.1.1.3 Code within <code>PushFinalLevels</code>","text":"<p>The final two levels of type loading are handled via <code>PushFinalLevels</code> which follows a different set of rules. <code>PushFinalLevels</code> runs code which in order to raise the level can only depend on other types being loaded to a level below the level that is desired. However, before marking the type as reaching a higher level, <code>PushFinalLevels</code> can require other types to also complete the <code>PushFinalLevels</code> algorithm to the new level. Only once all of the types are confirmed to have reached the new level can the entire set of types be marked as reaching the new level.</p>"},{"location":"type-loader/#212-usage-of-load-levels-outside-of-the-type-loader","title":"2.1.2 Usage of load levels outside of the type loader.","text":"<p>In the general case, its preferable to simply ignore load levels when not operating code that is part of the type loader, and to simply ask for fully loaded types. This should be the default, and always functionally correct choice. However, for performance reasons, it is possible to only require partially loaded types, which then requires the user of the type to ensure that their code does not have any dependencies on a fully loaded state.</p>"},{"location":"type-loader/#22-generics","title":"2.2 Generics","text":"<p>In the generics-free world, everything is nice and everyone is happy because every ordinary (not represented by a <code>TypeDesc</code>) type has one <code>MethodTable</code> pointing to its associated <code>EEClass</code> which in turn points back to the <code>MethodTable</code>. All instances of the type contain a pointer to the <code>MethodTable</code> as their first field at offset 0, i.e. at the address seen as the reference value. To conserve space, <code>MethodDesc</code>s representing methods declared by the type are organized in a linked list of chunks pointed to by the <code>EEClass</code><sup>4</sup>.</p> <p></p> <p>Figure 3 Non-generic type with non-generic methods</p> <p><sup>4</sup> Of course, when managed code runs, it does not call methods by looking them up in the chunks. Calling a method is a very \"hot\" operation and normally needs to access only information in the <code>MethodTable</code>.</p>"},{"location":"type-loader/#221-terminology","title":"2.2.1 Terminology","text":"<p>Generic Parameter</p> <p>A placeholder to be substituted by another type; the <code>T</code> in the declaration of <code>List&lt;T&gt;</code>. Sometimes called formal type parameter. A generic parameter has a name and optional generic constraints.</p> <p>Generic Argument</p> <p>A type being substituted for a generic parameter; the <code>int</code> in <code>List&lt;int&gt;</code>. Note that a generic parameter can also be used as an argument. Consider:</p> <pre><code>List&lt;T&gt; GetList&lt;T&gt;()\n{\n    return new List&lt;T&gt;();\n}\n</code></pre> <p>The method has one generic parameter <code>T</code> which is used as a generic argument for the generic list class.</p> <p>Generic Constraint</p> <p>An optional requirement placed by generic parameters on its potential generic arguments. Types that do not have the required properties may not be substituted for the generic parameter and it is enforced by the type loader. There are three kinds of generic constraints:</p> <ol> <li>Special constraints</li> <li> <p>Reference type constraint - the generic argument must be a reference type (as opposed to a value type). The <code>class</code> keyword is used in C# to express this constraint.</p> <pre><code>public class A&lt;T&gt; where T : class\n</code></pre> </li> <li> <p>Value type constraint - the generic argument must be a value type different from <code>System.Nullable&lt;T&gt;</code>. C# uses the <code>struct</code> keyword.</p> <pre><code>public class A&lt;T&gt; where T : struct\n</code></pre> </li> <li> <p>Default constructor constraint - the generic argument must have a public parameterless constructor. This is expressed by <code>new()</code> in C#.</p> <pre><code>public class A&lt;T&gt; where T : new()\n</code></pre> </li> <li> <p>Base type constraints - the generic argument must be derived from (or directly be of) the given non-interface type. It obviously makes sense to use only zero or one reference type as a base types constraint.</p> <pre><code>public class A&lt;T&gt; where T : EventArgs\n</code></pre> </li> <li> <p>Implemented interface constraints - the generic argument must implement (or directly be of) the given interface type.  Zero or more interfaces can be given.</p> <pre><code>public class A&lt;T&gt; where T : ICloneable, IComparable&lt;T&gt;\n</code></pre> </li> </ol> <p>The above constraints are combined with an implicit AND, i.e. a generic parameter can be constrained to be derived from a given type, implement several interfaces, and have the default constructor. All generic parameters of the declaring type can be used to express the constraints, introducing interdependencies among the parameters. For example:</p> <pre><code>public class A&lt;S, T, U&gt;\n    where S : T\n    where T : IList&lt;U&gt; {\n    void f&lt;V&gt;(V v) where V : S {}\n}\n</code></pre> <p>Instantiation</p> <p>The list of generic arguments that were substituted for generic parameters of a generic type or method. Each loaded generic type and method has its instantiation.</p> <p>Typical Instantiation</p> <p>An instantiation consisting purely of the type's or method's own type parameters and in the same order in which the parameters are declared. There exists exactly one typical instantiation for each generic type and method. Usually when one talks about an open generic type, they have the typical instantiation in mind. Example:</p> <pre><code>public class A&lt;S, T, U&gt; {}\n</code></pre> <p>The C# <code>typeof(A&lt;,,&gt;)</code> compiles to <code>ldtoken A`3</code> which makes the runtime load <code>A`3</code> instantiated at <code>S</code> , <code>T</code> , <code>U</code>.</p> <p>Canonical Instantiation</p> <p>An instantiation where all generic arguments are <code>System.__Canon</code>. <code>System.__Canon</code> is an internal type defined in corlib and its task is just to be well-known and different from any other type which may be used as a generic argument. Types/methods with canonical instantiation are used as representatives of all instantiations and carry information shared by all instantiations. Since <code>System.__Canon</code> can obviously not satisfy any constraints that the respective generic parameter may have on it, constraint checking is special-cased with respect to <code>System.__Canon</code> and ignores these violations.</p>"},{"location":"type-loader/#222-sharing","title":"2.2.2 Sharing","text":"<p>With the advent of generics, the number of types loaded by the runtime tends to be higher. Although generic types with different instantiations (for example <code>List&lt;string&gt;</code> and <code>List&lt;object&gt;</code>) are different types each with its own <code>MethodTable</code>, it turns out that there is a considerable amount of information that they can share. This sharing has a positive impact on the memory footprint and consequently also performance.</p> <p></p> <p>Figure 4 Generic type with non-generic methods - shared EEClass</p> <p>Currently all instantiations containing reference types share the same <code>EEClass</code> and its <code>MethodDesc</code>s. This is feasible because all references are of the same size - 4 or 8 bytes - and hence the layout of all these types is the same. The figure illustrates this for <code>List&lt;object&gt;</code> and <code>List&lt;string&gt;</code>. The canonical <code>MethodTable</code> was created automatically before the first reference type instantiation was loaded and contains data which is hot but not instantiation specific like non-virtual slots. Instantiations containing only value types are not shared and every such instantiated type gets its own unshared <code>EEClass</code>.</p> <p><code>MethodTable</code>s representing generic types loaded so far are cached in a hash table owned by their loader module<sup>5</sup>. This hash table is consulted before a new instantiation is constructed, making sure that there will never be two or more <code>MethodTable</code> instances representing the same type.</p> <p>See Design and Implementation of Generics for the .NET Common Language Runtime for more information about generic sharing.</p> <p><sup>5</sup> Things get a bit more complicated for types loaded from NGEN images.</p>"},{"location":"type-system/","title":"Type System Overview","text":"<p>Author: David Wrighton (@davidwrighton) - 2010</p>"},{"location":"type-system/#introduction","title":"Introduction","text":"<p>The CLR type system is our representation the type system described in the ECMA specification + extensions.</p>"},{"location":"type-system/#overview","title":"Overview","text":"<p>The type system is composed of a series of data structures, some of which are described in other Book of the Runtime chapters, as well as a set of algorithms which operate on and create those data structures. It is NOT the type system exposed through reflection, although that one does depend on this system.</p> <p>The major data structures maintained by the type system are:</p> <ul> <li>MethodTable</li> <li>EEClass</li> <li>MethodDesc</li> <li>FieldDesc</li> <li>TypeDesc</li> <li>ClassLoader</li> </ul> <p>The major algorithms contained within the type system are:</p> <ul> <li>Type Loader: Used to load types and create most of the primary data structures of the type system.</li> <li>CanCastTo and similar: The functionality of comparing types.</li> <li>LoadTypeHandle: Primarily used for finding types.</li> <li>Signature parsing: Used to compare and gather information about methods and fields.</li> <li>GetMethod/FieldDesc: Used to find/load methods/fields.</li> <li>Virtual Stub Dispatch: Used to find the destination of virtual calls to interfaces.</li> </ul> <p>There are significantly more ancillary data structures and algorithms that provide various bits of information to the rest of the CLR, but they are less significant to the overall understanding of the system.</p>"},{"location":"type-system/#component-architecture","title":"Component Architecture","text":"<p>The type system's data structures are generally used by all of the various algorithms. This document does not describe the type system algorithms (as there are or should be other book of the runtime documents for those), but it does attempt to describe the various major data structures below.</p>"},{"location":"type-system/#dependencies","title":"Dependencies","text":"<p>The type system is generally a service provided to many parts of the CLR, and most core components have some form of dependency on the behavior of the type system. This diagram describes the general dataflow that effects the type system. It is not exhaustive, but calls out the major information flows.</p> <p></p>"},{"location":"type-system/#component-dependencies","title":"Component Dependencies","text":"<p>The primary dependencies of the type system follow:</p> <ul> <li>The loader needed to get the correct metadata to work with.</li> <li>The metadata system provides a metadata API to gather information.</li> <li>The security system informs the type system whether or not certain type system structures are permitted (e.g. inheritance).</li> <li>The AppDomain provides a LoaderAllocator to handle allocation behavior for the type system data structures.</li> </ul>"},{"location":"type-system/#components-dependent-on-this-component","title":"Components Dependent on this Component","text":"<p>The type system has 3 primary components which depend on it.</p> <ul> <li>The Jit interface, and the jit helpers primarily depends on the type, method, and field searching functionality. Once the type system object is found, the data structures returned have been tailored to provide the information needed by the jit.</li> <li>Reflection uses the type system to provide relatively simple access to ECMA standardized concepts which we happen to capture in the CLR type system data structures.</li> <li>General managed code execution requires the use of the type system for type comparison logic, and virtual stub dispatch.</li> </ul>"},{"location":"type-system/#design-of-type-system","title":"Design of Type System","text":"<p>The core type system data structures are the data structures that represent the actual loaded types (e.g. TypeHandle, MethodTable, MethodDesc, TypeDesc, EEClass) and the data structure that allow types to be found once they are loaded (e.g. ClassLoader, Assembly, Module, RIDMaps).</p> <p>The data structures and algorithms for loading types are discussed in the Type Loader and MethodDesc Book of the Runtime chapters.</p> <p>Tying those data structures together is a set of functionality that allows the JIT/Reflection/TypeLoader/stackwalker to find existing types and methods. The general idea is that these searches should be easily driven by the metadata tokens/signatures that are specified in the ECMA CLI specification.</p> <p>And finally, when the appropriate type system data structure is found, we have algorithms to gather information from a type, and/or compare two types. A particularly complicated example of this form of algorithm may be found in the Virtual Stub Dispatch Book of the Runtime chapter.</p>"},{"location":"type-system/#design-goals-and-non-goals","title":"Design Goals and Non-goals","text":""},{"location":"type-system/#goals","title":"Goals","text":"<ul> <li>Accessing information needed at runtime from executing (non-reflection) code is very fast.</li> <li>Accessing information needed at compilation time for generating code is straightforward.</li> <li>The garbage collector/stackwalker is able to access necessary information without taking locks, or allocating memory.</li> <li>Minimal amounts of types are loaded at a time.</li> <li>Minimal amounts of a given type are loaded at type load time.</li> <li>Type system data structures must be storable in NGEN images.</li> </ul>"},{"location":"type-system/#non-goals","title":"Non-Goals","text":"<ul> <li>All information in the metadata is directly reflected in the CLR data structures.</li> <li>All uses of reflection are fast.</li> </ul>"},{"location":"type-system/#design-of-a-typical-algorithm-used-at-runtime-during-execution-of-managed-code","title":"Design of a typical algorithm used at runtime during execution of managed code","text":"<p>The casting algorithm is typical of algorithms in the type system that are heavily used during the execution of managed code.</p> <p>There are at least 4 separate entry points into this algorithm. Each entry point is chosen to provide a different fast path, in the hopes that the best performance possible will be achieved.</p> <ul> <li>Can an object be cast to a particular non-type equivalent non-array type?</li> <li>Can an object be cast to an interface type that does not implement generic variance?</li> <li>Can an object be cast to an array type?</li> <li>Can an object of a type be cast to an arbitrary other managed type?</li> </ul> <p>Each of these implementations with the exception of the last one is optimized to perform better at the expense of not being fully general.</p> <p>For instance, the \"Can a type be cast to a parent type\" which is a variant of \"Can an object be cast to a particular non-type equivalent non-array type?\" code is implemented with a single loop that walks a singly linked list. This is only able to search a subset of possible casting operations, but it is possible to determine if that is the appropriate set by examining the type the cast is trying to enforce. This algorithm is implemented in the jit helper JIT_ChkCastClass_Portable.</p> <p>Assumptions:</p> <ul> <li>Special purpose implementations of algorithms are a performance improvement in general.</li> <li>Extra versions of algorithms do not provide an insurmountable maintenance problem.</li> </ul>"},{"location":"type-system/#design-of-typical-search-algorithm-in-the-type-system","title":"Design of typical search algorithm in the Type System","text":"<p>There are a number of algorithms in the type system which follow this common pattern.</p> <p>The type system is commonly used to find a type. This may be triggered via any number of inputs such as the JIT, reflection, serialization, remoting, etc.</p> <p>The basic input to the type system in these cases is</p> <ul> <li>The context from which the search shall begin (a Module or assembly pointer).</li> <li>An identifier that describes the sought after type in the initial context. This is typically a token, or a string (if an assembly is the search context).</li> </ul> <p>The algorithm must first decode the identifier.</p> <p>For the search for a type scenario, the token may be either a TypeDef token, a TypeRef token, a TypeSpec token, or a string. Each of these different identifiers will cause a different form of lookup.</p> <ul> <li>A typedef token will cause a lookup in the RidMap of the Module. This is a simple array index.</li> <li>A typeref token will cause a lookup to find the assembly which this typeref token refers to, and then the type finding algorithm is begun anew with the found assembly pointer, and a string gathered from the typeref table.</li> <li>A typespec token indicates that a signature must be parsed to find the signature. Parse the signature to find the information necessary to load the type. This will recursively trigger more type finding.</li> <li>A name is used to bind between assemblies. The TypeDef/ExportedTypes table is searched for matches. Note: This search is optimized by hashtables on the manifest module object.</li> </ul> <p>From this design a number of common characteristics of search algorithms in the type system are evident.</p> <ul> <li>Searches use input that is tightly coupled to metadata. In particular, metadata tokens and string names are commonly passed around. Also, these searches are tied to Modules, which directly map to .dll and .exe files.</li> <li>Use of cached information to improve performance. The RidMap and hash tables are data structures optimized to improve these lookups.</li> <li>The algorithms typically have 3-4 different paths based on their input.</li> </ul> <p>In addition to this general design, there are a number of extra requirements that are layered onto this.</p> <ul> <li>ASSUMPTION: Searching for types that are already loaded is safe to perform while stopped in the GC.</li> <li>INVARIANT: A type which has already been loaded will always be found if searched for.</li> <li>ISSUE: Search routines rely on metadata reading. This can yield inadequate performance in some scenarios.</li> </ul> <p>This search algorithm is typical of the routines used during JITing. It has a number of common characteristics.</p> <ul> <li>It uses metadata.</li> <li>It requires looking for data in many places.</li> <li>There is relatively little duplication of data in our data structures.</li> <li>It typically does not recurse deeply, and does not have loops.</li> </ul> <p>This allows us to meet the performance requirements, and characteristics necessary for working with an IL based JIT.</p>"},{"location":"type-system/#garbage-collector-requirements-on-the-type-system","title":"Garbage Collector Requirements on the Type System","text":"<p>The garbage collector requires information about instances of types allocated in the GC heap. This is done via a pointer to a type system data structure (MethodTable) at the head of every managed object. Attached to the MethodTable, is a data structure that describes the GC layout of instances of types. There are two forms of this layout (one for normal types, and object arrays, and another for arrays of valuetypes).</p> <ul> <li>ASSUMPTION: Type system data structures have a lifetime that exceeds that of managed objects that are of types described in the type system data structure.</li> <li>REQUIREMENT: The garbage collector has a requirement to execute the stack walker while the runtime is suspended. This will be discussed next.</li> </ul>"},{"location":"type-system/#stackwalker-requirements-on-the-type-system","title":"Stackwalker requirements on the Type System","text":"<p>The stack walker/ GC stack walker requires type system input in 2 cases.</p> <ul> <li>For finding the size of valuetypes on the stack.</li> <li>For finding GC roots to report within valuetypes on the stack.</li> </ul> <p>For various reasons involving the desire to delay load types, and the avoidance of generating multiple versions of code (that only differ via associated gc info) the CLR currently requires the walking of signatures of methods that are on the stack. This need is rarely exercised, as it requires the stack walker to execute at very particular moments in time, but in order to meet our reliability goals, the signature walker must be able to function while stackwalking.</p> <p>The stack walker executes in approximately 3 modes.</p> <ul> <li>To walk the stack of the current thread for security or exception processing reasons.</li> <li>To walk the stack of all threads for GC purposes (all threads are suspended by the EE).</li> <li>To walk the stack of a particular thread for a profiler (that specific thread is suspended).</li> </ul> <p>In the GC stack walking case, and in the profiler stack walking case, due to thread suspension, it is not safe to allocate memory or take most locks.</p> <p>This has led us to develop a path through the type system which may be relied upon to follow the above requirement.</p> <p>The rule required for the type system to achieve this goal is:</p> <ul> <li>If a method has been called, then all valuetype parameters of the called method will have been loaded into some appdomain in the process.</li> <li>The assembly reference from the assembly with the signature to the assembly implementing the type must be resolved before a walk of the signature is necessary as part of a stack walk.</li> </ul> <p>This is enforced via an extensive and complicated set of enforcements within the type loader, NGEN image generation process, and JIT.</p> <ul> <li>ISSUE: Stackwalker requirements on the type system are HIGHLY fragile.</li> <li>ISSUE: Implementation of stack walker requirements in the type system requires a set of contract violations at every function in the type system that may be touched while searching for types which are loaded.</li> <li>ISSUE: The signature walks performed are done with the normal signature walking code. This code is designed to load types as it walks the signature, but in this case the type load functionality is used with the assumption that no type load will actually be triggered.</li> <li>ISSUE: Stackwalker requirements require support from not just the type system, but also the assembly loader. The Loader has had a number of issues meeting the needs of the type system here.</li> </ul>"},{"location":"type-system/#static-variables","title":"Static variables","text":"<p>Static variables in CoreCLR are handled by a combination of getting the \"static base\", and then adjusting it by an offset to get a pointer to the actual value. We define the statics base as either non-gc or gc for each field. Currently non-gc statics are any statics which are represented by primitive types (byte, sbyte, char, int, uint, long, ulong, float, double, pointers of various forms), and enums. GC statics are any statics which are represented by classes or by non-primitive valuetypes. For valuetype statics which are GC statics, the static variable is actually a pointer to a boxed instance of the valuetype.</p>"},{"location":"type-system/#per-type-static-variable-information","title":"Per type static variable information","text":"<p>As of .NET 9, the static variable bases are now all associated with their particular type. As you can see from this diagram, the data for statics can be acquired by starting at a <code>MethodTable</code> and then getting either the <code>DynamicStaticsInfo</code> to get a statics pointer, or by getting a <code>ThreadStaticsInfo</code> to get a TLSIndex, which then can be used with the thread static variable system to get the actual thread static base.</p> <pre><code>classDiagram\nMethodTable : MethodTableAuxiliaryData* m_pAuxData\nMethodTable --&gt; MethodTableAuxiliaryData\nMethodTableAuxiliaryData --&gt; DynamicStaticsInfo : If has static variables\nMethodTableAuxiliaryData --&gt; GenericStaticsInfo : If is generic and has static variables\nMethodTableAuxiliaryData --&gt; ThreadStaticsInfo : If has thread local static variables\n\nDynamicStaticsInfo : StaticsPointer m_pGCStatics\nDynamicStaticsInfo : StaticsPointer m_pNonGCStatics\n\nGenericStaticsInfo : FieldDesc* m_pFieldDescs\n\nThreadStaticsInfo : TLSIndex NonGCTlsIndex\nThreadStaticsInfo : TLSIndex GCTlsIndex\n</code></pre> <pre><code>classDiagram\n\nnote for StaticsPointer \"StaticsPointer is a pointer sized integer\"\nStaticsPointer : void* PointerToStaticBase\nStaticsPointer : bool HasClassConstructorBeenRun\n\nnote for TLSIndex \"TLSIndex is a 32bit integer\"\nTLSIndex : TLSIndexType indexType\nTLSIndex : 24bit int indexOffset\n</code></pre> <p>In the above diagram, you can see that we have separate fields for non-gc and gc statics, as well as thread and normal statics. For normal statics, we use a single pointer sized field, which also happens to encode whether or not the class constructor has been run. This is done to allow lock free atomic access to both get the static field address as well as determine if the class constructor needs to be triggered. For TLS statics, handling of detecting whether or not the class constructor has been run is a more complex process described as part of the thread statics infrastructure. The <code>DynamicStaticsInfo</code> and <code>ThreadStaticsInfo</code> structures are accessed without any locks, so it is important to ensure that access to fields on these structures can be done with a single memory access, to avoid memory order tearing issues.</p> <p>Also, notably, for generic types, each field has a <code>FieldDesc</code> which is allocated per type instance, and is not shared by multiple canonical instances.</p>"},{"location":"type-system/#lifetime-management-for-collectible-statics","title":"Lifetime management for collectible statics","text":"<p>Finally we have a concept of collectible assemblies in the CoreCLR runtime, so we need to handle lifetime management for static variables. The approach chosen was to build a special GC handle type which will allow the runtime to have a pointer in the runtime data structures to the interior of a managed object on the GC heap.</p> <p>The requirement of behavior here is that a static variable cannot keep its own collectible assembly alive, and so collectible statics have the peculiar property that they can exist and be finalized before the collectible assembly is finally collected. If there is some resurrection scenario, this can lead to very surprising behavior.</p>"},{"location":"type-system/#thread-statics","title":"Thread Statics","text":"<p>Thread statics are static variables which have a lifetime which is defined to be the shorter of the lifetime of the type containing the static, and the lifetime of the thread on which the static variable is accessed. They are created by having a static variable on a type which is attributed with <code>[System.Runtime.CompilerServices.ThreadStaticAttribute]</code>. The general scheme of how this works is to assign an \"index\" to the type which is the same on all threads, and then on each thread hold a data structure which is efficiently accessed by means of this index. However, we have a few peculiarities in our approach.</p> <ol> <li>We segregate collectible and non-collectible thread statics (<code>TLSIndexType::NonCollectible</code> and <code>TLSIndexType::Collectible</code>)</li> <li>We provide an ability to share a non-gc thread static between native CoreCLR code and managed code (Subset of <code>TLSIndexType::DirectOnThreadLocalData</code>)</li> <li>We provide an extremely efficient means to access a small number of non-gc thread statics. (The rest of the usage of <code>TLSIndexType::DirectOnThreadLocalData</code>)</li> </ol>"},{"location":"type-system/#per-thread-statics-data-structures","title":"Per-Thread Statics Data structures","text":"<pre><code>classDiagram\n\nnote for ThreadLocalInfo \"There is 1 of these per thread, and it is managed by the C++ compiler/OS using standard mechanisms.\nIt can be found as the t_ThreadStatics variable in a C++ compiler, and is also pointed at by the native Thread class.\"\nThreadLocalInfo : int cNonCollectibleTlsData\nThreadLocalInfo : void** pNonCollectibleTlsArrayData\nThreadLocalInfo : int cCollectibleTlsData\nThreadLocalInfo : void** pCollectibleTlsArrayData\nThreadLocalInfo : InFlightTLSData *pInFightData\nThreadLocalInfo : Thread* pThread\nThreadLocalInfo : Special Thread Statics Shared Between Native and Managed code\nThreadLocalInfo : byte[N] ExtendedDirectThreadLocalTLSData\n\nInFlightTLSData : InFlightTLSData* pNext\nInFlightTLSData : TLSIndex tlsIndex\nInFlightTLSData : OBJECTHANDLE hTLSData\n\nThreadLocalInfo --&gt; InFlightTLSData : For TLS statics which have their memory allocated, but have not been accessed since the class finished running its class constructor\nInFlightTLSData --&gt; InFlightTLSData : linked list\n</code></pre>"},{"location":"type-system/#access-patterns-for-getting-the-thread-statics-address","title":"Access patterns for getting the thread statics address","text":"<p>This is the pattern that the JIT will use to access a thread static which is not <code>DirectOnThreadLocalData</code>.</p> <ol> <li>Get the TLS index somehow</li> <li>Get TLS pointer to OS managed TLS block for the current thread ie. <code>pThreadLocalData = &amp;t_ThreadStatics</code></li> <li>Read 1 integer value <code>pThreadLocalData-&gt;cCollectibleTlsData OR pThreadLocalData-&gt;cNonCollectibleTlsData</code></li> <li>Compare cTlsData against the index we're looking up <code>if (cTlsData &lt; index.GetIndexOffset())</code></li> <li>If the index is not within range, jump to step 11.</li> <li>Read 1 pointer value from TLS block <code>pThreadLocalData-&gt;pCollectibleTlsArrayData</code> OR <code>pThreadLocalData-&gt;pNonCollectibleTlsArrayData</code></li> <li>Read 1 pointer from within the TLS Array. <code>pTLSBaseAddress = *(intptr_t*)(((uint8_t*)pTlsArrayData) + index.GetIndexOffset()</code></li> <li>If pointer is NULL jump to step 11 <code>if pTLSBaseAddress == NULL</code></li> <li>If TLS index not a Collectible index, return pTLSBaseAddress</li> <li>if <code>ObjectFromHandle((OBJECTHANDLE)pTLSBaseAddress)</code> is NULL, jump to step 11</li> <li>Return <code>ObjectFromHandle((OBJECTHANDLE)pTLSBaseAddress)</code></li> <li>Tail-call a helper <code>return GetThreadLocalStaticBase(index)</code></li> </ol> <p>This is the pattern that the JIT will use to access a thread static which is on <code>DirectOnThreadLocalData</code> 0. Get the TLS index somehow 1. Get TLS pointer to OS managed TLS block for the current thread ie. <code>pThreadLocalData = &amp;t_ThreadStatics</code> 2. Add the index offset to the start of the ThreadLocalData structure <code>pTLSBaseAddress = ((uint8_t*)pThreadLocalData) + index.GetIndexOffset()</code></p>"},{"location":"type-system/#lifetime-management-for-thread-static-variables","title":"Lifetime management for thread static variables","text":"<p>We distinguish between collectible and non-collectible thread static variables for efficiency purposes.</p> <p>A non-collectible thread static is a thread static defined on a type which cannot be collected by the runtime. This describes most thread statics in actual observed practice. The <code>DirectOnThreadLocalData</code> statics are a subset of this category which has a speical optimized form and does not need any GC reporting. For non-collectible thread statics, the pointer (<code>pNonCollectibleTlsArrayData</code>) in the <code>ThreadLocalData</code> is a pointer to a managed <code>object[]</code> which points at either <code>object[]</code>, <code>byte[]</code>, or <code>double[]</code> arrays. At GC scan time, the pointer to the initial object[] is the only detail which needs to be reported to the GC.</p> <p>A collectible thread static is a thread static which can be collected by the runtime. This describes the static variables defined on types which can be collected by the runtime. The pointer (<code>pCollectibleTlsArrayData</code>) in the <code>ThreadLocalData</code> is a pointer to a chunk of memory allocated via <code>malloc</code>, and holds pointers to <code>object[]</code>, <code>byte[]</code>, or <code>double[]</code> arrays. At GC scan time, each managed object must individually be kept alive only if the type and thread is still alive. This requires properly handling several situations. 1. If a collectible assembly becomes unreferenced, but a thread static variable associated with it has a finalizer, the object must move to the finalization queue. 2. If a thread static variable associated with a collectible assembly refers to the collectible assembly <code>LoaderAllocator</code> via a series of object references, it must not provide a reason for the collectible assembly to be considered referenced. 3. If a collectible assembly is collected, then the associated static variables no longer exist, and the TLSIndex values associated with that collectible assembly becomes re-useable. 4. If a thread is no longer executing, then all thread statics associated with that thread are no longer kept alive.</p> <p>The approach chosen is to use a pair of different handle types. For efficient access, the handle type stored in the dynamically adjusted array is a WeakTrackResurrection GCHandle. This handle instance is associated with the slot in the TLS data, not with the exact instantiation, so it can be re-used when the if the associated collectible assembly is collected, and then the slot is re-used. In addition, each slot that is in use will have a <code>LOADERHANDLE</code> which will keep the object alive until the <code>LoaderAllocator</code> is freed. This <code>LOADERHANDLE</code> will be abandoned if the <code>LoaderAllocator</code> is collected, but that's ok, as <code>LOADERHANDLE</code> only needs to be cleaned up if the <code>LoaderAllocator</code> isn't collected. On thread destroy, for each collectible slot in the tls array, we will explicitly free the <code>LOADERHANDLE</code> on the correct <code>LoaderAllocator</code>.</p>"},{"location":"type-system/#physical-architecture","title":"Physical Architecture","text":"<p>Major parts of the type system are found in:</p> <ul> <li>Class.cpp/inl/h \u2013 EEClass functions, and BuildMethodTable</li> <li>MethodTable.cpp/inl/h \u2013 Functions for manipulating methodtables.</li> <li>TypeDesc.cpp/inl/h \u2013 Functions for examining TypeDesc</li> <li>MetaSig.cpp SigParser \u2013 Signature code</li> <li>FieldDesc /MethodDesc \u2013 Functions for examining these data structures</li> <li>Generics \u2013 Generics specific logic.</li> <li>Array \u2013 Code for handling the special cases required for array processing</li> <li>VirtualStubDispatch.cpp/h/inl \u2013 Code for virtual stub dispatch</li> <li>VirtualCallStubCpu.hpp \u2013 Processor specific code for virtual stub dispatch.</li> <li>threadstatics.cpp/h - Handling for thread static variables.</li> </ul> <p>Major entry points are BuildMethodTable, LoadTypeHandleThrowing, CanCastTo*, GetMethodDescFromMemberDefOrRefOrSpecThrowing, GetFieldDescFromMemberRefThrowing, CompareSigs, and VirtualCallStubManager::ResolveWorkerStatic.</p>"},{"location":"type-system/#related-reading","title":"Related Reading","text":"<ul> <li>ECMA CLI Specification</li> <li>Type Loader Book of the Runtime Chapter</li> <li>Virtual Stub Dispatch Book of the Runtime Chapter</li> <li>MethodDesc Book of the Runtime Chapter</li> </ul>"},{"location":"vectors-and-intrinsics/","title":"Vectors and Hardware Intrinsics Support","text":""},{"location":"vectors-and-intrinsics/#introduction","title":"Introduction","text":"<p>The CoreCLR runtime has support for several varieties of hardware intrinsics, and various ways to compile code which uses them. This support varies by target processor, and the code produced depends on how the jit compiler is invoked. This document describes the various behaviors of intrinsics in the runtime, and concludes with implications  for developers working on the runtime and libraries portions of the runtime.</p>"},{"location":"vectors-and-intrinsics/#acronyms-and-definitions","title":"Acronyms and definitions","text":"Acronym Definition AOT Ahead of time. In this document, it refers to compiling code before the process launches and saving it into a file for later use."},{"location":"vectors-and-intrinsics/#intrinsics-apis","title":"Intrinsics apis","text":"<p>Most hardware intrinsics support is tied to the use of various Vector apis. There are 4 major api surfaces that are supported by the runtime</p> <ul> <li>The fixed length float vectors. <code>Vector2</code>, <code>Vector3</code>, and <code>Vector4</code>. These vector types represent a struct of floats of various lengths. For type layout, ABI and, interop purposes they are represented in exactly the same way as a structure with an appropriate number of floats in it. Operations on these vector types are supported on all architectures and platforms, although some architectures may optimize various operations.</li> <li>The variable length <code>Vector&lt;T&gt;</code>. This represents vector data of runtime-determined length. In any given process the length of a <code>Vector&lt;T&gt;</code> is the same in all methods, but this length may differ between various machines or environment variable settings read at startup of the process. The <code>T</code> type variable may be the following types (<code>System.Byte</code>, <code>System.SByte</code>, <code>System.Int16</code>, <code>System.UInt16</code>, <code>System.Int32</code>, <code>System.UInt32</code>, <code>System.Int64</code>, <code>System.UInt64</code>, <code>System.Single</code>, and <code>System.Double</code>), and allows use of integer or double data within a vector. The length and alignment of <code>Vector&lt;T&gt;</code> is unknown to the developer at compile time (although discoverable at runtime by using the <code>Vector&lt;T&gt;.Count</code> api), and <code>Vector&lt;T&gt;</code> may not exist in any interop signature. Operations on these vector types are supported on all architectures and platforms, although some architectures may optimize various operations if the <code>Vector&lt;T&gt;.IsHardwareAccelerated</code> api returns true.</li> <li><code>Vector64&lt;T&gt;</code>, <code>Vector128&lt;T&gt;</code>, <code>Vector256&lt;T&gt;</code>, and <code>Vector512&lt;T&gt;</code> represent fixed-sized vectors that closely resemble the fixed- sized vectors available in C++. These structures can be used in any code that runs, but very few features are supported directly on these types other than creation. They are used primarily in the processor specific hardware intrinsics apis.</li> <li>Processor specific hardware intrinsics apis such as <code>System.Runtime.Intrinsics.X86.Ssse3</code>. These apis map directly to individual instructions or short instruction sequences that are specific to a particular hardware instruction. These apis are only usable on hardware that supports the particular instruction. See https://github.com/dotnet/designs/blob/master/accepted/2018/platform-intrinsics.md for the design of these.</li> </ul>"},{"location":"vectors-and-intrinsics/#how-to-use-intrinsics-apis","title":"How to use intrinsics apis","text":"<p>There are 3 models for use of intrinsics apis.</p> <ol> <li>Usage of <code>Vector2</code>, <code>Vector3</code>, <code>Vector4</code>, and <code>Vector&lt;T&gt;</code>. For these, its always safe to just use the types. The jit will generate code that is as optimal as it can for the logic, and will do so unconditionally.</li> <li>Usage of <code>Vector64&lt;T&gt;</code>, <code>Vector128&lt;T&gt;</code>, <code>Vector256&lt;T&gt;</code>, and <code>Vector512&lt;T&gt;</code>. These types may be used unconditionally, but are only truly useful when also using the platform specific hardware intrinsics apis.</li> <li>Usage of platform intrinsics apis. All usage of these apis should be wrapped in an <code>IsSupported</code> check of the appropriate kind. Then, within the <code>IsSupported</code> check the platform specific api may be used. If multiple instruction sets are used, then the application developer must have checks for the instruction sets as used on each one of them.</li> </ol>"},{"location":"vectors-and-intrinsics/#effect-of-usage-of-hardware-intrinsics-on-how-code-is-generated","title":"Effect of usage of hardware intrinsics on how code is generated","text":"<p>Hardware intrinsics have dramatic impacts on codegen, and the codegen of these hardware intrinsics is dependent on the ISA available for the target machine when the code is compiled.</p> <p>If the code is compiled at runtime by the JIT in a just-in-time manner, then the JIT will generate the best code it can based on the current processor's ISA. This use of hardware intrinsics is indendent of jit compilation tier. <code>MethodImplOptions.AggressiveOptimization</code> may be used to bypass compilation of tier 0 code and always produce tier 1 code for the method. In addition, the current policy of the runtime is that <code>MethodImplOptions.AggressiveOptimization</code> may also be used to bypass compilation of code as R2R code, although that may change in the future.</p> <p>For AOT compilation, the situation is far more complex. This is due to the following principles of how our AOT compilation model works.</p> <ol> <li>AOT compilation must never under any circumstance change the semantic behavior of code except for changes in performance.</li> <li>If AOT code is generated, it should be used unless there is an overriding reason to avoid using it.</li> <li>It must be exceedingly difficult to misuse the AOT compilation tool to violate principle 1.</li> </ol>"},{"location":"vectors-and-intrinsics/#crossgen2-model-of-hardware-intrinsic-usage","title":"Crossgen2 model of hardware intrinsic usage","text":"<p>There are 2 sets of instruction sets known to the compiler. - The baseline instruction set which defaults to x86-64-v2 (SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, and POPCNT), but may be adjusted via compiler option. - The optimistic instruction set which defaults to (AES, GFNI, SHA, WAITPKG, and X86SERIALIZE).</p> <p>Code will be compiled using the optimistic instruction set to drive compilation, but any use of an instruction set beyond the baseline instruction set will be recorded, as will any attempt to use an instruction set beyond the optimistic set if that attempted use has a semantic effect. If the baseline instruction set includes <code>Avx2</code> then the size and characteristics of of <code>Vector&lt;T&gt;</code> is known. Any other decisions about ABI may also be encoded. For instance, it is likely that the ABI of <code>Vector256&lt;T&gt;</code> and <code>Vector512&lt;T&gt;</code> will vary based on the presence/absence of <code>Avx</code> support.</p> <ul> <li>Any code which uses <code>Vector&lt;T&gt;</code> will not be compiled AOT unless the size of <code>Vector&lt;T&gt;</code> is known.</li> <li>Any code which passes a <code>Vector256&lt;T&gt;</code> or <code>Vector512&lt;T&gt;</code> as a parameter on a Linux or Mac machine will not be compiled AOT unless the support for the <code>Avx</code> instruction set is known.</li> <li>Non-platform intrinsics which require more hardware support than the optimistic supported hardware capability will not take advantage of that capability. MethodImplOptions.AggressiveOptimization may be used to disable compilation of this sub-par code.</li> <li>Code which takes advantage of instructions sets in the optimistic set will not be used on a machine which only supports the baseline instruction set.</li> <li>Code which attempts to use instruction sets outside of the optimistic set will generate code that will not be used on machines with support for the instruction set.</li> </ul>"},{"location":"vectors-and-intrinsics/#characteristics-which-result-from-rules","title":"Characteristics which result from rules","text":"<ul> <li>Code which uses platform intrinsics within the optimistic instruction set will generate good code.</li> <li>Code which relies on platform intrinsics not within the baseline or optimistic set will cause runtime jit and startup time concerns if used on hardware which does support the instruction set.</li> <li><code>Vector&lt;T&gt;</code> code has runtime jit and startup time concerns unless the baseline is raised to include <code>Avx2</code>.</li> </ul>"},{"location":"vectors-and-intrinsics/#code-review-rules-for-use-of-platform-intrinsics","title":"Code review rules for use of platform intrinsics","text":"<ul> <li>Any use of a platform intrinsic in the codebase SHOULD be wrapped with a call to the associated IsSupported property. This wrapping may be done within the same function that uses the hardware intrinsic, but this is not required as long as the programmer can control all entrypoints to a function that uses the hardware intrinsic.</li> <li>If an application developer is highly concerned about startup performance, developers should avoid use intrinsics beyond Sse42, or should use Crossgen with an updated baseline instruction set support.</li> </ul>"},{"location":"vectors-and-intrinsics/#crossgen2-adjustment-to-rules-for-systemprivatecorelibdll","title":"Crossgen2 adjustment to rules for System.Private.CoreLib.dll","text":"<p>Since System.Private.CoreLib.dll is known to be code reviewed with the code review rules as written below with System.Private.CoreLib.dll, it is possible to relax rule \"Code which attempts to use instruction sets outside of the optimistic set will generate code that will not be used on machines with support for the instruction set.\" What this will do is allow the generation of non-optimal code for these situations, but through the magic of code review and analyzers, the generated logic will still work correctly.</p>"},{"location":"vectors-and-intrinsics/#code-review-and-analyzer-rules-for-code-written-in-systemprivatecorelibdll","title":"Code review and analyzer rules for code written in System.Private.CoreLib.dll","text":"<ul> <li>Any use of a platform intrinsic in the codebase MUST be wrapped with a call to an associated IsSupported property. This wrapping MUST be done within the same function that uses the hardware intrinsic, OR the function which uses the platform intrinsic must have the <code>CompExactlyDependsOn</code> attribute used to indicate that this function will unconditionally call platform intrinsics of from some type.</li> <li>Within a single function that uses platform intrinsics, unless marked with the <code>CompExactlyDependsOn</code> attribute it must behave identically regardless of whether IsSupported returns true or not. This allows the R2R compiler to compile with a lower set of intrinsics support, and yet expect that the behavior of the function will remain unchanged in the presence of tiered compilation.</li> <li>Excessive use of intrinsics may cause startup performance problems due to additional jitting, or may not achieve desired performance characteristics due to suboptimal codegen. To fix this, we may, in the future, change the compilation rules to compile the methods marked with<code>CompExactlyDependsOn</code> with the appropriate platform intrinsics enabled.</li> </ul> <p>Correct use of the <code>IsSupported</code> properties and <code>CompExactlyDependsOn</code> attribute is checked by an analyzer during build of <code>System.Private.CoreLib</code>. This analyzer requires that all usage of <code>IsSupported</code> properties conform to a few specific patterns. These patterns are supported via either if statements or the ternary operator.</p> <p>The supported conditional checks are</p> <ol> <li> <p>Simple if statement checking IsSupported flag surrounding usage <pre><code>if (PlatformIntrinsicType.IsSupported)\n{\n    PlatformIntrinsicType.IntrinsicMethod();\n}\n</code></pre></p> </li> <li> <p>If statement check checking a platform intrinsic type which implies that the intrinsic used is supported.</p> </li> </ol> <pre><code>if (Avx2.X64.IsSupported)\n{\n    Avx2.IntrinsicMethod();\n}\n</code></pre> <ol> <li>Nested if statement where there is an outer condition which is an OR'd together series of IsSupported checks for mutually exclusive conditions and where the inner check is an else clause where some checks are excluded from applying.</li> </ol> <pre><code>if (Avx2.IsSupported || ArmBase.IsSupported)\n{\n    if (Avx2.IsSupported)\n    {\n        // Do something\n    }\n    else\n    {\n        ArmBase.IntrinsicMethod();\n    }\n}\n</code></pre> <ol> <li>Within a method marked with <code>CompExactlyDependsOn</code> for a less advanced attribute, there may be a use of an explicit IsSupported check for a more advanced cpu feature. If so, the behavior of the overall function must remain the same regardless of whether or not the CPU feature is enabled. The analyzer will detect this usage as a warning, so that any use of IsSupported in a helper method is examined to verify that that use follows the rule of preserving exactly equivalent behavior.</li> </ol> <pre><code>[CompExactlyDependsOn(typeof(Sse41))]\nint DoSomethingHelper()\n{\n#pragma warning disable IntrinsicsInSystemPrivateCoreLibAttributeNotSpecificEnough // The else clause is semantically equivalent\n    if (Avx2.IsSupported)\n#pragma warning disable IntrinsicsInSystemPrivateCoreLibAttributeNotSpecificEnough\n    {\n        Avx2.IntrinsicThatDoesTheSameThingAsSse41IntrinsicAndSse41.Intrinsic2();\n    }\n    else\n    {\n        Sse41.Intrinsic();\n        Sse41.Intrinsic2();\n    }\n}\n</code></pre> <ul> <li>NOTE: If the helper needs to be used AND behave differently with different instruction sets enabled, correct logic requires spreading the <code>CompExactlyDependsOn</code> attribute to all callers such that no caller could be compiled expecting the wrong behavior. See the <code>Vector128.ShuffleUnsafe</code> method, and various uses.</li> </ul> <p>The behavior of the <code>CompExactlyDependsOn</code> is that 1 or more attributes may be applied to a given method. If any of the types specified via the attribute will not have an invariant result for its associated <code>IsSupported</code> property at runtime, then the method will not be compiled or inlined into another function during R2R compilation. If no type so described will have a true result for the <code>IsSupported</code> method, then the method will not be compiled or inlined into another function during R2R compilation.</p> <ol> <li>In addition to directly using the IsSupported properties to enable/disable support for intrinsics, simple static properties written in the following style may be used to reduce code duplication.</li> </ol> <pre><code>static bool IsVectorizationSupported =&gt; Avx2.IsSupported || PackedSimd.IsSupported\n\npublic void SomePublicApi()\n{\n    if (IsVectorizationSupported)\n        SomeVectorizationHelper();\n    else\n    {\n        // Non-Vectorized implementation\n    }\n}\n\n[CompExactlyDependsOn(typeof(Avx2))]\n[CompExactlyDependsOn(typeof(PackedSimd))]\nprivate void SomeVectorizationHelper()\n{\n}\n</code></pre>"},{"location":"vectors-and-intrinsics/#non-deterministic-intrinsics-in-systemprivatecorelib","title":"Non-Deterministic Intrinsics in System.Private.Corelib","text":"<p>Some APIs exposed in System.Private.Corelib are intentionally non-deterministic across hardware and instead only ensure determinism within the scope of a single process. To facilitate the support of such APIs, the JIT defines <code>Compiler::BlockNonDeterministicIntrinsics(bool mustExpand)</code> which should be used to help block such APIs from expanding in scenarios such as ReadyToRun. Additionally, such APIs should recursively call themselves so that indirect invocation (such as via a delegate, function pointer, reflection, etc) will compute the same result.</p> <p>An example of such a non-deterministic API is the <code>ConvertToIntegerNative</code> APIs exposed on <code>System.Single</code> and <code>System.Double</code>. These APIs convert from the source value to the target integer type using the fastest mechanism available for the underlying hardware. They exist due to the IEEE 754 specification leaving conversions undefined when the input cannot fit into the output (for example converting <code>float.MaxValue</code> to <code>int</code>) and thus different hardware having historically provided differing behaviors on these edge cases. They allow developers who do not need to be concerned with edge case handling but where the performance overhead of normalizing results for the default cast operator is too great.</p> <p>Another example is the various <code>*Estimate</code> APIs, such as <code>float.ReciprocalSqrtEstimate</code>. These APIs allow a user to likewise opt into a faster result at the cost of some inaccuracy, where the exact inaccuracy encountered depends on the input and the underlying hardware the instruction is executed against.</p>"},{"location":"vectors-and-intrinsics/#mechanisms-in-the-jit-to-generate-correct-code-to-handle-varied-instruction-set-support","title":"Mechanisms in the JIT to generate correct code to handle varied instruction set support","text":"<p>The JIT receives flags which instruct it on what instruction sets are valid to use, and has access to a new jit interface api <code>notifyInstructionSetUsage(isa, bool supportBehaviorRequired)</code>.</p> <p>The notifyInstructionSetUsage api is used to notify the AOT compiler infrastructure that the code may only execute if the runtime environment of the code is exactly the same as the boolean parameter indicates it should be. For instance, if <code>notifyInstructionSetUsage(Avx, false)</code> is used, then the code generated must not be used if the <code>Avx</code> instruction set is usable. Similarly <code>notifyInstructionSetUsage(Avx, true)</code> will indicate that the code may only be used if the <code>Avx</code> instruction set is available.</p> <p>While the above api exists, it is not expected that general purpose code within the JIT will use it. In general jitted code is expected to use a number of different apis to understand the available hardware instruction support available.</p> Api Description of use Exact behavior <code>compExactlyDependsOn(isa)</code> Use when making a decision to use or not use an instruction set when the decision will affect the semantics of the generated code. Should never be used in an assert. Return whether or not an instruction set is supported. Calls notifyInstructionSetUsage with the result of that computation. <code>compOpportunisticallyDependsOn(isa)</code> Use when making an opportunistic decision to use or not use an instruction set. Use when the instruction set usage is a \"nice to have optimization opportunity\", but do not use when a false result may change the semantics of the program. Should never be used in an assert. Return whether or not an instruction set is supported. Calls notifyInstructionSetUsage if the instruction set is supported. <code>compIsaSupportedDebugOnly(isa)</code> Use to assert whether or not an instruction set is supported Return whether or not an instruction set is supported. Does not report anything. Only available in debug builds. <code>getVectorTByteLength()</code> Use to get the size of a <code>Vector&lt;T&gt;</code> value. Determine the size of the <code>Vector&lt;T&gt;</code> type. If on the architecture the size may vary depending on whatever rules. Use <code>compExactlyDependsOn</code> to perform the queries so that the size is consistent between compile time and runtime. <code>getMaxVectorByteLength()</code> Get the maximum number of bytes that might be used in a SIMD type during this compilation. Query the set of instruction sets supported, and determine the largest simd type supported. Use <code>compOpportunisticallyDependsOn</code> to perform the queries so that the maximum size needed is the only one recorded."},{"location":"virtual-stub-dispatch/","title":"Virtual Stub Dispatch","text":"<p>Author: Simon Hall (@snwbrdwndsrf) - 2006</p>"},{"location":"virtual-stub-dispatch/#introduction","title":"Introduction","text":"<p>Virtual stub dispatching (VSD) is the technique of using stubs for virtual method invocations instead of the traditional virtual method table. In the past, interface dispatch required that interfaces had process-unique identifiers, and that every loaded interface was added to a global interface virtual table map. This requirement meant that all interfaces and all classes that implemented interfaces had to be restored at runtime in NGEN scenarios, causing significant startup working set increases. The motivation for stub dispatching was to eliminate much of the related working set, as well as distribute the remaining work throughout the lifetime of the process.</p> <p>Although it is possible for VSD to dispatch both virtual instance and interface method calls, it is currently used only for interface dispatch.</p>"},{"location":"virtual-stub-dispatch/#dependencies","title":"Dependencies","text":""},{"location":"virtual-stub-dispatch/#component-dependencies","title":"Component Dependencies","text":"<p>The stub dispatching code exists relatively independently of the rest of the runtime. It provides an API that allows dependent components to use it, and the dependencies listed below comprise a relatively small surface area.</p>"},{"location":"virtual-stub-dispatch/#code-manager","title":"Code Manager","text":"<p>VSD effectively relies on the code manager to provide information about state of a method, in particular, whether or not any particular method has transitioned to its final state in order that VSD may decide on details such as stub generation and target caching.</p>"},{"location":"virtual-stub-dispatch/#types-and-methods","title":"Types and Methods","text":"<p>MethodTables hold pointers to the dispatch maps used to determine the target code address for any given VSD call site.</p>"},{"location":"virtual-stub-dispatch/#special-types","title":"Special Types","text":"<p>Calls on COM interop types must be custom dispatched, as they both have specialized target resolution.</p>"},{"location":"virtual-stub-dispatch/#components-dependent-on-this-component","title":"Components Dependent on this Component","text":""},{"location":"virtual-stub-dispatch/#code-manager_1","title":"Code Manager","text":"<p>The code manager relies on VSD for providing the JIT compiler with call site targets for interface calls.</p>"},{"location":"virtual-stub-dispatch/#class-builder","title":"Class Builder","text":"<p>The class builder uses the API exposed by the dispatch mapping code to create dispatch maps during type building that will be used at dispatch type by the VSD code.</p>"},{"location":"virtual-stub-dispatch/#design-goals-and-non-goals","title":"Design Goals and Non-goals","text":""},{"location":"virtual-stub-dispatch/#goals","title":"Goals","text":""},{"location":"virtual-stub-dispatch/#working-set-reduction","title":"Working Set Reduction","text":"<p>Interface dispatch was previously implemented using a large, somewhat sparse vtable lookup map dealing with process-wide interface identifiers. The goal was to reduce the amount of cold working set by generating dispatch stubs as they were required, in theory keeping related call sites and their dispatch stubs close to each other and increasing the working set density.</p> <p>It is important to note that the initial working set involved with VSD is higher per call site due to the data structures required to track the various stubs that are created and collected as the system runs; however, as an application reaches  steady state, these data structures are not needed for simple dispatching and so gets paged out. Unfortunately, for client applications this equated to a slower startup time, which is one of the factors that led to disabling VSD for virtual methods.</p>"},{"location":"virtual-stub-dispatch/#throughput-parity","title":"Throughput Parity","text":"<p>It was important to keep interface and virtual method dispatch at an amortized parity with the previous vtable dispatch mechanism.</p> <p>While it was immediately obvious that this was achievable with interface dispatch, it turned out to be somewhat slower with virtual method dispatch, one of the factors that led to disabling VSD for virtual methods.</p>"},{"location":"virtual-stub-dispatch/#design-of-token-representation-and-dispatch-map","title":"Design of Token Representation and Dispatch Map","text":"<p>Dispatch tokens are native word-sized values that are allocated at runtime, consisting internally of a tuple that represents an interface and slot.</p> <p>The design uses a combination of assigned type identifier values and slot numbers. Dispatch tokens consist of a combination of these two values. To facilitate integration with the runtime, the implementation also assigns slot numbers in the same way as the classic v-table layout. This means that the runtime can still deal with MethodTables, MethodDescs, and slot numbers in exactly the same way, except that the v-table must be accessed via helper methods instead of being directly accessed in order to handle this abstraction.</p> <p>The term slot will always be used in the context of a slot index value in the classic v-table layout world and as created and interpreted by the mapping mechanism. What this means is that this is the slot number if you were to picture the classic method table layout of virtual method slots followed by non-virtual method slots, as previously implemented in the runtime. It's important to understand this distinction because within the runtime code, slot means both an index into the classic v-table structure, as well as the address of the pointer in the v-table itself. The change is that slot is now only an index value, and the code pointer addresses are contained in the implementation table (discussed below).</p> <p>The dynamically assigned type identifier values will be discussed later on.</p>"},{"location":"virtual-stub-dispatch/#method-table","title":"Method Table","text":""},{"location":"virtual-stub-dispatch/#implementation-table","title":"Implementation Table","text":"<p>This is an array that, for each method body introduced by the type, has a pointer to the entrypoint to that method. Its members are arranged in the following order:</p> <ul> <li>Introduced (newslot) virtual methods.</li> <li>Introduced non-virtual (instance and static) methods.</li> <li>Overriding virtual methods.</li> </ul> <p>The reason for this format is that it provides a natural extension to the classic v-table layout. As a result many entries in the slot map (described below) can be inferred by this order and other details such as the total number of virtuals and non-virtuals for the class.</p> <p>When stub dispatch for virtual instance methods is disabled (as it is currently), the implementation table is non-existent and is substituted with a true vtable. All mapping results are expressed as slots for the vtable rather than an implementation table. Keep this in mind when implementation tables are mentioned throughout this document.</p>"},{"location":"virtual-stub-dispatch/#slot-map","title":"Slot Map","text":"<p>The slot map is a table of zero or more &lt;type, [&lt;slot, scope, (index | slot)&gt;]&gt; entries. type is the dynamically assigned identification number mentioned above, and is either a sentinel value to indicate the current class (a call to a virtual instance method), or is an identifier for an interface implemented by the current class (or implicitly by one if its parents). The sub-map (contained in brackets) has one or more entries. Within each entry, the first element always indicates a slot within type. The second element, scope, specifies whether or not the third element is an implementation index or a slot number. scope can be a known sentinel value that indicates that the next number is to be interpreted as a virtual slot number, and should be resolved virtually as this.slot. scope can also identify a particular class in the inheritance hierarchy of the current class, and in such a case the third argument is an index into the implementation table of the class indicated by scope, and is the final method implementation for type.slot.</p>"},{"location":"virtual-stub-dispatch/#example","title":"Example","text":"<p>The following is a small class structure (modeled in C#), and what the resulting implementation table and slot map would be for each class.</p> <p></p> <p>Thus, looking at this map, we see that the first column of the sub-maps of the slot maps correspond to the slot number in the classic virtual table view (remember that System.Object contributes four virtual methods of its own, which are omitted for clarity). Searches for method implementations are always bottom-up. Thus, if I had an object of type B and I wished to invoke I.Foo, I would look for a mapping of I.Foo starting at B's slot map. Not finding it there, I would look in A's slot map and find it there. It states that virtual slot 0 of I (corresponding to I.Foo) is implemented by virtual slot 4. Then I return to B's slot map and search for an implementation for virtual slot 4, and find that it is implemented by slot 1 in its own implementation table.</p>"},{"location":"virtual-stub-dispatch/#additional-uses","title":"Additional Uses","text":"<p>It is important to note that this mapping technique can be used to implement methodimpl re-mapping of virtual slots (i.e., a virtual slot mapping in the map for the current class, similar to how an interface slot is mapped to a virtual slot). Because of the scoping capabilities of the map, non-virtual methods may also be referenced. This may be useful if ever the runtime wants to support the implementation of interfaces with non-virtual methods.</p>"},{"location":"virtual-stub-dispatch/#optimizations","title":"Optimizations","text":"<p>The slot maps are bit-encoded and take advantage of typical interface implementation patterns using delta values, thus reducing the map size significantly. In addition, new slots (both virtual and non-) can be implied by their order in the implementation table. If the table contains new virtual slots followed by new instance slots, then followed by overrides, then the appropriate slot map entries can be implied by their index in the implementation table combined with the number of virtuals inherited by the parent class. All such implied map entries have been indicated with a (*). The current layout of data structures uses the following pattern, where the DispatchMap is only present when mappings cannot be fully implied by ordering in the implementation table.</p> <pre><code>MethodTable -&gt; [DispatchMap -&gt;] ImplementationTable\n</code></pre>"},{"location":"virtual-stub-dispatch/#type-id-map","title":"Type ID Map","text":"<p>This will map types to IDs, which are allocated as monotonically increasing values as each previously unmapped type is encountered. Currently, all such types are interfaces.</p> <p>Currently, this is implemented using a HashMap, and contains entries for both lookup directions.</p>"},{"location":"virtual-stub-dispatch/#dispatch-tokens","title":"Dispatch Tokens","text":"<p>Dispatch tokens will be &lt;typeID,slot&gt; tuples. For interfaces, the type will be the interface ID assigned to that type. For virtual methods, this will be a constant value to indicate that the slot should just be resolved virtually within the type to be dispatched on (a virtual method call on this). This value pair will in most cases fit into the platform's native word size. On x86, this will likely be the lower 16 bits of each value, concatenated. This can be generalized to handle overflow issues similar to how a TypeHandle in the runtime can be either a MethodTable pointer or a &lt;TypeHandle,TypeHandle&gt; pair, using a sentinel bit to differentiate the two cases. It has yet to be determined if this is necessary.</p>"},{"location":"virtual-stub-dispatch/#design-of-virtual-stub-dispatch","title":"Design of Virtual Stub Dispatch","text":""},{"location":"virtual-stub-dispatch/#dispatch-token-to-implementation-resolution","title":"Dispatch Token to Implementation Resolution","text":"<p>Given a token and type, the implementation is found by mapping the token to an implementation table index for the type. The implementation table is reachable from the type's MethodTable. This map is created in BuildMethodTable: it enumerates all interfaces implemented by the type for which it is building a MethodTable and determines every interface method that the type implements or overrides. By keeping track of this information, at interface dispatch time it is possible to determine the target code given the token and the target object (from which the MethodTable and token mapping can be obtained).</p>"},{"location":"virtual-stub-dispatch/#stubs","title":"Stubs","text":"<p>Interface dispatch calls go through stubs. These stubs are all generated on demand, and all have the ultimate purpose of matching a token and object with an implementation, and forwarding the call to that implementation.</p> <p>There are currently three types of stubs. The below diagram shows the general control flow between these stubs, and will be explained below.</p> <p></p>"},{"location":"virtual-stub-dispatch/#generic-resolver","title":"Generic Resolver","text":"<p>This is in fact just a C function that serves as the final failure path for all stubs. It takes a &lt;token, type&gt; tuple and returns the target. The generic resolver is also responsible for creating dispatch and resolver stubs when they are required, patching indirection cells when better stubs become available, caching results, and all bookkeeping.</p>"},{"location":"virtual-stub-dispatch/#lookup-stubs","title":"Lookup Stubs","text":"<p>These stubs are the first to be assigned to an interface dispatch call site, and are created when the JIT compiles an interface call site. Since the JIT has no knowledge of the type being used to satisfy a token until the first call is made, this stub passes the token and type as arguments to the generic resolver. If necessary, the generic resolver will also create dispatch and resolve stubs, and will then back patch the call site to the dispatch stub so that the lookup stub is no longer used.</p> <p>One lookup stub is created for each unique token (i.e., call sites for the same interface slot will use the same lookup stub).</p>"},{"location":"virtual-stub-dispatch/#dispatch-stubs","title":"Dispatch Stubs","text":"<p>These stubs are used when a call site is believed to be monomorphic in behaviour. This means that the objects used at a particular call site are typically the same type (i.e. most of the time the object being invoked is the same as the last object invoked at the same site.) A dispatch stub takes the type (MethodTable) of the object being invoked and compares it with its cached type, and upon success jumps to its cached target. On x86, this is typically results in a \"comparison, conditional failure jump, jump to target\" sequence and provides the best performance of any stub. If a stub's type comparison fails, it jumps to its corresponding resolve stub (see below).</p> <p>One dispatch stub is created for each unique &lt;token,type&gt; tuple, but only lazily when a call site's lookup stub is invoked.</p>"},{"location":"virtual-stub-dispatch/#resolve-stubs","title":"Resolve Stubs","text":"<p>Polymorphic call sites are handled by resolve stubs. These stubs use the key pair &lt;token, type&gt; to resolve the target in a global cache, where token is known at JIT time and type is determined at call time. If the global cache does not contain a match, then the final step of the resolve stub is to call the generic resolver and jump to the returned target. Since the generic resolver will insert the &lt;token, type, target&gt; tuple into the cache, a subsequent call with the same &lt;token, type&gt; tuple will successfully find the target in the cache.</p> <p>When a dispatch stub fails frequently enough, the call site is deemed to be polymorphic and the resolve stub will back patch the call site to point directly to the resolve stub to avoid the overhead of a consistently failing dispatch stub. At sync points (currently the end of a GC), polymorphic sites will be randomly promoted back to monomorphic call sites under the assumption that the polymorphic attribute of a call site is usually temporary. If this assumption is incorrect for any particular call site, it will quickly trigger a backpatch to demote it to polymorphic again.</p> <p>One resolve stub is created per token, but they all use a global cache. A stub-per-token allows for a fast, effective hashing algorithm using a pre-calculated hash derived from the unchanging components of the &lt;token, type&gt; tuple.</p>"},{"location":"virtual-stub-dispatch/#code-sequences","title":"Code Sequences","text":"<p>The former interface virtual table dispatch mechanism results in a code sequence similar to this:</p> <p></p> <p>And the typical stub dispatch sequence is:</p> <p></p> <p>where expectedMT, failure and target are constants encoded in the stub.</p> <p>The typical stub sequence has the same number of instructions as the former interface dispatch mechanism, and fewer memory indirections may allow it to execute faster with a smaller working set contribution. It also results in smaller JITed code, since the bulk of the work is in the stub instead of the call site. This is only advantageous if a callsite is rarely invoked. Note that the failure branch is arranged so that x86 branch prediction will follow the success case.</p>"},{"location":"virtual-stub-dispatch/#current-state","title":"Current State","text":"<p>Currently, VSD is enabled only for interface method calls but not virtual instance method calls. There were several reasons for this:</p> <ul> <li>Startup: Startup working set and speed were hindered because of the need to generate a great deal of initial stubs.</li> <li>Throughput: While interface dispatches are generally faster with VSD, virtual instance method calls suffer an unacceptable speed degradation.</li> </ul> <p>As a result of disabling VSD for virtual instance method calls, every type has a vtable for virtual instance methods and the implementation table described above is disabled. Dispatch maps are still present to enable interface method dispatching.</p>"},{"location":"virtual-stub-dispatch/#physical-architecture","title":"Physical Architecture","text":"<p>For dispatch token and map implementation details, please see clr/src/vm/contractImpl.h and clr/src/vm/contractImpl.cpp.</p> <p>For virtual stub dispatch implementation details, please see clr/src/vm/virtualcallstub.h and clr/src/vm/virtualcallstub.cpp.</p>"},{"location":"xplat-minidump-generation/","title":"Introduction","text":"<p>Dump generation on Windows, Linux and other non-Windows platforms has several challenges. Dumps can be very large and the default name/location of a dump is not consistent across all our supported platforms.  The size of a full core dumps can be controlled somewhat with the \"coredump_filter\" file/flags but even with the smallest settings may be still too large and may not contain all the managed state needed for debugging. By default, some platforms use core as the name and place the core dump in the current directory from where the program is launched; others add the pid to the name. Configuring the core name and location requires superuser permission. Requiring superuser to make this consistent is not a satisfactory option.</p> <p>Our goal is to generate core dumps that are on par with WER (Windows Error Reporting) crash dumps on any supported Linux platform. To the very least we want to enable the following: - automatic generation of minimal size minidumps. The quality and quantity of the information contained in the dump should be on par with the information contained in a traditional Windows mini-dump. - simple configurability by the user (not su!).</p> <p>Our solution at this time is to intercept any unhandled exception in the PAL layer of the runtime and have coreclr itself trigger and generate a \"mini\" core dump.</p>"},{"location":"xplat-minidump-generation/#design","title":"Design","text":"<p>We looked at the existing technologies like Breakpad and its derivatives (e.g.: an internal MS version called msbreakpad from the SQL team....). Breakpad generates Windows minidumps but they are not compatible with existing tools like Windbg, etc. Msbreakpad even more so. There is a minidump to Linux core conversion utility but it seems like a wasted extra step. Breakpad does allow the minidump to be generated in-process inside the signal handlers. It restricts the APIs to what was allowed in a \"async\" signal handler (like SIGSEGV) and has a small subset of the C++ runtime that was also similarly constrained. We also need to add the set of memory regions for the \"managed\" state which requires loading and using the DAC's (*) enumerate memory interfaces. Loading modules is not allowed in an async signal handler but forking/execve is allowed so launching an utility that loads the DAC, enumerates the list of memory regions and writes the dump is the only reasonable option. It would also allow uploading the dump to a server too.</p> <p>* The DAC is a special build of parts of the coreclr runtime that allows inspection of the runtime's managed state (stacks, variables, GC state heaps) out of context. One of the many interfaces it provides is ICLRDataEnumMemoryRegions which enumerates all the managed state a minidump would require to enable a fruitful debugging experience.</p> <p>Breakpad could have still been used out of context in the generation utility but there seemed no value to their Windows-like minidump format when it would have to be converted to the native Linux core format away because in most scenarios using the platform tools like lldb is necessary. It also adds a coreclr build dependency on Google's Breakpad or SQL's msbreakpad source repo. The only advantage is that the breakpad minidumps may be a little smaller because minidumps memory regions are byte granule and Linux core memory regions need to be page granule.</p>"},{"location":"xplat-minidump-generation/#implementation-details","title":"Implementation Details","text":""},{"location":"xplat-minidump-generation/#linux","title":"Linux","text":"<p>Core dump generation is triggered anytime coreclr is going to abort (via PROCAbort()) the process because of an unhandled managed exception or an async signal like SIGSEGV, SIGILL, SIGFPE, etc. The createdump utility is located in the same directory as libcoreclr.so and is launched with fork/execve. The child createdump process is given permission to ptrace and access to the various special /proc files of the crashing process which waits until createdump finishes.</p> <p>The createdump utility starts by using ptrace to enumerate and suspend all the threads in the target process. The process and thread info (status, registers, etc.) is gathered. The auxv entries and DSO info is enumerated. DSO is the in memory data structures that described the shared modules loaded by the target. This memory is needed in the dump by gdb and lldb to enumerate the shared modules loaded and access their symbols. The module memory mappings are gathered from /proc/$pid/maps. None of the program or shared modules memory regions are explicitly added to dump's memory regions. The DAC is loaded and the enumerate memory region interfaces are used to build the memory regions list just like on Windows. The threads stacks and one page of code around the IP are added. The byte sized regions are rounded up to pages and then combined into contiguous regions.</p> <p>All the memory mappings from /proc/$pid/maps are in the PT_LOAD sections even though the memory is not actually in the dump. They have a file offset/size of 0.</p> <p>After all the process crash information has been gathered, the ELF core dump is written. The main ELF header created and written. The PT_LOAD note section is written one entry for each memory region in the dump. The process info, auxv data and NT_FILE entries are written to core. The NT_FILE entries are built from module memory mappings from /proc/$pid/maps. The threads state and registers are then written. Lastly all the memory regions gather above by the DAC, etc. are read from the target process and written to the core dump. All the threads in the target process are resumed and createdump terminates.</p> <p>Severe memory corruption</p> <p>As long as control can making it to the signal/abort handler and the fork/execve of the utility succeeds then the DAC memory enumeration interfaces can handle corruption to a point; the resulting dump just may not have enough managed state to be useful. We could investigate detecting this case and writing a full core dump.</p> <p>Stack overflow exception</p> <p>Like the severe memory corruption case, if the signal handler (<code>SIGSEGV</code>) gets control it can detect most stack overflow cases and does trigger a core dump. There are still many cases where this doesn't happen and the OS just terminates the process. There is a bug in the earlier versions (2.1.x or less) of the runtime where createdump isn't invoked for any stack overflow.</p>"},{"location":"xplat-minidump-generation/#freebsdopenbsdnetbsd","title":"FreeBSD/OpenBSD/NetBSD","text":"<p>There will be some differences gathering the crash information but these platforms still use ELF format core dumps so that part of the utility should be much different. The mechanism used for Linux to give createdump permission to use ptrace and access the /proc doesn't exists on these platforms.</p>"},{"location":"xplat-minidump-generation/#macos","title":"macOS","text":"<p>On .NET 5.0, createdump supported generating dumps on macOS but instead of the MachO dump format, it generates the ELF coredumps. This wad because of time constraints developing a MachO dump writer on the generation side and a MachO reader for the diagnostics tooling side (dotnet-dump and CLRMD). This means the native debuggers like gdb and lldb will not work with dumps obtained from apps running on a 5.0 runtime, but the dotnet-dump tool will allow the managed state to be analyzed. Because of this behavior an additional environment variable will need to be set (COMPlus_DbgEnableElfDumpOnMacOS=1) along with the ones below in the Configuration/Policy section.</p> <p>Starting .NET 6.0, native Mach-O core files get generated and the variable COMPlus_DbgEnableElfDumpOnMacOS has been deprecated.</p>"},{"location":"xplat-minidump-generation/#windows","title":"Windows","text":"<p>As of .NET 5.0, createdump and the below configuration environment variables are supported on Windows. It is implemented using the Windows MiniDumpWriteDump API. This allows consistent crash/unhandled exception dumps across all of our platforms.</p>"},{"location":"xplat-minidump-generation/#configurationpolicy","title":"Configuration/Policy","text":"<p>NOTE: Core dump generation in docker containers require the ptrace capability (--cap-add=SYS_PTRACE or --privileged run/exec options).</p> <p>Any configuration or policy is set with environment variables which are passed as options to the createdump utility.</p> <p>Environment variables supported:</p> <ul> <li><code>DOTNET_DbgEnableMiniDump</code>: if set to \"1\", enables this core dump generation. The default is NOT to generate a dump.</li> <li><code>DOTNET_DbgMiniDumpType</code>: See below. Default is \"2\" MiniDumpWithPrivateReadWriteMemory.</li> <li><code>DOTNET_DbgMiniDumpName</code>: if set, use as the template to create the dump path and file name. See \"Dump name formatting\" for how the dump name can be formatted. The default is /tmp/coredump.%p.</li> <li><code>DOTNET_CreateDumpDiagnostics</code>: if set to \"1\", enables the createdump utilities diagnostic messages (TRACE macro).</li> <li><code>DOTNET_CreateDumpVerboseDiagnostics</code>: if set to \"1\", enables the createdump utilities verbose diagnostic messages (TRACE_VERBOSE macro).</li> <li><code>DOTNET_CreateDumpLogToFile</code>: if set, it is the path of the file to write the createdump diagnostic messages.</li> <li><code>DOTNET_EnableCrashReport</code>: In .NET 6.0 or greater, if set to \"1\", createdump also generates a json formatted crash report which includes information about the threads and stack frames of the crashing application. The crash report name is the dump path/name with .crashreport.json appended.</li> <li><code>DOTNET_EnableCrashReportOnly</code>: In .NET 7.0 or greater, same as DOTNET_EnableCrashReport except the core dump is not generated.</li> </ul> <p>DOTNET_DbgMiniDumpType values:</p> Value Minidump Enum Description 1 MiniDumpNormal Include just the information necessary to capture stack traces for all existing threads in a process. Limited GC heap memory and information. 2 MiniDumpWithPrivateReadWriteMemory (default) Includes the GC heaps and information necessary to capture stack traces for all existing threads in a process. 3 MiniDumpFilterTriage Include just the information necessary to capture stack traces for all existing threads in a process. Limited GC heap memory and information. 4 MiniDumpWithFullMemory Include all accessible memory in the process. The raw memory data is included at the end, so that the initial structures can be mapped directly without the raw memory information. This option can result in a very large file. <p>(Please refer to MSDN for the meaning of the minidump enum values reported above)</p> <p>Command Line Usage</p> <p>The createdump utility can also be run from the command line on arbitrary .NET Core processes. The type of dump can be controlled with the below command switches. The default is a \"minidump\" which contains the majority the memory and managed state needed. Unless you have ptrace (CAP_SYS_PTRACE) administrative privilege, you need to run with sudo or su. The same as if you were attaching with lldb or other native debugger.</p> <pre><code>createdump [options] pid\n-f, --name - dump path and file name. The default is '/tmp/coredump.%p'. These specifiers are substituted with following values:\n   %p  PID of dumped process.\n   %e  The process executable filename.\n   %h  Hostname return by gethostname().\n   %t  Time of dump, expressed as seconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).\n-n, --normal - create minidump.\n-h, --withheap - create minidump with heap (default).\n-t, --triage - create triage minidump.\n-u, --full - create full core dump.\n-d, --diag - enable diagnostic messages.\n-v, --verbose - enable verbose diagnostic messages.\n-l, --logtofile - file path and name to log diagnostic messages.\n--crashreport - write crash report file (dump file path + .crashreport.json).\n--crashreportonly - write crash report file only (no dump).\n--crashthread &lt;id&gt; - the thread id of the crashing thread.\n--signal &lt;code&gt; - the signal code of the crash.\n--singlefile - enable single-file app check.\n</code></pre> <p>Dump name formatting</p> <p>As of .NET 5.0, the following subset of the core pattern (see core) dump name formatting is supported:</p> <pre><code>%%  A single % character.\n%d  PID of dumped process (for backwards createdump compatibility).\n%p  PID of dumped process.\n%e  The process executable filename.\n%h  Hostname return by gethostname().\n%t  Time of dump, expressed as seconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).\n</code></pre>"},{"location":"xplat-minidump-generation/#testing","title":"Testing","text":"<p>The test plan is to modify the SOS tests in the (still) private debuggertests repo to trigger and use the core minidumps generated. Debugging managed core dumps on Linux is not supported by mdbg at this time until we have a ELF core dump reader so only the SOS tests (which use lldb on Linux) will be modified.</p>"}]}